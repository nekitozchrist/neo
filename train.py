# train.py

ma = False
if __name__ == "__main__":
	ma = True

import time
tt = lambda: time.time()
tot = tt()

from colorama import Back, Fore, Style, init
init()

def ts(t, n):
	"""
	–§–æ—Ä–º–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫—É —Å –≤—Ä–µ–º–µ–Ω–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–∏.

	Args:
		t (float): –Ω–∞—á–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
		n (str): –Ω–∞–∑–≤–∞–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏

	Returns:
		str: —Å—Ç—Ä–æ–∫–∞ —Å –≤—Ä–µ–º–µ–Ω–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏ —Ü–≤–µ—Ç–æ–≤–æ–π –º–∞—Ä–∫–∏—Ä–æ–≤–∫–æ–π
	"""
	v = float(format(tt() - t, ".2f"))
	cp = ""
	if v <= 0.3: cp = f"{Fore.GREEN}{v}"
	if v > 0.3 and v <= 1.2: cp = f"{Fore.YELLOW}{v}"
	if v > 1.2: cp = f"{Fore.RED}{v}"
	c = f"{n}: {cp} —Å–µ–∫{Fore.RESET}"
	return c

st = tt()
if ma: print()

import os
if ma: print(ts(st, "os"))

import sys
if ma: print(ts(st, "sys"))

from contextlib import redirect_stdout, redirect_stderr
if ma: print(ts(st, "contextlib"))

import math
if ma: print(ts(st, "math"))

import pickle
if ma: print(ts(st, "pickle"))

import yaml
if ma: print(ts(st, "yaml"))

import numpy as np
if ma: print(ts(st, "numpy"))

from pathlib import Path
if ma: print(ts(st, "pathlib"))

from config import config, clear_screen, error, header, info, progress_bar, success, title, warning, rulables
if ma: print(ts(st, "config"))

from tqdm import tqdm
if ma: print(ts(st, "tqdm"))

import subprocess
if ma: print(ts(st, "subprocess"))

import torch
if ma: print(ts(st, "torch"))

import torch.nn as nn
if ma: print(ts(st, "torch.nn"))

import torch.nn.functional as F
if ma: print(ts(st, "torch.nn.functional"))

from sklearn.preprocessing import MultiLabelBinarizer
if ma: print(ts(st, "sklearn"))

from torch.amp import GradScaler, autocast
if ma: print(ts(st, "torch.amp"))

from torch.optim import AdamW
if ma: print(ts(st, "torch.optim"))

from torch.utils.data import DataLoader, Dataset
if ma: print(ts(st, "torch.utils.data"))

from torch.utils.tensorboard import SummaryWriter
if ma: print(ts(st, "torch.utils.tensorboard"))

from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_cosine_schedule_with_warmup
if ma: print(ts(st, "transformers"))

if ma: print("\n" + ts(tot, "–û–±—â–µ–µ –≤—Ä–µ–º—è –∏–º–ø–æ—Ä—Ç–æ–≤") + "\n")

def worker_init_fn(worker_id):
	"""
	–§—É–Ω–∫—Ü–∏—è, –≤—ã–∑—ã–≤–∞–µ–º–∞—è –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ worker'–∞ –≤ DataLoader.

	Args:
		worker_id (int): –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä worker'–∞
	"""
	print(f"üë∑ Worker {worker_id} —Å–æ–∑–¥–∞–Ω (PID: {os.getpid()})\n")
	return None

def training_menu():
	"""
	–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –º–µ–Ω—é –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

	Returns:
		tuple: (test_mode: bool, test_size: int or None)
			- test_mode: True –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞, False –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ, None –¥–ª—è –≤—ã—Ö–æ–¥–∞
			- test_size: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞ (–µ—Å–ª–∏ test_mode=True)
	"""
	#clear_screen()
	title("–†–ï–ñ–ò–ú –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò")

	print(f"{Style.BRIGHT}1.{Style.RESET_ALL} –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ({EPOCHS} —ç–ø–æ—Ö, –≤—Å–µ –¥–∞–Ω–Ω—ã–µ)")
	print(f"{Style.BRIGHT}2.{Style.RESET_ALL} –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º (1 —ç–ø–æ—Ö–∞, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)")
	print(f"{Style.BRIGHT}3.{Style.RESET_ALL} –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è")
	print(f"{Style.BRIGHT}4.{Style.RESET_ALL} –ù–∞–∑–∞–¥ –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é")
	print()

	while True:
		choice = input(f"{Fore.CYAN}–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º [1-4]: {Style.RESET_ALL}").strip()

		if choice == "1":
			return False, None  # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
		elif choice == "2":
			print(f"\n{Fore.YELLOW}–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º:{Style.RESET_ALL}")
			print(f"  ‚Ä¢ 1 —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è")
			print(f"  ‚Ä¢ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤")
			print(f"  ‚Ä¢ –ú–æ–¥–µ–ª—å –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è")
			print(f"  ‚Ä¢ –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏")

			test_size = input(f"{Fore.YELLOW}–ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 500): {Style.RESET_ALL}").strip()
			test_size = int(test_size) if test_size.isdigit() else 500
			return True, test_size

		elif choice == "3":
			# –ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
			print(f"\n{Fore.YELLOW}–¢–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:{Style.RESET_ALL}")
			print(f"  ‚Ä¢ –ú–æ–¥–µ–ª—å: {MODEL_NAME}")
			print(f"  ‚Ä¢ –≠–ø–æ—Ö–∏: {EPOCHS}")
			print(f"  ‚Ä¢ Batch size: {BATCH_SIZE}")
			print(f"  ‚Ä¢ Learning rate: {LEARNING_RATE}")
			print(f"  ‚Ä¢ Max length: {MAX_LEN}")
			print(f"\n–î–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª: config.yaml")
			input(f"\n{Fore.CYAN}–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è...{Style.RESET_ALL}")
			continue
		elif choice == "4":
			return None, None  # –í—ã—Ö–æ–¥
		else:
			error("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –í–≤–µ–¥–∏—Ç–µ 1, 2, 3 –∏–ª–∏ 4.")

CHECKPOINTS_DIR = config['checks_dir']
LOG_DIR = config['logs_dir']
OUTPUT_DIR = config['final_model_dir']
MODEL_NAME = config['source_model_dir']
NUM_WORKERS = int(config['num_workers'])

EPOCHS = int(config['epochs'])
MAX_LEN = int(config['max_len'])
BATCH_SIZE = int(config['batch_size'])
ACCUMULATION_STEPS = int(config['accumulation_steps'])
LEARNING_RATE = float(config['learning_rate'])
WARMUP_PERCENT = float(config['warmup_percent'])
WEIGHT_DECAY = float(config['weight_decay'])
FP32 = config['fp32']
USE_TRITON = config['use_triton']

def suppress_output(func):
	"""
	–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ (stdout/stderr) —Ñ—É–Ω–∫—Ü–∏–∏.

	Args:
		func (callable): —Ñ—É–Ω–∫—Ü–∏—è, –≤—ã–≤–æ–¥ –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω–æ –ø–æ–¥–∞–≤–∏—Ç—å

	Returns:
		callable: –æ–±—ë—Ä–Ω—É—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è
	"""
	def wrapper(*args, **kwargs):
		import os
		null_device = os.devnull

		with open(null_device, 'w') as f:
			old_stdout = sys.stdout
			old_stderr = sys.stderr
			sys.stdout = f
			sys.stderr = f
			try:
				result = func(*args, **kwargs)  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
				return result  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
			finally:
				sys.stdout = old_stdout
				sys.stderr = old_stderr
	return wrapper

def compile_model(model):
	"""
	–ö–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TorchInductor.

	Args:
		model (torch.nn.Module): –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–º–ø–∏–ª—è—Ü–∏–∏

	Returns:
		torch.nn.Module: —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
	"""
	return torch.compile(model, backend="inductor", mode="default")

def compute_class_weights(labels_list, num_classes=28):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç—ã –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏.

	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø—Ä–∏ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

	–ê–ª–≥–æ—Ä–∏—Ç–º:
	1. –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –º–µ—Ç–∫–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ —Å–ø–∏—Å–∫–æ–≤ –≤ –µ–¥–∏–Ω—ã–π —Å–ø–∏—Å–æ–∫.
	2. –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞.
	3. –í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –∫–∞–∫ –æ–±—Ä–∞—Ç–Ω—É—é –≤–µ–ª–∏—á–∏–Ω—É —á–∞—Å—Ç–æ—Ç—ã.
	4. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤–µ—Å–∞ —Ç–∞–∫, —á—Ç–æ–±—ã –∏—Ö —Å—É–º–º–∞ —Ä–∞–≤–Ω—è–ª–∞—Å—å 1.

	Args:
		labels_list (list of list of int): —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –º–µ—Ç–æ–∫,
			–≥–¥–µ –∫–∞–∂–¥—ã–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Å–ø–∏—Å–æ–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω–¥–µ–∫—Å—ã –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
		num_classes (int, optional): –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –≤ –∑–∞–¥–∞—á–µ.
			–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 28.

	Returns:
		torch.Tensor: —Ç–µ–Ω–∑–æ—Ä –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ —Ä–∞–∑–º–µ—Ä–æ–º [num_classes],
			–≥–¥–µ –≤–µ—Å –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω –æ–±—Ä–∞—Ç–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ –µ–≥–æ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏

	–ü—Ä–∏–º–µ—Ä:
		labels_list = [[0, 1], [1, 2], [0, 2]]
		num_classes = 3
		‚Üí –≤–µ—Å–∞: tensor([0.6, 0.3, 0.1]) (–ø—Ä–∏–º–µ—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏)
	"""
	from collections import Counter

	# –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –º–µ—Ç–∫–∏ –≤ –µ–¥–∏–Ω—ã–π —Å–ø–∏—Å–æ–∫
	all_labels = []
	for labels in labels_list:
		all_labels.extend(labels)

	# –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
	class_counts = Counter(all_labels)

	# –°–æ–∑–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ —á–∞—Å—Ç–æ—Ç –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤ (–æ—Ç 0 –¥–æ num_classes-1)
	# –ï—Å–ª–∏ –∫–ª–∞—Å—Å –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º 1 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
	counts = [class_counts.get(i, 1) for i in range(num_classes)]

	# –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–∞–∫ –æ–±—Ä–∞—Ç–Ω—É—é –≤–µ–ª–∏—á–∏–Ω—É —á–∞—Å—Ç–æ—Ç
	weights = 1.0 / torch.tensor(counts, dtype=torch.float32)

	# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞ —Ç–∞–∫, —á—Ç–æ–±—ã –∏—Ö —Å—É–º–º–∞ –±—ã–ª–∞ —Ä–∞–≤–Ω–∞ 1
	return weights / weights.sum()



class FocalLoss(nn.Module):
	"""
	–†–µ–∞–ª–∏–∑–∞—Ü–∏—è Focal Loss ‚Äî –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è –∑–∞–¥–∞—á –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
	—Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.

	Focal Loss —Å–Ω–∏–∂–∞–µ—Ç –≤–∫–ª–∞–¥ –ª–µ–≥–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö.
	–û—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –ø—Ä–∏ —Å–∏–ª—å–Ω–æ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ –∫–ª–∞—Å—Å–æ–≤.


	–§–æ—Ä–º—É–ª–∞: FL(p_t) = -Œ±_t * (1 - p_t)^Œ≥ * log(p_t), –≥–¥–µ:
	- p_t ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤–µ—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
	- Œ±_t ‚Äî –≤–µ—Å–æ–≤–æ–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–ª–∞—Å—Å–∞
	- Œ≥ (gamma) ‚Äî –ø–∞—Ä–∞–º–µ—Ç—Ä —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ –∞–∫—Ü–µ–Ω—Ç –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö)

	Args:
		alpha (float): –≤–µ—Å–æ–≤–æ–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1)
		gamma (float): –ø–∞—Ä–∞–º–µ—Ç—Ä —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2)
	"""
	def __init__(self, alpha=1, gamma=2):
		super().__init__()
		self.alpha = alpha
		self.gamma = gamma

	def forward(self, inputs, targets):
		"""
		–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.

		Args:
			inputs (torch.Tensor): –ª–æ–≥–∏—Ç—ã –º–æ–¥–µ–ª–∏ (–Ω–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è)
			targets (torch.Tensor): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (one-hot –∏–ª–∏ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–µ)

		Returns:
			torch.Tensor: —Å–∫–∞–ª—è—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
		"""
		# –í—ã—á–∏—Å–ª—è–µ–º –±–∏–Ω–∞—Ä–Ω—É—é –∫—Ä–æ—Å—Å‚Äë—ç–Ω—Ç—Ä–æ–ø–∏—é (–±–µ–∑ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è)
		bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')


		# –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ —Å–∏–≥–º–æ–∏–¥
		pt = torch.exp(-bce_loss)

		# –ü—Ä–∏–º–µ–Ω—è–µ–º Focal –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é
		focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss

		return focal_loss.mean()




class MultiLabelEmotionsDataset(Dataset):
	"""
	Dataset –¥–ª—è –∑–∞–¥–∞—á–∏ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —ç–º–æ—Ü–∏–π (–º—É–ª—å—Ç–∏‚Äë–ª–µ–π–±–ª).

	–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –º–µ—Ç–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç, –ø—Ä–∏–≥–æ–¥–Ω—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.
	"""
	def __init__(self, texts, labels, tokenizer, max_len):
		"""
		Args:
			texts (list of str): —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
			labels (list of list of int): —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –º–µ—Ç–æ–∫ (–∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç ‚Äî —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤)
			tokenizer (transformers.PreTrainedTokenizer): —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏
			max_len (int): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
		"""
		self.texts = texts
		self.labels = labels
		self.tokenizer = tokenizer
		self.max_len = max_len

	def __len__(self):
		"""–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ."""
		return len(self.texts)

	def __getitem__(self, item):
		"""
		–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –≥–æ—Ç–æ–≤–æ–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.

		Args:
			item (int): –∏–Ω–¥–µ–∫—Å –ø—Ä–∏–º–µ—Ä–∞

		Returns:
			dict: —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏:
				- 'input_ids': —Ç–µ–Ω–∑–æ—Ä ID —Ç–æ–∫–µ–Ω–æ–≤
				- 'attention_mask': —Ç–µ–Ω–∑–æ—Ä –º–∞—Å–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è
				- 'labels': —Ç–µ–Ω–∑–æ—Ä –º–µ—Ç–æ–∫
		"""
		text = str(self.texts[item])
		label = torch.tensor(self.labels[item], dtype=torch.float)  # float –¥–ª—è BCEWithLogitsLoss


		# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å padding –∏ truncation
		encoding = self.tokenizer(
			text,
			truncation=True,
			padding='max_length',
			max_length=self.max_len,
			return_tensors='pt',
		)

		return {
			'input_ids': encoding['input_ids'][0],
			'attention_mask': encoding['attention_mask'][0],
			'labels': label
		}



def calculate_checkpoint_frequency(total_steps):
	"""
	–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —à–∞–≥–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã –º–æ–¥–µ–ª–∏.

	–°—Ç—Ä–∞—Ç–µ–≥–∏—è:
	- –î–æ 100 —à–∞–≥–æ–≤ ‚Üí —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ
	- 100‚Äì1000 —à–∞–≥–æ–≤ ‚Üí —Å–µ—Ä–µ–¥–∏–Ω–∞ –∏ –∫–æ–Ω–µ—Ü
	- 1000‚Äì5000 —à–∞–≥–æ–≤ ‚Üí –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤ + –∫–æ–Ω–µ—Ü
	- 5000‚Äì10¬†000 —à–∞–≥–æ–≤ ‚Üí –∫–∞–∂–¥—ã–µ 2000 —à–∞–≥–æ–≤ + –∫–æ–Ω–µ—Ü
	- –ë–æ–ª–µ–µ 10¬†000 —à–∞–≥–æ–≤ ‚Üí –∫–∞–∂–¥—ã–µ 20% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ —à–∞–≥–æ–≤ + –∫–æ–Ω–µ—Ü

	Args:
		total_steps (int): –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è

	Returns:
		list of int: —Å–ø–∏—Å–æ–∫ —à–∞–≥–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
	"""
	if total_steps <= 100:
		return [total_steps]
	elif total_steps <= 1000:
		step1 = total_steps // 2
		return [step1, total_steps]
	elif total_steps <= 5000:
		interval = 1000
		steps = list(range(interval, total_steps, interval))
		if steps[-1] != total_steps:
			steps.append(total_steps)
		return steps
	elif total_steps <= 10000:
		interval = 2000
		steps = list(range(interval, total_steps, interval))
		if steps[-1] != total_steps:
			steps.append(total_steps)
		return steps
	else:
		interval = int(total_steps * 0.2)
		steps = list(range(interval, total_steps, interval))
		if steps[-1] != total_steps:
			steps.append(total_steps)
		return steps




def get_gpu_metrics_light():
	"""
	–°–æ–±–∏—Ä–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ GPU (–ø–∞–º—è—Ç—å).

	Returns:
		dict: —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏:
			- 'gpu_memory_used_mb': –∑–∞–Ω—è—Ç–∞—è –ø–∞–º—è—Ç—å (–ú–ë)
			- 'gpu_memory_reserved_mb': –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–∞–º—è—Ç—å (–ú–ë)
			- 'gpu_memory_total_mb': –æ–±—â–∞—è –ø–∞–º—è—Ç—å (–ú–ë)
	"""
	metrics = {"gpu_memory_used_mb": 0, "gpu_memory_total_mb": 0}

	if not torch.cuda.is_available():
		return metrics

	try:
		metrics["gpu_memory_used_mb"] = torch.cuda.memory_allocated() / 1024 / 1024
		metrics["gpu_memory_reserved_mb"] = torch.cuda.memory_reserved() / 1024 / 1024
		metrics["gpu_memory_total_mb"] = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024
	except:
		pass

	return metrics



def get_system_metrics_light():
	"""
	–°–æ–±–∏—Ä–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (CPU, RAM).

	Returns:
		dict: —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏:
			- 'cpu_percent': –∑–∞–≥—Ä—É–∑–∫–∞ CPU (%)
			- 'ram_percent': –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAM (%)
	"""
	try:
		import psutil
		return {
			"cpu_percent": psutil.cpu_percent(),
			"ram_percent": psutil.virtual_memory().percent,
		}
	except ImportError:
		return {"cpu_percent": 0, "ram_percent": 0}




class MetricsLogger:
	"""
	–ö–ª–∞—Å—Å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è –≤ TensorBoard –∏ –∫–æ–Ω—Å–æ–ª—å.
	"""
	def __init__(self, writer, device, log_interval=100):
		"""
		Args:
			writer (SummaryWriter): –æ–±—ä–µ–∫—Ç –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ TensorBoard
			device (torch.device): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cpu/cuda)
			log_interval (int): –∏–Ω—Ç–µ—Ä–≤–∞–ª —à–∞–≥–æ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
		"""
		self.writer = writer
		self.device = device
		self.log_interval = log_interval
		self.step = 0
		self._last_log_step = 0

		def log_training_step(self, loss, lr, grad_norm=None):
		"""
		–õ–æ–≥–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è –≤ TensorBoard –∏ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ ‚Äî –≤ –∫–æ–Ω—Å–æ–ª—å.

		–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç:
		- –ø–æ—Ç–µ—Ä–∏ (loss)
		- —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (lr)
		- –Ω–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω–∞)
		- —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (CPU, RAM, GPU) —Å –∑–∞–¥–∞–Ω–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º

		Args:
			loss (float): –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–∞ —Ç–µ–∫—É—â–µ–º —à–∞–≥–µ
			lr (float): —Ç–µ–∫—É—â–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate)
			grad_norm (float, optional): –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ clipping.
				–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None (–Ω–µ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è).
		"""
		# –ó–∞–ø–∏—Å—å –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –≤ TensorBoard
		self.writer.add_scalar("train/loss", loss, self.step)
		self.writer.add_scalar("train/lr", lr, self.step)

		if grad_norm is not None:
			self.writer.add_scalar("train/grad_norm", grad_norm, self.step)

		# –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –∑–∞–ø–∏—Å—å —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –∏ –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
		if self.step - self._last_log_step >= self.log_interval:
			self._log_resources_light()  # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
			self._last_log_step = self.step

			# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å –∫–∞–∂–¥—ã–µ 200 —à–∞–≥–æ–≤
			if self.step % 200 == 0:
				self._console_log_light(loss, lr)

		self.step += 1  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á—ë—Ç—á–∏–∫ —à–∞–≥–æ–≤

	def _log_resources_light(self):
		"""
		–°–æ–±–∏—Ä–∞–µ—Ç –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤ TensorBoard –±–∞–∑–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:
		- –∑–∞–≥—Ä—É–∑–∫—É CPU –∏ RAM (—á–µ—Ä–µ–∑ psutil)
		- –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
		"""
		# –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (CPU, RAM)
		system_metrics = get_system_metrics_light()
		for key, value in system_metrics.items():
			self.writer.add_scalar(f"system/{key}", value, self.step)

		# –ú–µ—Ç—Ä–∏–∫–∏ GPU (–µ—Å–ª–∏ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞)
		if self.device.type == "cuda":
			gpu_metrics = get_gpu_metrics_light()
			for key, value in gpu_metrics.items():
				self.writer.add_scalar(f"gpu/{key}", value, self.step)

	def _console_log_light(self, loss, lr):
		"""
		–í—ã–≤–æ–¥–∏—Ç –≤ –∫–æ–Ω—Å–æ–ª—å —Å–æ–∫—Ä–∞—â—ë–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –æ–±—É—á–µ–Ω–∏—è.
		–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM, –µ—Å–ª–∏ GPU –¥–æ—Å—Ç—É–ø–µ–Ω.

		Args:
			loss (float): —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
			lr (float): —Ç–µ–∫—É—â–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
		"""
		if torch.cuda.is_available():
			gpu_metrics = get_gpu_metrics_light()
			if gpu_metrics['gpu_memory_total_mb'] > 0:
				allocated_percent = (
					gpu_metrics['gpu_memory_used_mb'] /
					gpu_metrics['gpu_memory_total_mb'] * 100
				)
				reserved_percent = (
					gpu_metrics['gpu_memory_reserved_mb'] /
					gpu_metrics['gpu_memory_total_mb'] * 100
				)
				print(f"VRAM: {allocated_percent:.1f}% alloc / {reserved_percent:.1f}% reserved")

		def log_validation(self, metrics, epoch):
		"""
		–õ–æ–≥–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤ TensorBoard.

		Args:
			metrics (dict): —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç—Ä–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä:
				{
					'loss': 0.5,
					'accuracy': 0.9,
					'f1_score': 0.85
				}
			epoch (int): –Ω–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏ –æ–±—É—á–µ–Ω–∏—è
		"""
		for key, value in metrics.items():
			self.writer.add_scalar(f"val/{key}", value, epoch)


	def log_hyperparameters(self, hparams):
		"""
		–õ–æ–≥–∏—Ä—É–µ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤ TensorBoard (–≤–∫–ª–∞–¥–∫–∞ HParams).

		Args:
			hparams (dict): —Å–ª–æ–≤–∞—Ä—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä:
				{
					'batch_size': 32,
					'learning_rate': 1e-4,
					'optimizer': 'AdamW',
					'model_type': 'BERT-base'
				}
		"""
		# TensorBoard HParams —Ç—Ä–µ–±—É–µ—Ç –¥–≤–∞ —Å–ª–æ–≤–∞—Ä—è: hparams –∏ metrics
		# metrics –∑–¥–µ—Å—å ‚Äî —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
		metric_names = [f"val/{k}" for k in hparams.keys()]
		self.writer.add_hparams(hparams, {name: 0.0 for name in metric_names})


	def close(self):
		"""
		–ó–∞–∫—Ä—ã–≤–∞–µ—Ç writer, –æ—Å–≤–æ–±–æ–∂–¥–∞—è —Ä–µ—Å—É—Ä—Å—ã.
		–î–æ–ª–∂–µ–Ω –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –≤ –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è.
		"""
		self.writer.close()



def train_epoch(model, dataloader, optimizer, loss_fn, device, metrics_logger, scaler=None):
	"""
	–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–Ω—É —ç–ø–æ—Ö—É –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
		dataloader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
		optimizer (Optimizer): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
		loss_fn (Callable): —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
		device (torch.device): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cpu/cuda)
		metrics_logger (MetricsLogger): –ª–æ–≥–≥–µ—Ä –º–µ—Ç—Ä–∏–∫
		scaler (GradScaler, optional): –¥–ª—è AMP (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤)


	Returns:
		float: —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å –∑–∞ —ç–ø–æ—Ö—É
	"""
	model.train()
	total_loss = 0.0
	step = 0

	for batch in dataloader:
		optimizer.zero_grad()

		input_ids = batch['input_ids'].to(device)
		attention_mask = batch['attention_mask'].to(device)
		labels = batch['labels'].to(device)

		# Forward pass —Å AMP (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
		with autocast() if scaler is not None else torch.no_grad():
			outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
			loss = outputs.loss

		total_loss += loss.item()

		# Backward pass —Å AMP
		if scaler is not None:
			scaler.scale(loss).backward()
			# Gradient clipping
			scaler.unscale_(optimizer)
			torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
			scaler.step(optimizer)
			scaler.update()
		else:
			loss.backward()
			torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
			optimizer.step()

		# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —à–∞–≥–∞
		current_lr = optimizer.param_groups[0]['lr']
		grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), float('inf'))
		metrics_logger.log_training_step(loss.item(), current_lr, grad_norm)


		step += 1

	return total_loss / len(dataloader)




def validate_epoch(model, dataloader, loss_fn, device):
	"""
	–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–¥–Ω–æ–º —ç–ø–æ—Ö–µ.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		dataloader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
		loss_fn (Callable): —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
		device (torch.device): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ


	Returns:
		dict: —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç—Ä–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (loss, accuracy –∏ –¥—Ä.)
	"""
	model.eval()
	total_loss = 0.0
	correct = 0
	total = 0

	with torch.no_grad():
		for batch in dataloader:
			input_ids = batch['input_ids'].to(device)
			attention_mask = batch['attention_mask'].to(device)
			labels = batch['labels'].to(device)


			outputs = model(input_ids, attention_mask=attention_mask)
			logits = outputs.logits
			loss = loss_fn(logits, labels)
			total_loss += loss.item()


			# –†–∞—Å—á—ë—Ç accuracy (–¥–ª—è –º—É–ª—å—Ç–∏‚Äë–ª–µ–π–±–ª: –ø–æ—Ä–æ–≥ 0.5)
			preds = (torch.sigmoid(logits) > 0.5).float()
			correct += (preds == labels).all(dim=1).sum().item()
			total += labels.size(0)


	accuracy = correct / total
	avg_loss = total_loss / len(dataloader)


	return {
		'loss': avg_loss,
		'accuracy': accuracy
	}



def save_checkpoint(model, optimizer, epoch, loss, checkpoint_path):
	"""
	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–µ–∫–ø–æ–∏–Ω—Ç –º–æ–¥–µ–ª–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		optimizer (Optimizer): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
		epoch (int): –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏
		loss (float): –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
		checkpoint_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
	"""
	torch.save({
		'epoch': epoch,
		'model_state_dict': model.state_dict(),
		'optimizer_state_dict': optimizer.state_dict(),
		'loss': loss
	}, checkpoint_path)
	print(f"–ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {checkpoint_path}")



def load_checkpoint(model, optimizer, checkpoint_path, device):
	"""
	–ó–∞–≥—Ä—É–∂–∞–µ—Ç —á–µ–∫–ø–æ–∏–Ω—Ç –º–æ–¥–µ–ª–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å, –≤ –∫–æ—Ç–æ—Ä—É—é –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤–µ—Å–∞
		optimizer (Optimizer): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –≤ –∫–æ—Ç–æ—Ä—ã–π –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Å–æ—Å—Ç–æ—è–Ω–∏—è
		checkpoint_path (str): –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —á–µ–∫–ø–æ–∏–Ω—Ç–∞ (.pth –∏–ª–∏ .pt)
		device (torch.device): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cpu/cuda), –Ω–∞ –∫–æ—Ç–æ—Ä–æ–µ —Å–ª–µ–¥—É–µ—Ç –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

	Returns:
		int: –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–π –±—ã–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω —á–µ–∫–ø–æ–∏–Ω—Ç
	Raises:
		FileNotFoundError: –µ—Å–ª–∏ —Ñ–∞–π–ª —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω
		KeyError: –µ—Å–ª–∏ –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–∂–∏–¥–∞–µ–º—ã–µ –∫–ª—é—á–∏
		RuntimeError: –µ—Å–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π
	"""
	try:
		checkpoint = torch.load(checkpoint_path, map_location=device)

		# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–ª—é—á–µ–π
		required_keys = ['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss']
		for key in required_keys:
			if key not in checkpoint:
				raise KeyError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á '{key}' –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ")

		# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π
		model.load_state_dict(checkpoint['model_state_dict'])
		optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

		# –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
		epoch = checkpoint['epoch']
		loss = checkpoint['loss']

		print(f"–ß–µ–∫–ø–æ–∏–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: —ç–ø–æ—Ö–∞ {epoch}, loss={loss:.4f}")
		return epoch

	except FileNotFoundError:
		raise FileNotFoundError(f"–§–∞–π–ª —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {checkpoint_path}")
	except KeyError as e:
		raise KeyError(f"–û—à–∏–±–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}. "
					 "–û–∂–∏–¥–∞—é—Ç—Å—è –∫–ª—é—á–∏: 'epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'")
	except RuntimeError as e:
		raise RuntimeError(f"–û—à–∏–±–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏/–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞: {e}. "
						"–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n"
						"  - –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏\n"
						"  - –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n"
						"  - –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤–µ—Ä—Å–∏–π PyTorch")
	except Exception as e:
		raise Exception(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {type(e).__name__}: {e}")



def save_training_state(model, optimizer, scheduler, epoch, loss, metrics,
					   checkpoint_dir, filename_prefix="checkpoint"):
	"""
	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è (–º–æ–¥–µ–ª—å, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, scheduler, –º–µ—Ç—Ä–∏–∫–∏).

	Args:
		model (nn.Module): –æ–±—É—á–∞–µ–º–∞—è –º–æ–¥–µ–ª—å
		optimizer (Optimizer): —Ç–µ–∫—É—â–∏–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
		scheduler (LRScheduler): –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ learning rate
		epoch (int): –Ω–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏
		loss (float): –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å –Ω–∞ —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–µ
		metrics (dict): —Å–ª–æ–≤–∞—Ä—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, {'accuracy': 0.95})
		checkpoint_dir (str): –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
		filename_prefix (str): –ø—Ä–µ—Ñ–∏–∫—Å –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "checkpoint")


	Returns:
		str: –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–º—É —á–µ–∫–ø–æ–∏–Ω—Ç—É
	"""
	# –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —Å –Ω–æ–º–µ—Ä–æ–º —ç–ø–æ—Ö–∏
	filename = f"{filename_prefix}_epoch_{epoch}.pth"
	checkpoint_path = os.path.join(checkpoint_dir, filename)

	# –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
	checkpoint = {
		'epoch': epoch,
		'model_state_dict': model.state_dict(),
		'optimizer_state_dict': optimizer.state_dict(),
		'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
		'loss': loss,
		'metrics': metrics,
		'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
	}
	# –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
	torch.save(checkpoint, checkpoint_path)
	print(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {checkpoint_path}")
	return checkpoint_path

def load_training_state(model, optimizer, scheduler, checkpoint_path, device):
	"""
	–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–ª–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤
		optimizer (Optimizer): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
		scheduler (LRScheduler): –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ LR (–º–æ–∂–µ—Ç –±—ã—Ç—å None)
		checkpoint_path (str): –ø—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É
		device (torch.device): —Ü–µ–ª–µ–≤–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ


	Returns:
		dict: —Å–ª–æ–≤–∞—Ä—å —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏:
			- 'epoch': –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏
			- 'loss': –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
			- 'metrics': –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
			- 'timestamp': –≤—Ä–µ–º—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
	"""
	checkpoint = torch.load(checkpoint_path, map_location=device)

	model.load_state_dict(checkpoint['model_state_dict'])
	optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

	if scheduler and checkpoint['scheduler_state_dict']:
		scheduler.load_state_dict(checkpoint['scheduler_state_dict'])


	info = {
		'epoch': checkpoint['epoch'],
		'loss': checkpoint['loss'],
		'metrics': checkpoint.get('metrics', {}),
		'timestamp': checkpoint.get('timestamp', 'N/A')
	}
	print(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–æ: —ç–ø–æ—Ö–∞ {info['epoch']}, "
		  f"loss={info['loss']:.4f}, –≤—Ä–µ–º—è={info['timestamp']}")
	return info



def save_final_model(model, tokenizer, config, output_dir, model_name="final_model"):
	"""
	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤–º–µ—Å—Ç–µ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π.

	–î–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ inference.

	Args:
		model (nn.Module): –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
		tokenizer (PreTrainedTokenizer): —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
		config (dict): –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏/—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
		output_dir (str): –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
		model_name (str): –∏–º—è –º–æ–¥–µ–ª–∏ (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)


	Returns:
		str: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
	"""
	model_path = os.path.join(output_dir, model_name)

	# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å (state_dict + config)
	model.save_pretrained(model_path)

	# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
	tokenizer.save_pretrained(model_path)

	# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
	config_path = os.path.join(model_path, "training_config.json")
	with open(config_path, 'w', encoding='utf-8') as f:
		json.dump(config, f, ensure_ascii=False, indent=2)


	print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
	return model_path



def setup_training(model, train_params):
	"""
	–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è: –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, scheduler, AMP.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
		train_params (dict): –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä:
			{
				'learning_rate': 1e-4,
				'weight_decay': 0.01,
				'warmup_percent': 0.1,
				'total_steps': 1000,
				'fp16': True
			}

	Returns:
		tuple: (optimizer, scheduler, scaler)
			- optimizer: –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
			- scheduler: –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ LR
			- scaler: GradScaler –¥–ª—è AMP (–∏–ª–∏ None)
	"""
	# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (AdamW —Å weight decay)
	optimizer = AdamW(
		model.parameters(),
		lr=train_params['learning_rate'],
		weight_decay=train_params['weight_decay']
	)

	# –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ LR —Å warmup
	scheduler = get_cosine_schedule_with_warmup(
		optimizer,
		num_warmup_steps=int(train_params['warmup_percent'] * train_params['total_steps']),
		num_training_steps=train_params['total_steps']
	)

	# AMP Scaler (–µ—Å–ª–∏ fp16 –≤–∫–ª—é—á—ë–Ω)
	scaler = GradScaler() if train_params['fp16'] else None


	return optimizer, scheduler, scaler


def count_parameters(model):
	"""
	–°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å

	Returns:
		dict: —Å–ª–æ–≤–∞—Ä—å —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:
			- 'total': –æ–±—â–µ–µ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
			- 'trainable': —á–∏—Å–ª–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
	"""
	total = sum(p.numel() for p in model.parameters())
	trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
	return {'total': total, 'trainable': trainable}


def print_model_info(model, model_name="–ú–æ–¥–µ–ª—å"):
	"""
	–í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –≤ –∫–æ–Ω—Å–æ–ª—å.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		model_name (str): –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã–≤–æ–¥–∞
	"""
	params = count_parameters(model)
	print(f"\n{model_name}:")
	print(f"  –û–±—â–µ–µ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {params['total']:,}")
	print(f"  –û–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:    {params['trainable']:,}")
	print(f"  –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {type(model).__name__}")

def set_seed(seed=42):
	"""
	–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.

	Args:
		seed (int): –∑–Ω–∞—á–µ–Ω–∏–µ seed
	"""
	torch.manual_seed(seed)
	torch.cuda.manual_seed_all(seed)
	np.random.seed(seed)
	random.seed(seed)
	torch.backends.cudnn.deterministic = True
	torch.backends.cudnn.benchmark = False


def create_dataloaders(train_dataset, val_dataset, batch_size, num_workers=4):
	"""
	–°–æ–∑–¥–∞—ë—Ç DataLoader'—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.

	Args:
		train_dataset (Dataset): –æ–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç
		val_dataset (Dataset): –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
		batch_size (int): —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
		num_workers (int): —á–∏—Å–ª–æ —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö


	Returns:
		tuple: (train_loader, val_loader)
	"""
	train_loader = DataLoader(
		train_dataset,
		batch_size=batch_size,
		shuffle=True,
		num_workers=num_workers,
		pin_memory=True,
		worker_init_fn=worker_init_fn
	)

	val_loader = DataLoader(
		val_dataset,
		batch_size=batch_size,
		shuffle=False,
		num_workers=num_workers,
		pin_memory=True
	)

	return train_loader, val_loader



def worker_init_fn(worker_id):
	"""
	–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å DataLoader.

	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–∏ –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö.

	"""
	np.random.seed(np.random.get_state()[1][0] + worker_id)



def setup_device(use_cuda=True):
	"""
	–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (CPU/GPU).

	Args:
		use_cuda (bool): –µ—Å–ª–∏ True, –ø—ã—Ç–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU

	Returns:
		torch.device: –≤—ã–±—Ä–∞–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
	"""
	if use_cuda and torch.cuda.is_available():
		device = torch.device("cuda")
		print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º GPU: {torch.cuda.get_device_name(0)}")
		print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {torch.cuda.device_count()}")
	else:
		device = torch.device("cpu")
		print("–ò—Å–ø–æ–ª—å–∑—É–µ–º CPU")
	return device

def freeze_layers(model, freeze_pattern):
	"""
	–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç —Å–ª–æ–∏ –º–æ–¥–µ–ª–∏ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É —à–∞–±–ª–æ–Ω—É.

	–ü–æ–ª–µ–∑–Ω–æ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		freeze_pattern (str): —à–∞–±–ª–æ–Ω –¥–ª—è –∑–∞–º–æ—Ä–æ–∑–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "bert.encoder.layer.0.")


	–ü—Ä–∏–º–µ—Ä:
		freeze_layers(model, "bert.embeddings.")  # –∑–∞–º–æ—Ä–æ–∑–∏—Ç –≤—Å–µ —Å–ª–æ–∏ embeddings
	"""
	for name, param in model.named_parameters():
		if freeze_pattern in name:
			param.requires_grad = False
			print(f"–ó–∞–º–æ—Ä–æ–∂–µ–Ω —Å–ª–æ–π: {name}")


def unfreeze_layers(model):
	"""
	–†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç –≤—Å–µ —Å–ª–æ–∏ –º–æ–¥–µ–ª–∏.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
	"""
	for param in model.parameters():
		param.requires_grad = True
	print("–í—Å–µ —Å–ª–æ–∏ —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω—ã")

def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
	"""
	–°–æ–∑–¥–∞—ë—Ç –ª–∏–Ω–µ–π–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ LR —Å –ø—Ä–æ–≥—Ä–µ–≤–æ–º.

	Args:
		optimizer (Optimizer): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
		num_warmup_steps (int): –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –ø—Ä–æ–≥—Ä–µ–≤–∞
		num_training_steps (int): –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è

	Returns:
		LambdaLR: –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ LR
	"""
	def lr_lambda(current_step: int):
		if current_step < num_warmup_steps:
			return float(current_step) / float(max(1, num_warmup_steps))
		return max(
			0.0, float(num_training_steps - current_step) /
			float(max(1, num_training_steps - num_warmup_steps))
		)

	return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


def compute_f1_score(y_true, y_pred, threshold=0.5):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç F1-score –¥–ª—è –º—É–ª—å—Ç–∏‚Äë–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

	Args:
		y_true (torch.Tensor): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		y_pred (torch.Tensor): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –ª–æ–≥–∏—Ç—ã
		threshold (float): –ø–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

	Returns:
		float: –∑–Ω–∞—á–µ–Ω–∏–µ F1-score
	"""
	y_pred_bin = (torch.sigmoid(y_pred) > threshold).float()
	tp = (y_pred_bin * y_true).sum().item()
	fp = (y_pred_bin * (1 - y_true)).sum().item()
	fn = ((1 - y_pred_bin) * y_true).sum().item()

	precision = tp / (tp + fp) if (tp + fp) > 0 else 0
	recall = tp / (tp + fn) if (tp + fn) > 0 else 0

	f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
	return f1


def log_metrics_to_file(metrics_dict, filepath):
	"""
	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ JSON-—Ñ–∞–π–ª.

	Args:
		metrics_dict (dict): —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç—Ä–∏–∫
		filepath (str): –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
	"""
	with open(filepath, 'w', encoding='utf-8') as f:
		json.dump(metrics_dict, f, ensure_ascii=False, indent=2)
	print(f"–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filepath}")


def plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies, output_path):
	"""
	–†–∏—Å—É–µ—Ç –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è (loss –∏ accuracy).

	Args:
		train_losses (list): –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏
		val_losses (list): –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
		train_accuracies (list): —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏
		val_accuracies (list): —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
	"""
	import matplotlib.pyplot as plt

	fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

	ax1.plot(train_losses, label='Train Loss')
	ax1.plot(val_losses, label='Val Loss')
	ax1.set_title('Loss')
	ax1.legend()

	ax2.plot(train_accuracies, label='Train Accuracy')
	ax2.plot(val_accuracies, label='Val Accuracy')
	ax2.set_title('Accuracy')
	ax2.legend()

	plt.savefig(output_path)
	plt.close()
	print(f"–ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")




def evaluate_model(model, dataloader, device, threshold=0.5):
	"""
	–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º/–≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ.

	–°–æ–±–∏—Ä–∞–µ—Ç loss, accuracy, F1-score –∏ –¥—Ä—É–≥–∏–µ –º–µ—Ç—Ä–∏–∫–∏.


	Args:
		model (nn.Module): –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
		dataloader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
		device (torch.device): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
		threshold (float): –ø–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π


	Returns:
		dict: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏:
			- 'loss': —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
			- 'accuracy': —Ç–æ—á–Ω–æ—Å—Ç—å
			- 'f1_score': F1-–º–µ—Ä–∞
			- 'precision': —Ç–æ—á–Ω–æ—Å—Ç—å (precision)
			- 'recall': –ø–æ–ª–Ω–æ—Ç–∞ (recall)
	"""
	model.eval()
	total_loss = 0.0
	all_preds = []
	all_labels = []

	loss_fn = nn.BCEWithLogitsLoss()

	with torch.no_grad():
		for batch in dataloader:
			input_ids = batch['input_ids'].to(device)
			attention_mask = batch['attention_mask'].to(device)
			labels = batch['labels'].to(device)

			outputs = model(input_ids, attention_mask=attention_mask)
			logits = outputs.logits
			loss = loss_fn(logits, labels)
			total_loss += loss.item()

			# –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
			preds = (torch.sigmoid(logits) > threshold).float()

			all_preds.append(preds)
			all_labels.append(labels)

	# –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç–∫–∏
	all_preds = torch.cat(all_preds, dim=0)
	all_labels = torch.cat(all_labels, dim=0)


	# –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
	tp = (all_preds * all_labels).sum().item()
	fp = (all_preds * (1 - all_labels)).sum().item()
	fn = ((1 - all_preds) * all_labels).sum().item()
	tn = ((1 - all_preds) * (1 - all_labels)).sum().item()

	accuracy = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) > 0 else 0
	precision = tp / (tp + fp) if (tp + fp) > 0 else 0
	recall = tp / (tp + fn) if (tp + fn) > 0 else 0
	f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0


	return {
		'loss': total_loss / len(dataloader),
		'accuracy': accuracy,
		'f1_score': f1,
		'precision': precision,
		'recall': recall
	}



def predict(model, tokenizer, text, device, max_len=512, threshold=0.5):
	"""
	–î–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

	Args:
		model (nn.Module): –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
		tokenizer (PreTrainedTokenizer): —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
		text (str): –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
		device (torch.device): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
		max_len (int): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
		threshold (float): –ø–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏


	Returns:
		dict: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
			- 'probabilities': –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
			- 'predictions': –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
	"""
	model.eval()
	encoding = tokenizer(
		text,
		truncation=True,
		padding='max_length',
		max_length=max_len,
		return_tensors='pt'
	)

	input_ids = encoding['input_ids'].to(device)
	attention_mask = encoding['attention_mask'].to(device)

	with torch.no_grad():
		outputs = model(input_ids, attention_mask=attention_mask)
		logits = outputs.logits
		probs = torch.sigmoid(logits).cpu().numpy()[0]

		preds = (probs > threshold).astype(int)


	return {
		'probabilities': probs,
		'predictions': preds
	}


def create_experiment_dir(base_path, experiment_name):
	"""
	–°–æ–∑–¥–∞—ë—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —Å —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –º–µ—Ç–∫–æ–π –≤—Ä–µ–º–µ–Ω–∏.


	Args:
		base_path (str): –±–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
		experiment_name (str): –Ω–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞


	Returns:
		str: –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
	"""
	timestamp = time.strftime("%Y%m%d-%H%M%S")
	exp_dir = os.path.join(base_path, f"{experiment_name}_{timestamp}")
	os.makedirs(exp_dir, exist_ok=True)
	print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω–∞: {exp_dir}")
	return exp_dir


def save_config(config, filepath):
	"""
	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤ JSON-—Ñ–∞–π–ª.

	Args:
		config (dict): —Å–ª–æ–≤–∞—Ä—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
		filepath (str): –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
	"""
	with open(filepath, 'w', encoding='utf-8') as f:
		json.dump(config, f, ensure_ascii=False, indent=2)
	print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")


def load_config(filepath):
	"""
	–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –∏–∑ JSON-—Ñ–∞–π–ª–∞.

	Args:
		filepath (str): –ø—É—Ç—å –∫ JSON-—Ñ–∞–π–ª—É

	Returns:
		dict: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
	"""
	with open(filepath, 'r', encoding='utf-8') as f:
		config = json.load(f)
	print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}")
	return config

def get_current_time_str():
	"""
	–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è –≤ —Å—Ç—Ä–æ–∫–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.

	Returns:
		str: —Å—Ç—Ä–æ–∫–∞ —Å –¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º (—Ñ–æ—Ä–º–∞—Ç: YYYY-MM-DD_HH-MM-SS)
	"""
	return time.strftime("%Y-%m-%d_%H-%M-%S")


def setup_logging(log_file):
	"""
	–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª.

	Args:
		log_file (str): –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –ª–æ–≥–æ–≤
	"""
	logging.basicConfig(
		level=logging.INFO,
		format='%(asctime)s - %(levelname)s - %(message)s',
		handlers=[
			logging.FileHandler(log_file, encoding='utf-8'),
			logging.StreamHandler()
		]
	)
	print(f"–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ: {log_file}")

def compute_class_weights(labels):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–æ—Ä–º—É–ª—É: weight = total_samples / (n_classes * class_count)


	Args:
		labels (torch.Tensor): —Ç–µ–Ω–∑–æ—Ä –º–µ—Ç–æ–∫ (—Ñ–æ—Ä–º–∞: [n_samples, n_classes])


	Returns:
		torch.Tensor: –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
	"""
	n_samples, n_classes = labels.shape
	class_counts = labels.sum(dim=0)  # —Å—É–º–º–∞ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É
	weights = n_samples / (n_classes * class_counts)
	return weights.float()



def create_balanced_sampler(labels):
	"""
	–°–æ–∑–¥–∞—ë—Ç –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π sampler –¥–ª—è DataLoader, —á—Ç–æ–±—ã —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å—ã –≤ –±–∞—Ç—á–∞—Ö.

	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ —Å–∏–ª—å–Ω–æ–π –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ –∫–ª–∞—Å—Å–æ–≤.


	Args:
		labels (torch.Tensor): —Ç–µ–Ω–∑–æ—Ä –º–µ—Ç–æ–∫ —Ñ–æ—Ä–º—ã [n_samples, n_classes]
							 –∏–ª–∏ [n_samples] –¥–ª—è –º—É–ª—å—Ç–∏‚Äë–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

	Returns:
		WeightedRandomSampler: sampler –¥–ª—è DataLoader
	"""
	if labels.dim() == 2:  # –º—É–ª—å—Ç–∏‚Äë–ª–µ–π–±–ª
		class_counts = labels.sum(dim=0).float()
	else:  # –º—É–ª—å—Ç–∏‚Äë–∫–ª–∞—Å—Å
		class_counts = torch.bincount(labels).float()


	# –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞: —á–µ–º –º–µ–Ω—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Å–∞, —Ç–µ–º –≤—ã—à–µ –≤–µ—Å
	weights = 1.0 / class_counts
	# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
	weights /= weights.sum()

	# –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Å (–ø–æ –µ–≥–æ –∫–ª–∞—Å—Å—É)
	if labels.dim() == 2:
		sample_weights = torch.mm(labels, weights.unsqueeze(1)).squeeze(1)
	else:
		sample_weights = weights[labels]


	sampler = WeightedRandomSampler(
		weights=sample_weights,
		num_samples=len(labels),
		replacement=True
	)
	return sampler



def split_dataset(dataset, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):
	"""
	–†–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ train/val/test –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞.


	Args:
		dataset (Dataset): –∏—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
		train_ratio (float): –¥–æ–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏
		val_ratio (float): –¥–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
		test_ratio (float): –¥–æ–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
		seed (int): seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

	Returns:
		tuple: (train_subset, val_subset, test_subset)
	"""
	total_size = len(dataset)
	train_size = int(total_size * train_ratio)
	val_size = int(total_size * val_ratio)
	test_size = total_size - train_size - val_size

	indices = list(range(total_size))
	np.random.seed(seed)
	np.random.shuffle(indices)

	train_indices = indices[:train_size]
	val_indices = indices[train_size:train_size + val_size]
	test_indices = indices[train_size + val_size:]


	train_subset = Subset(dataset, train_indices)
	val_subset = Subset(dataset, val_indices)
	test_subset = Subset(dataset, test_indices)

	return train_subset, val_subset, test_subset




def collate_fn_batch_padding(batch):
	"""
	–ö–æ–ª–ª–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è DataLoader —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –ø–∞–¥–¥–∏–Ω–≥–æ–º.
	–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –≤ –±–∞—Ç—á –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –ø–∞–¥–¥–∏–Ω–≥ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –≤ –±–∞—Ç—á–µ.


	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω–æ–π.

	Args:
		batch (list): —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ (–∫–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä ‚Äî dict —Å 'input_ids', 'attention_mask', 'labels')


	Returns:
		dict: –±–∞—Ç—á —Å –ø–∞–¥–¥–∏–Ω–≥–æ–º
	"""
	max_len = max([len(item['input_ids']) for item in batch])


	padded_batch = {
		'input_ids': [],
		'attention_mask': [],
		'labels': []
	}

	for item in batch:
		pad_len = max_len - len(item['input_ids'])
		padded_input = item['input_ids'] + [0] * pad_len
		padded_mask = item['attention_mask'] + [0] * pad_len

		padded_batch['input_ids'].append(padded_input)
		padded_batch['attention_mask'].append(padded_mask)
		padded_batch['labels'].append(item['labels'])


	# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã
	padded_batch['input_ids'] = torch.tensor(padded_batch['input_ids'], dtype=torch.long)
	padded_batch['attention_mask'] = torch.tensor(padded_batch['attention_mask'], dtype=torch.long)
	padded_batch['labels'] = torch.tensor(padded_batch['labels'], dtype=torch.float)


	return padded_batch




def save_predictions(predictions, labels, output_path):
	"""
	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –≤ CSV‚Äë—Ñ–∞–π–ª.


	Args:
		predictions (torch.Tensor –∏–ª–∏ np.ndarray): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏/–∫–ª–∞—Å—Å—ã
		labels (torch.Tensor –∏–ª–∏ np.ndarray): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		output_path (str): –ø—É—Ç—å –∫ CSV‚Äë—Ñ–∞–π–ª—É
	"""
	df = pd.DataFrame({
		'predictions': predictions.flatten(),
		'true_labels': labels.flatten()
	})
	df.to_csv(output_path, index=False)
	print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")




def plot_confusion_matrix(y_true, y_pred, class_names, output_path):
	"""
	–†–∏—Å—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫ (confusion matrix) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Ñ–∞–π–ª.


	Args:
		y_true (array-like): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		y_pred (array-like): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		class_names (list): –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
	"""
	cm = confusion_matrix(y_true, y_pred)
	df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)

	plt.figure(figsize=(10, 7))
	sns.heatmap(df_cm, annot=True, fmt='d', cmap='Blues')
	plt.title('Confusion Matrix')
	plt.ylabel('True Label')
	plt.xlabel('Predicted Label')
	plt.savefig(output_path)
	plt.close()
	print(f"–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")




def calculate_roc_auc(y_true, y_scores):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç ROC‚ÄëAUC –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –∏ —Å—Ä–µ–¥–Ω–∏–π ROC‚ÄëAUC.

	–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –º—É–ª—å—Ç–∏‚Äë–ª–µ–π–±–ª –∏ –º—É–ª—å—Ç–∏‚Äë–∫–ª–∞—Å—Å–æ–≤—ã—Ö –∑–∞–¥–∞—á.


	Args:
		y_true (torch.Tensor): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ [n_samples, n_classes]
		y_scores (torch.Tensor): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ [n_samples, n_classes]

	Returns:
		dict: {'class_auc': —Å–ø–∏—Å–æ–∫ AUC –ø–æ –∫–ª–∞—Å—Å–∞–º, 'macro_auc': —Å—Ä–µ–¥–Ω–∏–π AUC}
	"""
	y_true = y_true.cpu().numpy()
	y_scores = y_scores.cpu().numpy()

	n_classes = y_true.shape[1]
	class_auc = []

	for i in range(n_classes):
		auc = roc_auc_score(y_true[:, i], y_scores[:, i])
		class_auc.append(auc)


	macro_auc = np.mean(class_auc)
	return {'class_auc': class_auc, 'macro_auc': macro_auc}




def early_stopping(monitor_value, best_value, patience, counter, mode='min'):
	"""
	–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å–ª–æ–≤–∏–µ —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–±—É—á–µ–Ω–∏—è.


	Args:
		monitor_value (float): —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, val_loss)
		best_value (float): –ª—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
		patience (int): –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–æ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
		counter (int): —Å—á—ë—Ç—á–∏–∫ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
		mode (str): 'min' ‚Äî –∏—â–µ–º –º–∏–Ω–∏–º—É–º (loss), 'max' ‚Äî –º–∞–∫—Å–∏–º—É–º (accuracy)


	Returns:
		bool: True, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ
		float: –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–µ best_value
		int: –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π counter
	"""
	improvement = False

	if mode == 'min':
		if monitor_value < best_value:
			best_value = monitor_value
			counter = 0
			improvement = True
		else:
			counter += 1
	elif mode == 'max':
		if monitor_value > best_value:
			best_value = monitor_value
			counter = 0
			improvement = True
		else:
			counter += 1

	stop_training = counter >= patience
	return stop_training, best_value, counter





def calculate_precision_recall_f1(y_true, y_pred, average='macro'):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç precision, recall –∏ F1‚Äëscore –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.


	Args:
		y_true (array-like): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		y_pred (array-like): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		average (str): —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è ('macro', 'micro', 'weighted')


	Returns:
		dict: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ {'precision', 'recall', 'f1_score'}
	"""
	precision = precision_score(y_true, y_pred, average=average)
	recall = recall_score(y_true, y_pred, average=average)
	f1 = f1_score(y_true, y_pred, average=average)

	return {
		'precision': precision,
		'recall': recall,
		'f1_score': f1
	}



def generate_classification_report(y_true, y_pred, class_names=None):
	"""
	–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–≤–∫–ª—é—á–∞—è precision, recall, F1, support).


	Args:
		y_true (array-like): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		y_pred (array-like): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		class_names (list): –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)


	Returns:
		str: —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç (–∫–∞–∫ –≤ sklearn.classification_report)
	"""
	report = classification_report(
		y_true,
		y_pred,
		target_names=class_names,
		output_dict=False
	)
	return report

def plot_learning_rate(lr_history, output_path):
	"""
	–†–∏—Å—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è learning rate –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è.


	Args:
		lr_history (list): —Å–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π LR –ø–æ —à–∞–≥–∞–º/—ç–ø–æ—Ö–∞–º
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
	"""
	plt.figure(figsize=(10, 6))
	plt.plot(lr_history, label='Learning Rate')
	plt.title('Learning Rate Schedule')
	plt.xlabel('Steps/Epochs')
	plt.ylabel('LR')
	plt.grid(True)
	plt.savefig(output_path)
	plt.close()
	print(f"–ì—Ä–∞—Ñ–∏–∫ LR —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")


def save_model_onnx(model, dummy_input, filepath, input_names=None, output_names=None):
	"""
	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ ONNX.

	–ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –≤–Ω–µ PyTorch (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ C++, Java, JavaScript).


	Args:
		model (nn.Module): –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
		dummy_input (torch.Tensor): —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä –¥–ª—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏
		filepath (str): –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É (.onnx)
		input_names (list): –∏–º–µ–Ω–∞ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤
		output_names (list): –∏–º–µ–Ω–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤
	"""
	model.eval()
	torch.onnx.export(
		model,
		dummy_input,
		filepath,
		export_params=True,
		opset_version=11,
		do_constant_folding=True,
		input_names=input_names or ['input'],
		output_names=output_names or ['output'],
		dynamic_axes={
			'input': {0: 'batch_size'},
			'output': {0: 'batch_size'}
		}
	)
	print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ONNX: {filepath}")


def load_onnx_model(filepath):
	"""
	–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å ONNX (–¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞).

	–¢—Ä–µ–±—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å onnxruntime: `pip install onnxruntime`


	Args:
		filepath (str): –ø—É—Ç—å –∫ ONNX-—Ñ–∞–π–ª—É


	Returns:
		InferenceSession: —Å–µ—Å—Å–∏—è –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
	"""
	import onnxruntime as ort
	session = ort.InferenceSession(filepath)
	return session

def run_inference_onnx(session, input_data):
	"""
	–í—ã–ø–æ–ª–Ω—è–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ ONNX-–º–æ–¥–µ–ª–∏.


	Args:
		session (InferenceSession): —Å–µ—Å—Å–∏—è ONNX
		input_data (np.ndarray): –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ


	Returns:
		np.ndarray: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
	"""
	input_name = session.get_inputs()[0].name
	output = session.run(None, {input_name: input_data})
	return output[0]

def create_tensorboard_logger(log_dir):
	"""
	–°–æ–∑–¥–∞—ë—Ç –ª–æ–≥–≥–µ—Ä TensorBoard.


	Args:
		log_dir (str): –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ª–æ–≥–æ–≤ TensorBoard


	Returns:
		SummaryWriter: –æ–±—ä–µ–∫—Ç –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ TensorBoard
	"""
	from torch.utils.tensorboard import SummaryWriter
	writer = SummaryWriter(log_dir=log_dir)
	print(f"TensorBoard –ª–æ–≥–≥–µ—Ä —Å–æ–∑–¥–∞–Ω: {log_dir}")
	return writer


def log_scalar_to_tensorboard(writer, tag, value, step):
	"""
	–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Å–∫–∞–ª—è—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ TensorBoard.


	Args:
		writer (SummaryWriter): –ª–æ–≥–≥–µ—Ä TensorBoard
		tag (str): –º–µ—Ç–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'train/loss')
		value (float): –∑–Ω–∞—á–µ–Ω–∏–µ
		step (int): —à–∞–≥/—ç–ø–æ—Ö–∞
	"""
	writer.add_scalar(tag, value, step)


def log_histogram_to_tensorboard(writer, tag, values, step, bins='auto'):
	"""
	–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É –∑–Ω–∞—á–µ–Ω–∏–π –≤ TensorBoard.

	Args:
		writer (SummaryWriter): –ª–æ–≥–≥–µ—Ä TensorBoard
		tag (str): –º–µ—Ç–∫–∞
		values (array-like): –º–∞—Å—Å–∏–≤ –∑–Ω–∞—á–µ–Ω–∏–π
		step (int): —à–∞–≥/—ç–ø–æ—Ö–∞
		bins (str –∏–ª–∏ int): –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏–Ω–æ–≤ –¥–ª—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã
	"""
	writer.add_histogram(tag, values, step, bins=bins)

def log_embedding_to_tensorboard(writer, features, metadata, step):
	"""
	–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ TensorBoard (–¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏).


	Args:
		writer (SummaryWriter): –ª–æ–≥–≥–µ—Ä TensorBoard
		features (torch.Tensor –∏–ª–∏ np.ndarray): —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
		metadata (list): –º–µ—Ç–∫–∏ –¥–ª—è —Ç–æ—á–µ–∫
		step (int): —à–∞–≥/—ç–ø–æ—Ö–∞
	"""
	writer.add_embedding(features, metadata=metadata, global_step=step)

def set_deterministic_mode():
	"""
	–í–∫–ª—é—á–∞–µ—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º PyTorch (–¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏).

	–í–Ω–∏–º–∞–Ω–∏–µ: –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è!
	"""
	torch.backends.cudnn.deterministic = True
	torch.backends.cudnn.benchmark = False
	torch.use_deterministic_algorithms(True)
	print("–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º –≤–∫–ª—é—á—ë–Ω")

def count_trainable_params(model):
	"""
	–°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å

	Returns:
		int: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
	"""
	return sum(p.numel() for p in model.parameters() if p.requires_grad)


def print_model_summary(model, input_size, batch_size=-1, device="cuda"):
	"""
	–ü–µ—á–∞—Ç–∞–µ—Ç —Å–≤–æ–¥–∫—É –º–æ–¥–µ–ª–∏ (–∫–∞–∫ –≤ Keras).

	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç torchsummary.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		input_size (tuple): —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ (–±–µ–∑ batch_size)
		batch_size (int): —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é -1)
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ('cuda' –∏–ª–∏ 'cpu')
	"""
	try:
		from torchsummary import summary
		summary(model, input_size, batch_size=batch_size, device=device)
	except ImportError:
		print("torchsummary –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torchsummary")




def compute_perplexity(logits, labels):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–ø–æ –ª–æ—Å—Å–∞–º).


	–ü–µ—Ä–ø–ª–µ–∫—Å–∏—è ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.

	Perplexity = exp(avg_negative_log_likelihood)


	Args:
		logits (torch.Tensor): –ª–æ–≥–∏—Ç—ã –º–æ–¥–µ–ª–∏ [batch_size, seq_len, vocab_size]
		labels (torch.Tensor): –∏—Å—Ç–∏–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã [batch_size, seq_len]


	Returns:
		float: –∑–Ω–∞—á–µ–Ω–∏–µ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏
	"""
	loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)
	shift_logits = logits[..., :-1, :].contiguous()
	shift_labels = labels[..., 1:].contiguous()


	# –ü—Ä–∏–º–µ–Ω—è–µ–º loss
	loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
	perplexity = torch.exp(loss)
	return perplexity.item()



def calculate_bleu_score(references, hypotheses, n_grams=4):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç BLEU‚Äëscore –¥–ª—è –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä–µ–≤–æ–¥–∞, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏).


	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç nltk.bleu_score.


	Args:
		references (list of list of str): —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ (–∏—Å—Ç–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
		hypotheses (list of str): —Å–ø–∏—Å–æ–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (—É–∂–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö)
		n_grams (int): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ n‚Äë–≥—Ä–∞–º–º –¥–ª—è —É—á—ë—Ç–∞


	Returns:
		float: BLEU‚Äëscore (–æ—Ç 0 –¥–æ 1)
	"""
	from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
	smoothing = SmoothingFunction()


	scores = []
	for ref, hyp in zip(references, hypotheses):
		score = sentence_bleu(
			[ref],
			hyp,
			weights=[1/n_grams] * n_grams,
			smoothing_function=smoothing.method1
		)
		scores.append(score)

	return sum(scores) / len(scores)

def calculate_rouge_scores(references, hypotheses):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç ROUGE‚Äë–æ—Ü–µ–Ω–∫–∏ (ROUGE‚Äë1, ROUGE‚Äë2, ROUGE‚ÄëL) –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏/–ø–µ—Ä–µ–≤–æ–¥–∞.


	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç rouge‚Äëscore –±–∏–±–ª–∏–æ—Ç–µ–∫—É.

	Args:
		references (list of str): –∏—Å—Ç–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
		hypotheses (list of str): —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã


	Returns:
		dict: {'rouge1', 'rouge2', 'rougeL'} ‚Äî —Å–ª–æ–≤–∞—Ä–∏ —Å precision, recall, f1
	"""
	try:
		from rouge_score import rouge_scorer
		scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)


		scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}

		for ref, hyp in zip(references, hypotheses):
			score = scorer.score(ref, hyp)
			for key in scores:
				scores[key].append(score[key])


		# –°—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º –ø—Ä–∏–º–µ—Ä–∞–º
		avg_scores = {}
		for key in scores:
			p = np.mean([s.precision for s in scores[key]])
			r = np.mean([s.recall for s in scores[key]])
			f = np.mean([s.fmeasure for s in scores[key]])
			avg_scores[key] = {'precision': p, 'recall': r, 'f1': f}


		return avg_scores
	except ImportError:
		print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ rouge-score: pip install rouge-score")
		return {}

def tokenize_texts(texts, tokenizer, max_length=512, padding=True, truncation=True):
	"""
	–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.


	Args:
		texts (list of str): —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
		tokenizer (PreTrainedTokenizer): —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
		max_length (int): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
		padding (bool): –¥–æ–±–∞–≤–ª—è—Ç—å –ø–∞–¥–¥–∏–Ω–≥
		truncation (bool): –æ–±—Ä–µ–∑–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã


	Returns:
		dict: –≤—ã—Ö–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (input_ids, attention_mask –∏ —Ç.–¥.)
	"""
	encoded = tokenizer(
		texts,
		max_length=max_length,
		padding=padding,
		truncation=truncation,
		return_tensors='pt'
	)
	return encoded

def extract_features(model, dataloader, device, layer_name=None):
	"""
	–ò–∑–≤–ª–µ–∫–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏/–ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∑–∞–¥–∞–Ω–Ω–æ–π —Å–ª–æ–∏ –º–æ–¥–µ–ª–∏.


	–ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –∞–Ω–∞–ª–∏–∑–∞.

	–ï—Å–ª–∏ layer_name –Ω–µ —É–∫–∞–∑–∞–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—ã—Ö–æ–¥ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		dataloader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
		device (torch.device): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
		layer_name (str): –∏–º—è —Å–ª–æ—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)


	Returns:
		torch.Tensor: –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ [n_samples, feature_dim]
	"""
	model.eval()
	features = []

	with torch.no_grad():
		for batch in dataloader:
			input_ids = batch['input_ids'].to(device)
			attention_mask = batch['attention_mask'].to(device)

			# –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Å–ª–æ–π ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º forward hook
			if layer_name:
				activation = {}
				def hook(module, input, output):
					activation[layer_name] = output

				# –ù–∞—Ö–æ–¥–∏–º —Å–ª–æ–π –ø–æ –∏–º–µ–Ω–∏
				layer = dict(model.named_modules())[layer_name]
				handle = layer.register_forward_hook(hook)

				outputs = model(input_ids, attention_mask=attention_mask)
				handle.remove()
				feat = activation[layer_name]
			else:
				# –ò–Ω–∞—á–µ ‚Äî –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏
				outputs = model(input_ids, attention_mask=attention_mask)
				feat = outputs.last_hidden_state  # –∏–ª–∏ logits, –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–æ–¥–µ–ª–∏

			features.append(feat.cpu())

	return torch.cat(features, dim=0)

def visualize_embeddings(embeddings, labels, class_names=None, output_path=None):
	"""
	–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –ø–æ–º–æ—â—å—é PCA/t‚ÄëSNE.

	–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ –∫–ª–∞—Å—Å–∞–º.

	Args:
		embeddings (np.ndarray –∏–ª–∏ torch.Tensor): —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ [n_samples, dim]
		labels (array-like): –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ [n_samples]
		classnames (list): –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
	"""
	import matplotlib.pyplot as plt
	from sklearn.decomposition import PCA
	from sklearn.manifold import TSNE

	# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy
	if isinstance(embeddings, torch.Tensor):
		embeddings = embeddings.numpy()
	if isinstance(labels, torch.Tensor):
		labels = labels.numpy()


	# –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–æ 2D
	pca = PCA(n_components=2)
	embeddings_2d = pca.fit_transform(embeddings)

	plt.figure(figsize=(10, 8))
	scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)

	if classnames:
		plt.legend(*scatter.legend_elements(), title="Classes", labels=classnames)
	else:
		plt.legend(*scatter.legend_elements(), title="Classes")


	plt.title("Embeddings Visualization (PCA)")
	plt.xlabel("PC1")
	plt.ylabel("PC2")

	if output_path:
		plt.savefig(output_path, dpi=300, bbox_inches='tight')
		print(f"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
	plt.show()


def save_predictions_json(predictions, output_path):
	"""
	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ JSON‚Äë—Ñ–∞–π–ª (—É–¥–æ–±–Ω–æ –¥–ª—è –ø–æ—Å—Ç‚Äë–æ–±—Ä–∞–±–æ—Ç–∫–∏/—Å—É–±–º–∏—Å—Å–∏–∏).


	Args:
		predictions (list of dict): —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
		output_path (str): –ø—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É JSON‚Äë—Ñ–∞–π–ª—É
	"""
	with open(output_path, 'w', encoding='utf-8') as f:
		json.dump(predictions, f, ensure_ascii=False, indent=2)
	print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ JSON: {output_path}")



def load_predictions_json(input_path):
	"""
	–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–∑ JSON‚Äë—Ñ–∞–π–ª–∞.


	Args:
		input_path (str): –ø—É—Ç—å –∫ JSON‚Äë—Ñ–∞–π–ª—É —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏


	Returns:
		list of dict: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏
	"""
	with open(input_path, 'r', encoding='utf-8') as f:
		predictions = json.load(f)
	print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ JSON: {input_path}")
	return predictions

def calculate_accuracy_per_class(y_true, y_pred, class_names=None):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å (accuracy) –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞.


	–î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞:
	  accuracy = (TP + TN) / (TP + TN + FP + FN)
	–ù–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ—â–µ —Å—á–∏—Ç–∞—Ç—å –∫–∞–∫:
	  –¥–æ–ª—è –≤–µ—Ä–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞


	Args:
		y_true (array-like): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		y_pred (array-like): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		classnames (list of str): –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)


	Returns:
		dict: {–∫–ª–∞—Å—Å: —Ç–æ—á–Ω–æ—Å—Ç—å} –∏–ª–∏ {–∏–Ω–¥–µ–∫—Å_–∫–ª–∞—Å—Å–∞: —Ç–æ—á–Ω–æ—Å—Ç—å}
	"""
	from collections import defaultdict

	if classnames is None:
		classnames = sorted(set(y_true))

	acc_per_class = {}
	for cls in classnames:
		idx = (np.array(y_true) == cls)
		if np.sum(idx) == 0:
			acc_per_class[cls] = 0.0
		else:
			correct = (np.array(y_pred)[idx] == cls).sum()
			total = idx.sum()
			acc_per_class[cls] = correct / total


	return acc_per_class

def plot_loss_curves(train_losses, val_losses, output_path):
	"""
	–†–∏—Å—É–µ—Ç –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏.


	Args:
		train_losses (list of float): –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏
		val_losses (list of float): –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Ç–µ—Ä—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
	"""
	plt.figure(figsize=(10, 6))
	epochs = range(1, len(train_losses) + 1)
	plt.plot(epochs, train_losses, 'b-', label='Train Loss')
	plt.plot(epochs, val_losses, 'r--', label='Val Loss')
	plt.title('Training and Validation Loss')
	plt.xlabel('Epoch')
	plt.ylabel('Loss')
	plt.legend()
	plt.grid(True)
	plt.savefig(output_path, dpi=300, bbox_inches='tight')
	plt.close()
	print(f"–ö—Ä–∏–≤—ã–µ –ø–æ—Ç–µ—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")


def plot_accuracy_curves(train_accs, val_accs, output_path):
	"""
	–†–∏—Å—É–µ—Ç –∫—Ä–∏–≤—ã–µ —Ç–æ—á–Ω–æ—Å—Ç–∏: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π.


	Args:
		train_accs (list of float): —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏
		val_accs (list of float): —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
	"""
	plt.figure(figsize=(10, 6))
	epochs = range(1, len(train_accs) + 1)
	plt.plot(epochs, train_accs, 'g-', label='Train Accuracy')
	plt.plot(epochs, val_accs, 'm--', label='Val Accuracy')
	plt.title('Training and Validation Accuracy')
	plt.xlabel('Epoch')
	plt.ylabel('Accuracy')
	plt.legend()
	plt.grid(True)
	plt.savefig(output_path, dpi=300, bbox_inches='tight')
	plt.close()
	print(f"–ö—Ä–∏–≤—ã–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")


def extract_top_k_predictions(logits, k=5, tokenizer=None):
	"""
	–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–ø‚Äëk –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤/–∫–ª–∞—Å—Å–æ–≤ –ø–æ –ª–æ–≥–∏—Ç–∞–º.


	–ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

	Args:
		logits (torch.Tensor): –ª–æ–≥–∏—Ç—ã –º–æ–¥–µ–ª–∏ [batch_size, vocab_size] –∏–ª–∏ [batch_size, n_classes]
		k (int): –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø‚Äë–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
		tokenizer (PreTrainedTokenizer): —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤)


	Returns:
		tuple: (top_values, top_indices, top_tokens)
			- top_values: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–ø‚Äëk [batch_size, k]
			- top_indices: –∏–Ω–¥–µ–∫—Å—ã —Ç–æ–ø‚Äëk [batch_size, k]
			- top_tokens: –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (–µ—Å–ª–∏ tokenizer –∑–∞–¥–∞–Ω)
	"""
	probs = torch.softmax(logits, dim=-1)
	top_values, top_indices = torch.topk(probs, k, dim=-1)

	if tokenizer:
		top_tokens = []
		for row in top_indices:
			tokens = [tokenizer.decode([idx]) for idx in row]
			top_tokens.append(tokens)
		return top_values, top_indices, top_tokens
	else:
		return top_values, top_indices, None

def log_predictions_sample(predictions, labels, tokenizer, output_path, max_samples=10):
	"""
	–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤ —Ñ–∞–π–ª –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (—Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.


	Args:
		predictions (torch.Tensor –∏–ª–∏ list): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
		labels (torch.Tensor –∏–ª–∏ list): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		tokenizer (PreTrainedTokenizer): —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
		output_path (str): –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –ª–æ–≥–∞
		max_samples (int): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∑–∞–ø–∏—Å–∏
	"""
	predictions = predictions[:max_samples]
	labels = labels[:max_samples]

	with open(output_path, 'w', encoding='utf-8') as f:
		f.write("–ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏\n")
		f.write("=" * 50 + "\n")
		for i, (pred, label) in enumerate(zip(predictions, labels)):
			if isinstance(pred, torch.Tensor):
				pred = pred.item()
			if isinstance(label, torch.Tensor):
				label = label.item()

			pred_token = tokenizer.decode([pred]) if tokenizer else str(pred)
			label_token = tokenizer.decode([label]) if tokenizer else str(label)


			f.write(f"–ü—Ä–∏–º–µ—Ä {i+1}:\n")
			f.write(f"  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: {pred_token} (id={pred})\n")
			f.write(f"  –ò—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {label_token} (id={label})\n")
			f.write("-" * 30 + "\n")

	print(f"–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∑–∞–ø–∏—Å–∞–Ω—ã: {output_path}")


def compute_confidence_metrics(probs, labels, threshold=0.5):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏.

	- –î–æ–ª—è —É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
	- –î–æ–ª—è –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
	- –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö/–æ—à–∏–±–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö


	Args:
		probs (torch.Tensor): –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π [n_samples, n_classes]
		labels (torch.Tensor): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ [n_samples]
		threshold (float): –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 0.5)


	Returns:
		dict: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
	"""
	preds = torch.argmax(probs, dim=1)
	confidences = torch.max(probs, dim=1).values


	correct = (preds == labels)
	conf_correct = confidences[correct]
	conf_incorrect = confidences[~correct]


	# –î–æ–ª—è —É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å > threshold)
	confident_correct = (conf_correct > threshold).float().mean().item()


	# –î–æ–ª—è –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å <= threshold)
	unconfident_errors = (conf_incorrect <= threshold).float().mean().item()

	# –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
	mean_conf_correct = conf_correct.mean().item() if len(conf_correct) > 0 else 0.0
	# –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—à–∏–±–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
	mean_conf_incorrect = conf_incorrect.mean().item() if len(conf_incorrect) > 0 else 0.0


	return {
		'confident_correct': confident_correct,
		'unconfident_errors': unconfident_errors,
		'mean_conf_correct': mean_conf_correct,
		'mean_conf_incorrect': mean_conf_incorrect,
		'accuracy': correct.float().mean().item()
	}


def analyze_prediction_uncertainty(probs, method='entropy'):
	"""
	–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏.

	–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–¥–æ–≤:
	  - entropy: —ç–Ω—Ç—Ä–æ–ø–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
	  - variance: –¥–∏—Å–ø–µ—Ä—Å–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∫–ª–∞—Å—Å–æ–≤
	  - margin: —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Ç–æ–ø‚Äë1 –∏ —Ç–æ–ø‚Äë2 –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏


	Args:
		probs (torch.Tensor): –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π [n_samples, n_classes]
		method (str): –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ ('entropy', 'variance', 'margin')

	Returns:
		torch.Tensor: —Å–∫–∞–ª—è—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
	"""
	if method == 'entropy':
		# –≠–Ω—Ç—Ä–æ–ø–∏—è: -sum(p * log(p))
		eps = 1e-10
		entropy = -torch.sum(probs * torch.log(probs + eps), dim=1)
		return entropy
	elif method == 'variance':
		# –î–∏—Å–ø–µ—Ä—Å–∏—è –ø–æ –∫–ª–∞—Å—Å–∞–º
		variance = torch.var(probs, dim=1)
		return variance
	elif method == 'margin':
		# –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Ç–æ–ø‚Äë1 –∏ —Ç–æ–ø‚Äë2 –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
		top2_probs = torch.topk(probs, 2, dim=1).values
		margin = top2_probs[:, 0] - top2_probs[:, 1]
		return 1 - margin  # –ß–µ–º –º–µ–Ω—å—à–µ margin, —Ç–µ–º –≤—ã—à–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å
	else:
		raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {method}")


def detect_out_of_distribution(probs, threshold=0.1):
	"""
	–í—ã—è–≤–ª—è–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (out‚Äëof‚Äëdistribution, OOD).

	–ö—Ä–∏—Ç–µ—Ä–∏–∏: –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (max prob < threshold) –∏–ª–∏ –≤—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è.


	Args:
		probs (torch.Tensor): –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π [n_samples, n_classes]
		threshold (float): –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è OOD

	Returns:
		torch.BoolTensor: –º–∞—Å–∫–∞ OOD‚Äë–ø—Ä–∏–º–µ—Ä–æ–≤ [n_samples]
	"""
	max_probs = probs.max(dim=1).values
	entropy = analyze_prediction_uncertainty(probs, method='entropy')


	# –ö—Ä–∏—Ç–µ—Ä–∏–∏ OOD: –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ò–õ–ò –≤—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è
	ood_mask = (max_probs < threshold) | (entropy > 2.0)  # —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
	return ood_mask

def evaluate_calibration(probs, labels, n_bins=10):
	"""
	–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞–ª–∏–±—Ä–æ–≤–∫—É –º–æ–¥–µ–ª–∏ (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏).

	–í—ã—á–∏—Å–ª—è–µ—Ç Expected Calibration Error (ECE) –∏ —Ä–∏—Å—É–µ—Ç –¥–∏–∞–≥—Ä–∞–º–º—É –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏.


	Args:
		probs (torch.Tensor): –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π [n_samples, n_classes]
		labels (torch.Tensor): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ [n_samples]
		n_bins (int): —á–∏—Å–ª–æ –±–∏–Ω–æ–≤ –¥–ª—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã


	Returns:
		dict: {'ece': ECE, 'bin_accuracies': ..., 'bin_confidences': ...}
	"""
	preds = torch.argmax(probs, dim=1)
	confidences = torch.max(probs, dim=1).values

	correct = (preds == labels).float()


	# –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∏–Ω—ã –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
	bin_boundaries = torch.linspace(0, 1, n_bins + 1)
	bin_lows = bin_boundaries[:-1]
	bin_highs = bin_boundaries[1:]


	bin_accuracies = []
	bin_confidences = []
	bin_counts = []

	for low, high in zip(bin_lows, bin_highs):
		mask = (confidences >= low) & (confidences < high)
		if mask.sum() > 0:
			acc = correct[mask].mean().item()
			conf = confidences[mask].mean().item()
			count = mask.sum().item()
		else:
			acc, conf, count = 0.0, 0.0, 0
		bin_accuracies.append(acc)
		bin_confidences.append(conf)
		bin_counts.append(count)


	# ECE: –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ |accuracy - confidence|
	total_count = sum(bin_counts)
	ece = sum(
		abs(acc - conf) * count for acc, conf, count in zip(bin_accuracies, bin_confidences, bin_counts)
	) / total_count if total_count > 0 else 0.0


	return {
		'ece': ece,
		'bin_accuracies': bin_accuracies,
		'bin_confidences': bin_confidences,
		'bin_counts': bin_counts
	}

def plot_reliability_diagram(bin_accuracies, bin_confidences, output_path):
	"""
	–†–∏—Å—É–µ—Ç –¥–∏–∞–≥—Ä–∞–º–º—É –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ (reliability diagram) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏.

	Args:
		bin_accuracies (list of float): —Ç–æ—á–Ω–æ—Å—Ç—å –≤ –±–∏–Ω–∞—Ö
		bin_confidences (list of float): —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –±–∏–Ω–∞—Ö
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
	"""
	plt.figure(figsize=(8, 8))
	plt.bar(bin_confidences, bin_accuracies, width=0.1, alpha=0.7, label='Accuracy')
	plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')
	plt.xlabel('Confidence')
	plt.ylabel('Accuracy')
	plt.title('Reliability Diagram')
	plt.legend()
	plt.grid(True)
	plt.savefig(output_path, dpi=300, bbox_inches='tight')
	plt.close()
	print(f"–î–∏–∞–≥—Ä–∞–º–º–∞ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")


def compute_shap_values(model, background_data, test_data, feature_names=None):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç SHAP‚Äë–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏.

	–¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏: pip install shap

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		background_data (torch.Tensor): —Ñ–æ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–∞—Ä–∏—Ü–µ–Ω—Ç—Ä–∞
		test_data (torch.Tensor): —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
		featurenames (list of str): –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)


	Returns:
		np.ndarray: SHAP‚Äë–∑–Ω–∞—á–µ–Ω–∏—è [n_test, n_features]
	"""
	try:
		import shap

		# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy
		background = background_data.cpu().numpy()
		test = test_data.cpu().numpy()

		# –°–æ–∑–¥–∞—ë–º explainer
		explainer = shap.DeepExplainer(model, background)
		shap_values = explainer.shap_values(test)

		return shap_values
	except ImportError:
		print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ shap: pip install shap")
		return None

def log_model_metadata(model, optimizer, epoch, metrics, output_path):
	"""
	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –º–µ—Ç—Ä–∏–∫–∏, —Å–æ—Å—Ç–æ—è–Ω–∏–µ) –≤ JSON.

	–ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		optimizer (torch.optim.Optimizer): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
		epoch (int): —Ç–µ–∫—É—â–∞—è —ç–ø–æ—Ö–∞
		metrics (dict): —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
		output_path (str): –ø—É—Ç—å –∫ JSON‚Äë—Ñ–∞–π–ª—É
	"""
		metadata = {
			'model_name': model.__class__.__name__,
			'model_config': str(model),
			'optimizer_name': optimizer.__class__.__name__,
			'optimizer_state': optimizer.state_dict(),
			'epoch': epoch,
			'metrics': metrics,
			'timestamp': datetime.now().isoformat(),
			'torch_version': torch.__version__,
			'device': str(next(model.parameters()).device)
		}

		with open(output_path, 'w', encoding='utf-8') as f:
			json.dump(metadata, f, ensure_ascii=False, indent=2)
		print(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")


	def load_model_metadata(input_path):
		"""
		–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ JSON‚Äë—Ñ–∞–π–ª–∞.


		Args:
			input_path (str): –ø—É—Ç—å –∫ JSON‚Äë—Ñ–∞–π–ª—É —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏


		Returns:
			dict: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª–∏
		"""
		with open(input_path, 'r', encoding='utf-8') as f:
			metadata = json.load(f)
		print(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {input_path}")
		return metadata

	def freeze_model_layers(model, layer_names):
		"""
		–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–µ —Å–ª–æ–∏ –º–æ–¥–µ–ª–∏ (–æ—Ç–∫–ª—é—á–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã).


		–ü–æ–ª–µ–∑–Ω–æ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å —Ä–∞–Ω–Ω–∏–µ —Å–ª–æ–∏, –æ–±—É—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ.


		Args:
			model (nn.Module): –º–æ–¥–µ–ª—å
			layer_names (list of str): –∏–º–µ–Ω–∞ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º—ã—Ö —Å–ª–æ—ë–≤
		"""
		for name, param in model.named_parameters():
			if any(ln in name for ln in layer_names):
				param.requires_grad = False
		print(f"–ó–∞–º–æ—Ä–æ–∂–µ–Ω—ã —Å–ª–æ–∏: {layer_names}")


	def unfreeze_model_layers(model, layer_names=None):
		"""
		–†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç —Å–ª–æ–∏ –º–æ–¥–µ–ª–∏ (–≤–∫–ª—é—á–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã).

		–ï—Å–ª–∏ layer_names –Ω–µ —É–∫–∞–∑–∞–Ω ‚Äî —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç –≤—Å–µ —Å–ª–æ–∏.

		Args:
			model (nn.Module): –º–æ–¥–µ–ª—å
			layer_names (list of str, optional): –∏–º–µ–Ω–∞ —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º—ã—Ö —Å–ª–æ—ë–≤
		"""
		if layer_names is None:
			for param in model.parameters():
				param.requires_grad = True
			print("–í—Å–µ —Å–ª–æ–∏ —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω—ã")
		else:
			for name, param in model.named_parameters():
				if any(ln in name for ln in layer_names):
					param.requires_grad = True
			print(f"–†–∞–∑–º–æ—Ä–æ–∂–µ–Ω—ã —Å–ª–æ–∏: {layer_names}")


	def count_model_params_by_layer(model):
		"""
		–°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Å–ª–æ—è–º –º–æ–¥–µ–ª–∏.

		–ü–æ–º–æ–≥–∞–µ—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏ –≤—ã—è–≤–ª—è—Ç—å ¬´—Ç—è–∂—ë–ª—ã–µ¬ª —Å–ª–æ–∏.


		Args:
			model (nn.Module): –º–æ–¥–µ–ª—å

		Returns:
			dict: {–∏–º—è_—Å–ª–æ—è: —á–∏—Å–ª–æ_–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤}
		"""
		param_counts = {}
		for name, module in model.named_modules():
			if len(list(module.parameters())) > 0:  # –µ—Å–ª–∏ —É –º–æ–¥—É–ª—è –µ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
				count = sum(p.numel() for p in module.parameters() if p.requires_grad)
				param_counts[name] = count
		return param_counts


	def print_model_param_summary(model):
		"""
		–ü–µ—á–∞—Ç–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –º–æ–¥–µ–ª–∏ (–ø–æ —Å–ª–æ—è–º).

		Args:
			model (nn.Module): –º–æ–¥–µ–ª—å
		"""
		param_counts = count_model_params_by_layer(model)
		total = sum(param_counts.values())

		print("–°–≤–æ–¥–∫–∞ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –º–æ–¥–µ–ª–∏:")
		print("-" * 60)
		for name, count in param_counts.items():
			print(f"{name:30} : {count:>12} params")
		print("-" * 60)
		print(f"–ò—Ç–æ–≥–æ: {total} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")


	def save_checkpoint(model, optimizer, epoch, metrics, output_path):
		"""
		–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–µ–∫–ø–æ–π–Ω—Ç –º–æ–¥–µ–ª–∏ (–≤–µ—Å–∞ + —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ + –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ).


		Args:
			model (nn.Module): –º–æ–¥–µ–ª—å
			optimizer (torch.optim.Optimizer): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
			epoch (int): –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏
			metrics (dict): –º–µ—Ç—Ä–∏–∫–∏
			output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–π–Ω—Ç–∞ (.pth)
		"""
		checkpoint = {
			'model_state_dict': model.state_dict(),
			'optimizer_state_dict': optimizer.state_dict(),
			'epoch': epoch,
			'metrics': metrics,
			'timestamp': datetime.now().isoformat()
		}
		torch.save(checkpoint, output_path)
		print(f"–ß–µ–∫–ø–æ–π–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")


	def load_checkpoint(model, optimizer, input_path, device='cpu'):
		"""
		–ó–∞–≥—Ä—É–∂–∞–µ—Ç —á–µ–∫–ø–æ–π–Ω—Ç –º–æ–¥–µ–ª–∏ –∏ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ.


		Args:
			model (nn.Module): –º–æ–¥–µ–ª—å (–±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∞)
			optimizer (torch.optim.Optimizer): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (–±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω)
			input_path (str): –ø—É—Ç—å –∫ —á–µ–∫–ø–æ–π–Ω—Ç—É (.pth)
			device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏


		Returns:
			int: –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–π –±—ã–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω —á–µ–∫–ø–æ–π–Ω—Ç
		"""
		checkpoint = torch.load(input_path, map_location=device)
		model.load_state_dict(checkpoint['model_state_dict'])
		optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
		epoch = checkpoint['epoch']
		metrics = checkpoint['metrics']

		print(f"–ß–µ–∫–ø–æ–π–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {input_path} (—ç–ø–æ—Ö–∞ {epoch}, –º–µ—Ç—Ä–∏–∫–∏: {metrics})")
		return epoch

	def set_random_seed(seed):
		"""
		–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.


		–ó–∞–¥–∞—ë—Ç seed –¥–ª—è:
		  - Python random
		  - NumPy
		  - PyTorch (CPU –∏ CUDA)

		Args:
			seed (int): –∑–Ω–∞—á–µ–Ω–∏–µ seed
		"""
		random.seed(seed)
		np.random.seed(seed)
		torch.manual_seed(seed)
		if torch.cuda.is_available():
			torch.cuda.manual_seed_all(seed)
			torch.backends.cudnn.deterministic = True
			torch.backends.cudnn.benchmark = False
		print(f"Random seed —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {seed}")




	def create_learning_rate_scheduler(optimizer, scheduler_type='step', **kwargs):
	"""
	–°–æ–∑–¥–∞—ë—Ç scheduler –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è learning rate.


	–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã:
	  - 'step': StepLR (—É–º–µ–Ω—å—à–µ–Ω–∏–µ LR –∫–∞–∂–¥—ã–µ N —ç–ø–æ—Ö)
	  - 'cosine': CosineAnnealingLR
	  - 'plateau': ReduceLROnPlateau (–ø–æ –º–µ—Ç—Ä–∏–∫–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
	  - 'exponential': ExponentialLR

	Args:
		optimizer (torch.optim.Optimizer): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
		scheduler_type (str): —Ç–∏–ø scheduler-–∞
		**kwargs: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è scheduler-–∞

	Returns:
		torch.optim.lr_scheduler: —ç–∫–∑–µ–º–ø–ª—è—Ä scheduler-–∞
	"""
	if scheduler_type == 'step':
		step_size = kwargs.get('step_size', 30)
		gamma = kwargs.get('gamma', 0.1)
		scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
	elif scheduler_type == 'cosine':
		T_max = kwargs.get('T_max', 50)
		eta_min = kwargs.get('eta_min', 1e-6)
		scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)
	elif scheduler_type == 'plateau':
		monitor = kwargs.get('monitor', 'val_loss')
		factor = kwargs.get('factor', 0.5)
		patience = kwargs.get('patience', 5)
		threshold = kwargs.get('threshold', 1e-4)
		min_lr = kwargs.get('min_lr', 1e-8)
		scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
			optimizer, mode='min' if 'loss' in monitor else 'max',
			factor=factor, patience=patience, threshold=threshold, min_lr=min_lr
		)
	elif scheduler_type == 'exponential':
		gamma = kwargs.get('gamma', 0.95)
		scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)
	else:
		raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø scheduler: {scheduler_type}")


	return scheduler

def apply_gradient_clipping(model, max_norm=1.0):
	"""
	–ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—Ä–µ–∑–∫—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (gradient clipping) –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è.


	–û–±—Ä–µ–∑–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ç–∞–∫, —á—Ç–æ–±—ã –∏—Ö –Ω–æ—Ä–º–∞ L2 –Ω–µ –ø—Ä–µ–≤—ã—à–∞–ª–∞ max_norm.


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		max_norm (float): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
	"""
	torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
	print(f"Gradient clipping –ø—Ä–∏–º–µ–Ω—ë–Ω (max_norm={max_norm})")


def compute_gradient_norm(model):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç –Ω–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏ (L2).


	–ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã—è–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º (–≤–∑—Ä—ã–≤–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã).


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å

	Returns:
		float: –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
	"""
	total_norm = 0.0
	for p in model.parameters():
		if p.grad is not None:
			param_norm = p.grad.data.norm(2)
			total_norm += param_norm.item() ** 2
	total_norm = total_norm ** 0.5
	return total_norm


def initialize_weights(model, init_type='xavier'):
	"""
	–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∑–∞–¥–∞–Ω–Ω—ã–º –º–µ—Ç–æ–¥–æ–º.

	–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
	  - 'xavier': Xavier/Glorot (–¥–ª—è ReLU, Tanh)
	  - 'kaiming': Kaiming/He (–¥–ª—è ReLU)
	  - 'normal': –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ N(0, 0.02)
	  - 'uniform': —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ U(-0.05, 0.05)


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		init_type (str): –º–µ—Ç–æ–¥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
	"""
	def init_func(m):
		classname = m.__class__.__name__
		if hasattr(m, 'weight') and classname.find('Conv') != -1 or classname.find('Linear') != -1:
			if init_type == 'xavier':
				torch.nn.init.xavier_normal_(m.weight.data, gain=1.0)
			elif init_type == 'kaiming':
				torch.nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in', nonlinearity='relu')
			elif init_type == 'normal':
				torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
			elif init_type == 'uniform':
				torch.nn.init.uniform_(m.weight.data, -0.05, 0.05)
			else:
				raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {init_type}")
			if hasattr(m, 'bias') and m.bias is not None:
				torch.nn.init.constant_(m.bias.data, 0.0)


	model.apply(init_func)
	print(f"–í–µ—Å–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –º–µ—Ç–æ–¥–æ–º: {init_type}")

def count_flops(model, input_tensor):
	"""
	–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–ø–µ—Ä–∞—Ü–∏–π (FLOPs) –º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞–¥–∞–Ω–Ω–æ–º –≤—Ö–æ–¥–µ.

	–¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏: pip install thop

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		input_tensor (torch.Tensor): –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä (–±–µ–∑ batch_size)

	Returns:
		int: –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ FLOPs
	"""
	try:
		import thop
		flops, params = thop.profile(model, inputs=(input_tensor.unsqueeze(0),), verbose=False)
		return flops
	except ImportError:
		print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ thop: pip install thop")
		return None

def profile_model(model, example_input, device='cuda'):
	"""
	–ü—Ä–æ—Ñ–∏–ª–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å (–≤—Ä–µ–º—è –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è, –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏).

	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç torch.profiler.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		example_input (torch.Tensor): –ø—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ('cuda' –∏–ª–∏ 'cpu')
	"""
	model.to(device)
	exampleinput = exampleinput.to(device)

	with torch.profiler.profile(
		activities=[
			torch.profiler.ProfilerActivity.CPU,
			torch.profiler.ProfilerActivity.CUDA
		],
		record_shapes=True,
		profile_memory=True,
		with_stack=True
	) as prof:
		_ = model(exampleinput)

	print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))

def convert_model_to_half(model):
	"""
	–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –≤ float16 (half precision) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏ —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.

	–í–Ω–∏–º–∞–Ω–∏–µ: –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å!

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å

	Returns:
		nn.Module: –º–æ–¥–µ–ª—å –≤ float16
	"""
	model.half()
	print("–ú–æ–¥–µ–ª—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ float16 (half precision)")
	return model

def move_model_to_device(model, device):
	"""
	–ü–µ—Ä–µ–Ω–æ—Å–∏—Ç –º–æ–¥–µ–ª—å –Ω–∞ –∑–∞–¥–∞–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		device (torch.device –∏–ª–∏ str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ('cpu', 'cuda', 'cuda:0' –∏ —Ç.–ø.)

	Returns:
		nn.Module: –º–æ–¥–µ–ª—å –Ω–∞ –Ω–æ–≤–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
	"""
	device = torch.device(device)
	model.to(device)
	print(f"–ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
	return model

def get_model_device(model):
	"""
	–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –º–æ–¥–µ–ª—å.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å

	Returns:
		torch.device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
	"""
	return next(model.parameters()).device

def check_model_consistency(model, example_input):
	"""
	–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –≤—Ö–æ–¥–∞.


	–í—ã–ø–æ–ª–Ω—è–µ—Ç forward-–ø—Ä–æ—Ö–æ–¥ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –Ω–µ—Ç –æ—à–∏–±–æ–∫.


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		exampleinput (torch.Tensor): –ø—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
		"""
	try:
		with torch.no_grad():
			output = model(example_input)
		print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–∏–ª–∞ forward-–ø—Ä–æ—Ö–æ–¥ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –≤—Ö–æ–¥–µ.")
		return True
	except Exception as e:
		print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ forward-–ø—Ä–æ—Ö–æ–¥–∞: {e}")
		return False

def freeze_batchnorm_stats(model):
	"""
	–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É BatchNorm (mean/var) ‚Äî –æ—Ç–∫–ª—é—á–∞–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.

	–ü–æ–ª–µ–∑–Ω–æ –ø—Ä–∏ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, —á—Ç–æ–±—ã –Ω–µ ¬´—Å–±–∏—Ç—å¬ª –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
	"""
	for module in model.modules():
		if isinstance(module, torch.nn.BatchNorm2d) or \
		   isinstance(module, torch.nn.BatchNorm1d) or \
		   isinstance(module, torch.nn.BatchNorm3d):
			module.eval()  # –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –≤ —Ä–µ–∂–∏–º inference (–Ω–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç running_mean/var)
	print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ BatchNorm –∑–∞–º–æ—Ä–æ–∂–µ–Ω–∞ (—Ä–µ–∂–∏–º eval)")


def unfreeze_batchnorm(model):
	"""
	–†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç BatchNorm ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è (–æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É).


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
	"""
	model.train()  # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –≤—Å–µ –º–æ–¥—É–ª–∏ –≤ —Ä–µ–∂–∏–º–µ train
	for module in model.modules():
		if isinstance(module, torch.nn.BatchNorm2d) or \
		   isinstance(module, torch.nn.BatchNorm1d) or \
		   isinstance(module, torch.nn.BatchNorm3d):
			module.train()  # —è–≤–Ω–æ —Å—Ç–∞–≤–∏–º train, —á—Ç–æ–±—ã –æ–±–Ω–æ–≤–ª—è–ª–∏—Å—å running_mean/var
	print("BatchNorm —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω (—Ä–µ–∂–∏–º train)")


def get_trainable_params(model):
	"""
	–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —Ç—Ä–µ–±—É–µ—Ç—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç (trainable).

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å

	Returns:
		list of torch.nn.Parameter: —Å–ø–∏—Å–æ–∫ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
	"""
	return [p for p in model.parameters() if p.requires_grad]

def get_frozen_params(model):
	"""
	–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞).

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å

	Returns:
		list of torch.nn.Parameter: —Å–ø–∏—Å–æ–∫ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
	"""
	return [p for p in model.parameters() if not p.requires_grad]

def count_trainable_params(model):
	"""
	–°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å

	Returns:
		int: —á–∏—Å–ª–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
	"""
	return sum(p.numel() for p in get_trainable_params(model))


def count_frozen_params(model):
	"""
	–°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å

	Returns:
		int: —á–∏—Å–ª–æ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
	"""
	return sum(p.numel() for p in get_frozen_params(model))

def print_trainable_status(model):
	"""
	–ü–µ—á–∞—Ç–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ (—Å–∫–æ–ª—å–∫–æ –æ–±—É—á–∞–µ–º—ã—Ö / –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö).

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
	"""
	trainable = count_trainable_params(model)
	frozen = count_frozen_params(model)
	total = trainable + frozen

	print(f"–°—Ç–∞—Ç—É—Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏:")
	print(f"  –û–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {trainable} ({trainable/total:.1%})")
	print(f"  –ó–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {frozen} ({frozen/total:.1%})")
	print(f"  –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total}")


def apply_weight_decay(model, weight_decay, exclude_names=None):
	"""
	–ü—Ä–∏–º–µ–Ω—è–µ—Ç weight decay (L2‚Äë—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é) –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –º–æ–¥–µ–ª–∏.

	–ú–æ–∂–Ω–æ –∏—Å–∫–ª—é—á–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, bias, BatchNorm).


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		weight_decay (float): –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç L2‚Äë—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
		exclude_names (list of str): –∏–º–µ–Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫ –∫–æ—Ç–æ—Ä—ã–º –Ω–µ –ø—Ä–∏–º–µ–Ω—è—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, ['bias', 'batch_norm'])
	"""
	if exclude_names is None:
		exclude_names = ['bias', 'running_mean', 'running_var', 'num_batches_tracked']


	params_to_decay = []
	params_no_decay = []


	for name, param in model.named_parameters():
		if not param.requires_grad:
			continue
		if any(ex in name for ex in exclude_names):
			params_no_decay.append(param)
		else:
			params_to_decay.append(param)

	# –í –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–µ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –≥—Ä—É–ø–ø—ã
	# –ü—Ä–∏–º–µ—Ä:
	# optim = torch.optim.AdamW([
	#     {'params': params_to_decay, 'weight_decay': weight_decay},
	#     {'params': params_no_decay, 'weight_decay': 0.0}
	# ], lr=lr)
	print(f"Weight decay ({weight_decay}) –ø—Ä–∏–º–µ–Ω—ë–Ω –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º (–∏—Å–∫–ª—é—á–∞—è {exclude_names})")


def replace_activation(model, old_act, new_act):
	"""
	–ó–∞–º–µ–Ω—è–µ—Ç –≤—Å–µ —ç–∫–∑–µ–º–ø–ª—è—Ä—ã —Å—Ç–∞—Ä–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –Ω–∞ –Ω–æ–≤—É—é –≤ –º–æ–¥–µ–ª–∏.


	–ü—Ä–∏–º–µ—Ä: –∑–∞–º–µ–Ω–∏—Ç—å ReLU –Ω–∞ LeakyReLU.

	–í–Ω–∏–º–∞–Ω–∏–µ: —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –º–æ–¥—É–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –ø—Ä—è–º—ã–º–∏ –¥–µ—Ç—å–º–∏ model.
	–î–ª—è –≥–ª—É–±–æ–∫–æ–π –∑–∞–º–µ–Ω—ã –Ω—É–∂–Ω–æ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ model.modules().

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		old_act (type): —Ç–∏–ø —Å—Ç–∞—Ä–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, nn.ReLU)
		new_act (nn.Module): –Ω–æ–≤—ã–π –º–æ–¥—É–ª—å –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, nn.LeakyReLU())
	"""
	for child in model.children():
		if isinstance(child, old_act):
			# –ó–∞–º–µ–Ω–∞ –º–æ–¥—É–ª—è
			idx = list(model._modules.keys()).index(child._get_name())
			model._modules[idx] = new_act
	print(f"–ó–∞–º–µ–Ω–µ–Ω—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–∏: {old_act} ‚Üí {new_act}")

def summary_model_shapes(model, input_shape):
	"""
	–í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–∫—É –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º —Ç–µ–Ω–∑–æ—Ä–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ (–∞–Ω–∞–ª–æ–≥ keras.summary).

	–¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏: pip install torchinfo

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		input_shape (tuple): —Ñ–æ—Ä–º–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ (–±–µ–∑ batch_size)
	"""
	try:
		from torchinfo import summary
		batch_size = 1
		input_size = (batch_size,) + input_shape
		summary(model, input_size=input_size)
	except ImportError:
		print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ torchinfo: pip install torchinfo")



def export_model_to_onnx(model, input_tensor, output_path, input_names=None, output_names=None):
	"""
	–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –≤ —Ñ–æ—Ä–º–∞—Ç ONNX.

	ONNX –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –≤ –¥—Ä—É–≥–∏—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞—Ö –∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		input_tensor (torch.Tensor): –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä (–ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö)
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ONNX‚Äë—Ñ–∞–π–ª–∞
		inputnames (list of str, optional): –∏–º–µ–Ω–∞ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤
		outputnames (list of str, optional): –∏–º–µ–Ω–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤
	"""
	model.eval()  # –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º inference
	try:
		torch.onnx.export(
			model,
			input_tensor,
			output_path,
			export_params=True,
			opset_version=11,
			do_constant_folding=True,
			input_names=input_names or ['input'],
			output_names=output_names or ['output'],
			dynamic_axes=None  # –º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
		)
		print(f"–ú–æ–¥–µ–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ ONNX: {output_path}")
	except Exception as e:
		print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –≤ ONNX: {e}")

def convert_model_to_torchscript(model, example_input, output_path, mode='trace'):
	"""
	–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –≤ TorchScript (–¥–ª—è —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏—è –±–µ–∑ Python).


	–†–µ–∂–∏–º—ã:
	  - 'trace': —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ (–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –≤—Ö–æ–¥–∞)
	  - 'script': –∫–æ–º–ø–∏–ª—è—Ü–∏—è (–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–¥ –º–æ–¥–µ–ª–∏)

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		exampleinput (torch.Tensor): –ø—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è TorchScript‚Äë–º–æ–¥—É–ª—è
		mode (str): —Ä–µ–∂–∏–º –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ ('trace' –∏–ª–∏ 'script')
	"""
	model.eval()
	if mode == 'trace':
		traced_script_module = torch.jit.trace(model, exampleinput)
	elif mode == 'script':
		traced_scriptmodule = torch.jit.script(model)
	else:
		raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∂–∏–º: {mode}")

	traced_scriptmodule.save(output_path)
	print(f"–ú–æ–¥–µ–ª—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ TorchScript ({mode}) –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")


def quantize_model(model, backend='fbgemm'):
	"""
	–ö–≤–∞–Ω—Ç—É–µ—Ç –º–æ–¥–µ–ª—å (—É–º–µ–Ω—å—à–∞–µ—Ç –±–∏—Ç–Ω–æ—Å—Ç—å –≤–µ—Å–æ–≤/–∞–∫—Ç–∏–≤–∞—Ü–∏–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏ —Å–∂–∞—Ç–∏—è).


	–¢—Ä–µ–±—É–µ—Ç, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –±—ã–ª–∞ –≤ —Ä–µ–∂–∏–º–µ eval().

	–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ (–¥–ª—è CPU).

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ eval())
		backend (str): –±—ç–∫–µ–Ω–¥ –¥–ª—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è ('fbgemm' –¥–ª—è x86, 'qnnpack' –¥–ª—è ARM)


	Returns:
		nn.Module: –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
	"""
	model_q = torch.quantization.quantize_dynamic(
		model, {torch.nn.Linear}, dtype=torch.qint8, backend=backend
	)
	print(f"–ú–æ–¥–µ–ª—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∞ (backend={backend})")
	return model_q

def benchmark_model(model, test_loader, device='cuda'):
	"""
	–ó–∞–º–µ—Ä—è–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		test_loader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ('cuda' –∏–ª–∏ 'cpu')


	Returns:
		dict: {'latency_ms': ..., 'memory_mb': ..., 'fps': ...}
	"""
	model.to(device)
	model.eval()

	latencies = []
	memory_allocated = []

	with torch.no_grad():
		for data in test_loader:
			data = data.to(device)

			# –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏
			start_event = torch.cuda.Event(enable_timing=True)
			end_event = torch.cuda.Event(enable_timing=True)

			start_event.record()
			_ = model(data)
			end_event.record()
			torch.cuda.synchronize()
			latency = start_event.elapsed_time(end_event)  # –º—Å
			latencies.append(latency)

			# –ó–∞–º–µ—Ä –ø–∞–º—è—Ç–∏
			memory_allocated.append(torch.cuda.memory_allocated() / 1024**2)  # –ú–ë


	avg_latency = sum(latencies) / len(latencies)
	avg_memory = sum(memory_allocated) / len(memory_allocated)
	fps = 1000 / avg_latency if avg_latency > 0 else 0

	return {
		'latency_ms': avg_latency,
		'memory_mb': avg_memory,
		'fps': fps
	}

def visualize_feature_maps(model, input_tensor, layer_names, output_dir):
	"""
	–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (feature maps) –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—ë–≤.


	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		input_tensor (torch.Tensor): –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä
		layernames (list of str): –∏–º–µ–Ω–∞ —Å–ª–æ—ë–≤, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –∏–∑–≤–ª–µ–∫–∞—Ç—å feature maps
		outputdir (str): –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
	"""
	from torchvision.utils import make_grid
	import cv2

	# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ö—É–∫–∏ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –≤—ã—Ö–æ–¥–æ–≤ —Å–ª–æ—ë–≤
	feature_maps = {}
	hooks = []

	def hook_fn(module, input, output):
		feature_maps[module._get_name()] = output

	for name, module in model.named_modules():
		if name in layernames:
			hooks.append(module.register_forward_hook(hook_fn))

	# –ü—Ä–æ–≥–æ–Ω—è–µ–º forward
	with torch.no_grad():
		_ = model(input_tensor)

	# –£–¥–∞–ª—è–µ–º —Ö—É–∫–∏
	for hook in hooks:
		hook.remove()

	# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º
	for name, fm in feature_maps.items():
		fm = fm.detach().cpu()
		# –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ N –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 16)
		fm = fm[0, :16].unsqueeze(1)  # [16, 1, H, W]
		grid = make_grid(fm, nrow=4, normalize=True, scale_each=True)
		grid = grid.permute(1, 2, 0).numpy()  # HWC
		grid = (grid * 255).astype(np.uint8)

		cv2.imwrite(f"{output_dir}/feature_map_{name}.png", cv2.cvtColor(grid, cv2.COLOR_RGB2BGR))
	print(f"–ö–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")


def compute_activation_statistics(model, data_loader, device='cuda'):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∞–∫—Ç–∏–≤–∞—Ü–∏–π (—Å—Ä–µ–¥–Ω–µ–µ, –¥–∏—Å–ø–µ—Ä—Å–∏—é) –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É.

	–ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ ¬´–º—ë—Ä—Ç–≤—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤¬ª –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		data_loader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ

	Returns:
		dict: {–∏–º—è_—Å–ª–æ—è: {'mean': ..., 'var': ...}}
	"""
	activation_stats = {}

	def hook_fn(module, input, output):
		act = output.detach()
		mean = act.mean().item()
		var = act.var().item()
		name = module._get_name()
		if name not in activation_stats:
			activation_stats[name] = {'mean': [], 'var': []}
		activation_stats[name]['mean'].append(mean)
		activation_stats[name]['var'].append(var)

	# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ö—É–∫–∏ –Ω–∞ –≤—Å–µ –º–æ–¥—É–ª–∏ —Å –∞–∫—Ç–∏–≤–∞—Ü–∏—è–º–∏
	hooks = []
	for name, module in model.named_modules():
		if isinstance(module, torch.nn.ReLU) or \
		   isinstance(module, torch.nn.LeakyReLU) or \
		   isinstance(module, torch.nn.ELU) or \
		   isinstance(module, torch.nn.PReLU):
			hooks.append(module.register_forward_hook(hook_fn))


	model.to(device)
	model.eval()

	with torch.no_grad():
		for data, _ in data_loader:
			data = data.to(device
			model(data)


	# –£–¥–∞–ª—è–µ–º —Ö—É–∫–∏
	for hook in hooks:
		hook.remove()

	# –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –≤—Å–µ–º –±–∞—Ç—á–∞–º
	for name, stats in activation_stats.items():
		stats['mean'] = np.mean(stats['mean'])
		stats['var'] = np.mean(stats['var'])

	print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤—ã—á–∏—Å–ª–µ–Ω–∞.")
	return activation_stats

def analyze_gradient_flow(model, data_loader, device='cuda'):
	"""
	–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Ç–æ–∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ —Å–ª–æ—è–º (gradient flow).


	–ü–æ–º–æ–≥–∞–µ—Ç –≤—ã—è–≤–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã: –∏—Å—á–µ–∑–∞—é—â–∏–µ/–≤–∑—Ä—ã–≤–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã.

	–ó–∞–º–µ—Ä—è–µ—Ç –Ω–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.

	"""
	model.to(device)
	model.train()

	layer_grad_norms = {}

	def hook_fn(module, grad_input, grad_output):
		if grad_output[0] is not None:
			norm = grad_output[0].norm().item()
			name = module._get_name()
			if name not in layer_grad_norms:
				layer_grad_norms[name] = []
			layer_grad_norms[name].append(norm)


	hooks = []
	for name, module in model.named_modules():
		if hasattr(module, 'weight') and module.weight is not None:
			hooks.append(module.register_backward_hook(hook_fn))


	optimizer = torch.optim.SGD(model.parameters(), lr=0.01)


	for data, target in data_loader:
		data, target = data.to(device), target.to(device)

		optimizer.zero_grad()
		output = model(data)
		loss = torch.nn.functional.cross_entropy(output, target)
		loss.backward()
		optimizer.step()

		# –ü–æ—Å–ª–µ backward —Å–æ–±–∏—Ä–∞–µ–º –Ω–æ—Ä–º—ã
		# (–æ–Ω–∏ —É–∂–µ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ layer_grad_norms —á–µ—Ä–µ–∑ —Ö—É–∫–∏)


	# –£–¥–∞–ª—è–µ–º —Ö—É–∫–∏
	for hook in hooks:
		hook.remove()

	# –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –±–∞—Ç—á–∞–º
	for name in layer_grad_norms:
		layer_grad_norms[name] = np.mean(layer_grad_norms[name])

	print("–ê–Ω–∞–ª–∏–∑ –ø–æ—Ç–æ–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω.")
	return layer_grad_norms


def detect_dead_neurons(activation_stats, threshold=1e-6):
	"""
	–í—ã—è–≤–ª—è–µ—Ç ¬´–º—ë—Ä—Ç–≤—ã–µ¬ª –Ω–µ–π—Ä–æ–Ω—ã (—Å –ø–æ—á—Ç–∏ –Ω—É–ª–µ–≤–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π).


	–°—á–∏—Ç–∞–µ—Ç –Ω–µ–π—Ä–æ–Ω –º—ë—Ä—Ç–≤—ã–º, –µ—Å–ª–∏ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ < threshold.


	Args:
		activation_stats (dict): —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –∏–∑ compute_activation_statistics
		threshold (float): –ø–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º—ë—Ä—Ç–≤–æ–≥–æ –Ω–µ–π—Ä–æ–Ω–∞


	Returns:
		dict: {–∏–º—è_—Å–ª–æ—è: –¥–æ–ª—è_–º—ë—Ä—Ç–≤—ã—Ö_–Ω–µ–π—Ä–æ–Ω–æ–≤}
	"""
	dead_ratio = {}
	for name, stats in activation_stats.items():
		mean_act = stats['mean']
		ratio = 1.0 if mean_act < threshold else 0.0
		dead_ratio[name] = ratio
	print("–ê–Ω–∞–ª–∏–∑ –º—ë—Ä—Ç–≤—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω.")
	return dead_ratio

def plot_training_curves(train_losses, val_losses, train_accs, val_accs, output_path):
	"""
	–†–∏—Å—É–µ—Ç –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è (loss –∏ accuracy).


	Args:
		train_losses (list): –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏
		val_losses (list): –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
		train_accs (list): —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏
		val_accs (list): —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
	"""
	epochs = len(train_losses)

	plt.figure(figsize=(12, 5))

	plt.subplot(1, 2, 1)
	plt.plot(range(epochs), train_losses, label='Train Loss')
	plt.plot(range(epochs), val_losses, label='Val Loss')
	plt.xlabel('Epoch')
	plt.ylabel('Loss')
	plt.title('Loss Curve')
	plt.legend()


	plt.subplot(1, 2, 2)
	plt.plot(range(epochs), train_accs, label='Train Acc')
	plt.plot(range(epochs), val_accs, label='Val Acc')
	plt.xlabel('Epoch')
	plt.ylabel('Accuracy')
	plt.title('Accuracy Curve')
	plt.legend()


	plt.savefig(output_path)
	plt.close()
	print(f"–ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")


def save_predictions(model, data_loader, output_path, device='cuda'):
	"""
	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		data_loader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (.npy)
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
	"""
	model.to(device)
	model.eval()

	all_preds = []
	all_targets = []

	with torch.no_grad():
		for data, target in data_loader:
			data, target = data.to(device), target.to(device)
			output = model(data)
			pred = output.argmax(dim=1, keepdim=True)
			all_preds.append(pred.cpu().numpy())
			all_targets.append(target.cpu().numpy())

	all_preds = np.concatenate(all_preds)
	all_targets = np.concatenate(all_targets)

	np.savez(output_path, predictions=all_preds, targets=all_targets)
	print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")

def compute_confusion_matrix(model, data_loader, num_classes, device='cuda'):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫ (confusion matrix).

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		data_loader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
		num_classes (int): —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ

	Returns:
		np.ndarray: –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (num_classes, num_classes)
	"""
	model.to(device)
	model.eval()

	confusion_mat = np.zeros((num_classes, num_classes), dtype=np.int64)

	with torch.no_grad():
		for data, target in data_loader:
			data, target = data.to(device), target.to(device)
			output = model(data)
			pred = output.argmax(dim=1)
			for t, p in zip(target.view(-1), pred.view(-1)):
				confusion_mat[t.long(), p.long()] += 1

	print("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –≤—ã—á–∏—Å–ª–µ–Ω–∞.")
	return confusion_mat

def plot_confusion_matrix(confusion_mat, class_names, output_path):
	"""
	–†–∏—Å—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫.

	Args:
		confusion_mat (np.ndarray): –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
		classnames (list of str): –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
	"""
	import seaborn as sns

	plt.figure(figsize=(10, 8))
	sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
	plt.title('Confusion Matrix')
	plt.xlabel('Predicted')
	plt.ylabel('True')
	plt.savefig(output_path)
	plt.close()
	print(f"–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")




def evaluate_model_robustness(model, test_loader, device='cuda', noise_levels=None):
	"""
	–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ —à—É–º—É –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

	–î–æ–±–∞–≤–ª—è–µ—Ç –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–π —à—É–º —Ä–∞–∑–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –∏ –∑–∞–º–µ—Ä—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		test_loader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
		noise_levels (list of float): —É—Ä–æ–≤–Ω–∏ —à—É–º–∞ (std) –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

	Returns:
		dict: {noise_level: accuracy}
	"""
	if noise_levels is None:
		noise_levels = [0.0, 0.05, 0.1, 0.15, 0.2]

	model.to(device)
	model.eval()
	results = {}

	with torch.no_grad():
		for noise_std in noise_levels:
			correct = 0
			total = 0

			for data, target in test_loader:
				data, target = data.to(device), target.to(device)

				# –î–æ–±–∞–≤–ª—è–µ–º —à—É–º
				if noise_std > 0:
					noise = torch.randn_like(data) * noise_std
					data = data + noise

				output = model(data)
				pred = output.argmax(dim=1, keepdim=True)
				correct += pred.eq(target.view_as(pred)).sum().item()
				total += target.size(0)

			accuracy = correct / total
			results[noise_std] = accuracy
			print(f"Noise std={noise_std}: Accuracy = {accuracy:.4f}")

	return results


def compute_feature_importance(model, data_loader, target_class, device='cuda'):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (feature importance) –º–µ—Ç–æ–¥–æ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—Ö–æ–¥–∞ –ø–æ –≤—Ö–æ–¥—É (saliency maps).


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		data_loader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
		target_class (int): –∫–ª–∞—Å—Å, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ –≤—ã—á–∏—Å–ª—è—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ

	Returns:
		torch.Tensor: —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω–∞—è –∫–∞—Ä—Ç–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ (—Ñ–æ—Ä–º–∞ –∫–∞–∫ —É –≤—Ö–æ–¥–∞)
	"""
	model.to(device)
	model.eval()

	total_saliency = None
	count = 0

	for data, target in data_loader:
		data, target = data.to(device), target.to(device)

		# –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–ª–∞—Å—Å—É
		mask = (target == target_class)
		if mask.sum() == 0:
			continue

		data = data[mask]
		if data.size(0) == 0:
			continue

		data.requires_grad = True
		optimizer = torch.optim.SGD([data], lr=0)

		output = model(data)
		loss = -output[:, target_class].sum()  # –≥—Ä–∞–¥–∏–µ–Ω—Ç –≤ —Å—Ç–æ—Ä–æ–Ω—É —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∞


		optimizer.zero_grad()
		loss.backward()

		saliency = data.grad.data.abs()
		if total_saliency is None:
			total_saliency = saliency.sum(dim=0, keepdim=True)
		else:
			total_saliency += saliency.sum(dim=packed_dim=0, keepdim=True)

		count += data.size(0)

	if count > 0:
		total_saliency /= count
	else:
		total_saliency = torch.zeros_like(data[0:1])

	print(f"–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤—ã—á–∏—Å–ª–µ–Ω–∞ –¥–ª—è –∫–ª–∞—Å—Å–∞ {target_class}.")
	return total_saliency.cpu()


def plot_feature_importance(saliency_map, input_shape, output_path):
	"""
	–†–∏—Å—É–µ—Ç –∫–∞—Ä—Ç—É –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (saliency map).


	Args:
		saliency_map (torch.Tensor): –∫–∞—Ä—Ç–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ (C, H, W)
		input_shape (tuple): –∏—Å—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞ –≤—Ö–æ–¥–∞ (H, W) –¥–ª—è —Ä–µ—Å–∞–π–∑–∞
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
	"""
	import cv2

	# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
	saliency = saliency_map.numpy()
	saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)
	saliency = (saliency * 255).astype(np.uint8)


	# –ï—Å–ª–∏ –º–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–æ–≤ ‚Äî –±–µ—Ä—ë–º –º–∞–∫—Å–∏–º—É–º –ø–æ –∫–∞–Ω–∞–ª–∞–º
	if saliency.shape[0] > 1:
		saliency = saliency.max(axis=0, keepdims=True)


	# –†–µ—Å–∞–π–∑ –¥–æ –∏—Å—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º—ã
	saliency = cv2.resize(saliency[0], input_shape, interpolation=cv2.INTER_LINEAR)
	saliency = cv2.applyColorMap(saliency, cv2.COLORMAP_JET)


	cv2.imwrite(output_path, saliency)
	print(f"–ö–∞—Ä—Ç–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")


def extract_embeddings(model, data_loader, layer_name, device='cuda'):
	"""
	–ò–∑–≤–ª–µ–∫–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è) –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å–ª–æ—è.


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		data_loader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
		layer_name (str): –∏–º—è —Å–ª–æ—è, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –±—Ä–∞—Ç—å –≤—ã—Ö–æ–¥
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ

	Returns:
		np.ndarray: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (N, D)
		np.ndarray: –º–µ—Ç–∫–∏ (N,)
	"""
	model.to(device)
	model.eval()

	embeddings = []
	labels = []

	def hook_fn(module, input, output):
		embeddings.append(output.detach().cpu().numpy())

	# –ù–∞—Ö–æ–¥–∏–º —Å–ª–æ–π –∏ —Å—Ç–∞–≤–∏–º —Ö—É–∫
	target_module = None
	for name, module in model.named_modules():
		if name == layer_name:
			target_module = module
			break

	if target_module is None:
		raise ValueError(f"–°–ª–æ–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {layer_name}")

	hook = target_module.register_forward_hook(hook_fn)

	with torch.no_grad():
		for data, target in data_loader:
			data, target = data.to(device), target.to(device)
			model(data)
			labels.append(target.cpu().numpy())

	hook.remove()

	embeddings = np.concatenate(embeddings, axis=0)
	labels = np.concatenate(labels, axis=0)

	print(f"–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã –∏–∑ —Å–ª–æ—è {layer_name}: {embeddings.shape}")
	return embeddings, labels

def analyze_learning_rate_impact(model_fn, data_loaders, lr_list, epochs, device='cuda'):
	"""
	–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ learning rate –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ.

	–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Å —Ä–∞–∑–Ω—ã–º–∏ LR –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∫—Ä–∏–≤—ã–µ –ø–æ—Ç–µ—Ä—å.


	Args:
		model_fn (callable): —Ñ—É–Ω–∫—Ü–∏—è, –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∞—è –º–æ–¥–µ–ª—å
		data_loaders (dict): {'train': train_loader, 'val': val_loader}
		lr_list (list of float): —Å–ø–∏—Å–æ–∫ LR –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
		epochs (int): —á–∏—Å–ª–æ —ç–ø–æ—Ö
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ

	Returns:
		dict: {lr: {'train_loss': [...], 'val_loss': [...]}}
	"""
	results = {}

	for lr in lr_list:
		print(f"\n–û–±—É—á–µ–Ω–∏–µ —Å LR={lr}")
		model = modelfn().to(device)
		optimizer = torch.optim.Adam(model.parameters(), lr=lr)
		scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epochs//2, gamma=0.1)

		train_losses = []
		val_losses = []

		for epoch in range(epochs):
			# –û–±—É—á–µ–Ω–∏–µ
			model.train()
			train_loss = 0.0
			for data, target in data_loaders['train']:
				data, target = data.to(device), target.to(device)
				optimizer.zero_grad()
				output = model(data)
				loss = torch.nn.functional.cross_entropy(output, target)
				loss.backward()
				optimizer.step()
				train_loss += loss.item()
			train_loss /= len(data_loaders['train'])
			train_losses.

			train_losses.append(train_loss)


			# –í–∞–ª–∏–¥–∞—Ü–∏—è
			model.eval()
			val_loss = 0.0
			with torch.no_grad():
				for data, target in data_loaders['val']:
					data, target = data.to(device), target.to(device)
					output = model(data)
					loss = torch.nn.functional.cross_entropy(output, target)
					val_loss += loss.item()
			val_loss /= len(data_loaders['val'])
			val_losses.append(val_loss)

			scheduler.step()

			print(f"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

		results[lr] = {
			'train_loss': train_losses,
			'val_loss': val_losses
		}

	print("–ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è LR –∑–∞–≤–µ—Ä—à—ë–Ω.")
	return results




def plot_lr_analysis(results, output_path):
	"""
	–†–∏—Å—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–ª–∏—è–Ω–∏—è learning rate.

	Args:
		results (dict): —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ analyze_learning_rate_impact
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
	"""
	plt.figure(figsize=(12, 8))

	for lr, losses in results.items():
		epochs = len(losses['train_loss'])
		plt.plot(range(epochs), losses['train_loss'], label=f'Train LR={lr}')
		plt.plot(range(epochs), losses['val_loss'], '--', label=f'Val LR={lr}')


	plt.xlabel('Epoch')
	plt.ylabel('Loss')
	plt.title('Learning Rate Impact Analysis')
	plt.legend()
	plt.grid(True)
	plt.savefig(output_path)
	plt.close()
	print(f"–ì—Ä–∞—Ñ–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ LR —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")

def compute_model_complexity(model, input_size):
	"""
	–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (—á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, FLOPs, –ø–∞–º—è—Ç—å).


	–¢—Ä–µ–±—É–µ—Ç: pip install thop


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		input_size (tuple): —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ (C, H, W)

	Returns:
		dict: {'params': ..., 'flops': ..., 'memory_mb': ...}
	"""
	try:
		import thop
		input_tensor = torch.randn(1, *input_size).to(next(model.parameters()).device)
		flops, params = thop.profile(model, inputs=(input_tensor,), verbose=False)


		# –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ (–≥—Ä—É–±–æ)
		memory_mb = params * 4 / (1024 ** 2)  # 4 –±–∞–π—Ç–∞ –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä (float32)


		return {
			'params': params,
			'flops': flops,
			'memory_mb': memory_mb
		}
	except ImportError:
		print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ thop: pip install thop")
		return None

def print_model_complexity_summary(model, input_size):
	"""
	–ü–µ—á–∞—Ç–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		input_size (tuple): —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞ (C, H, W)
	"""
	complexity = compute_model_complexity(model, input_size)
	if complexity:
		print("–°–≤–æ–¥–∫–∞ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏:")
		print(f"  –ß–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {complexity['params']:,}")
		print(f"  FLOPs: {complexity['flops']:,}")
		print(f"  –ü–∞–º—è—Ç—å (–æ—Ü–µ–Ω–∫–∞): {complexity['memory_mb']:.2f} –ú–ë")


def compare_models_complexity(models, model_names, input_size):
	"""
	–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π.

	Args:
		models (list of nn.Module): —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
		model_names (list of str): –∏–º–µ–Ω–∞ –º–æ–¥–µ–ª–µ–π
		input_size (tuple): —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞ (C, H, W)
	"""
	print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π:")
	print("-" * 60)
	for name, model in zip(model_names, models):
		complexity = compute_model_complexity(model, input_size)
		if complexity:
			print(f"{name}:")
			print(f"  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {complexity['params']:,}")
			print(f"  FLOPs: {complexity['flops']:,}")
			print(f"  –ü–∞–º—è—Ç—å: {complexity['memory_mb']:.2f} –ú–ë")
			print("-" * 40)

def save_model_with_metadata(model, optimizer, epoch, metrics, output_path, extra_info=None):
	"""
	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –≤–º–µ—Å—Ç–µ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, —ç–ø–æ—Ö–∞, –º–µ—Ç—Ä–∏–∫–∏).


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		optimizer (torch.optim.Optimizer): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
		epoch (int): –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏
		metrics (dict): –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
		extra_info (dict, optional): –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
	"""
	checkpoint = {
		'model_state_dict': model.state_dict(),
		'optimizer_state_dict': optimizer.state_dict(),
		'epoch': epoch,
		'metrics': metrics,
		'timestamp': datetime.now().isoformat()
	}
	if extra_info:
		checkpoint['extra_info'] = extra_info

	torch.save(checkpoint, output_path)
	print(f"–ß–µ–∫–ø–æ–∏–Ω—Ç –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")

def load_model_with_metadata(input_path, model, optimizer=None):
	"""
	–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.

	Args:
		input_path (str): –ø—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É
		model (nn.Module): –º–æ–¥–µ–ª—å (–¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤)
		optimizer (torch.optim.Optimizer, optional): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ)

	Returns:
		dict: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
	"""
	checkpoint = torch.load(input_path)
	model.load_state_dict(checkpoint['model_state_dict'])
	if optimizer and 'optimizer_state_dict' in checkpoint:
		optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

	print(f"–ß–µ–∫–ø–æ–∏–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {input_path}")
	return checkpoint




def setup_learning_rate_scheduler(optimizer, scheduler_type='step', **kwargs):
	"""
	–°–æ–∑–¥–∞—ë—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è learning rate.


	Args:
		optimizer (torch.optim.Optimizer): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
		scheduler_type (str): —Ç–∏–ø –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
			- 'step': StepLR
			- 'multistep': MultiStepLR
			- 'exponential': ExponentialLR
			- 'plateau': ReduceLROnPlateau
			- 'cyclic': CyclicLR
		**kwargs: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞


	Returns:
		torch.optim.lr_scheduler._LRScheduler: –æ–±—ä–µ–∫—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
	"""
	if scheduler_type == 'step':
		return torch.optim.lr_scheduler.StepLR(
			optimizer,
			step_size=kwargs.get('step_size', 30),
			gamma=kwargs.get('gamma', 0.1)
		)
	elif scheduler_type == 'multistep':
		return torch.optim.lr_scheduler.MultiStepLR(
			optimizer,
			milestones=kwargs.get('milestones', [30, 60, 90]),
			gamma=kwargs.get('gamma', 0.1)
		)
	elif scheduler_type == 'exponential':
		return torch.optim.lr_scheduler.ExponentialLR(
			optimizer,
			gamma=kwargs.get('gamma', 0.99)
		)
	elif scheduler_type == 'plateau':
		return torch.optim.lr_scheduler.ReduceLROnPlateau(
			optimizer,
			mode=kwargs.get('mode', 'min'),
			factor=kwargs.get('factor', 0.1),
			patience=kwargs.get('patience', 10),
			verbose=True
		)
	elif scheduler_type == 'cyclic':
		return torch.optim.lr_scheduler.CyclicLR(
			optimizer,
			base_lr=kwargs.get('base_lr', 0.001),
			max_lr=kwargs.get('max_lr', 0.01),
			step_size_up=kwargs.get('step_size_up', 2000),
			mode=kwargs.get('mode', 'triangular')
		)
	else:
		raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞: {scheduler_type}")


def train_with_scheduler(model, train_loader, val_loader, optimizer, scheduler,
					  epochs, device='cuda', criterion=None):
	"""
	–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ LR.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		train_loader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
		val_loader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
		optimizer (torch.optim.Optimizer): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
		scheduler (torch.optim.lr_scheduler._LRScheduler): –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ LR
		epochs (int): —á–∏—Å–ª–æ —ç–ø–æ—Ö
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
		criterion (callable, optional): —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å


	Returns:
		dict: –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è (train_loss, val_loss, train_acc, val_acc)
	"""
	model.to(device)
	criterion = criterion or torch.nn.CrossEntropyLoss()


	train_losses, val_losses = [], []
	train_accs, val_accs = [], []


	for epoch in range(epochs):
		# –û–±—É—á–µ–Ω–∏–µ
		model.train()
		running_loss = 0.0
		correct = 0
		total = 0

		for data, target in train_loader:
			data, target = data.to(device), target.to(device)
			optimizer.zero_grad()
			output = model(data)
			loss = criterion(output, target)
			loss.backward()
			optimizer.step()

			running_loss += loss.item()
			_, predicted = output.max(1)
			total += target.size(0)
			correct += predicted.eq(target).sum().item()


		train_loss = running_loss / len(train_loader)
		train_acc = correct / total
		train_losses.append(train_loss)
		train_accs.append(train_acc)


		# –í–∞–ª–∏–¥–∞—Ü–∏—è
		model.eval()
		val_loss = 0.0
		correct = 0
		total = 0

		with torch.no_grad():
			for data, target in val_loader:
				data, target = data.to(device), target.to(device)
				output = model(data)
				loss = criterion(output, target)
				val_loss += loss.item()

				_, predicted = output.max(1)
				total += target.size(0)
				correct += predicted.eq(target).sum().item()


		val_loss /= len(val_loader)
		val_acc = correct / total
		val_losses.append(val_loss)
		val_accs.append(val_acc)

		# –®–∞–≥ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
		if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
			scheduler.step(val_loss)
		else:
			scheduler.step()


		print(f"Epoch {epoch+1}/{epochs}, "
			  f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
			  f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

	return {
		'train_loss': train_losses,
		'val_loss': val_losses,
		'train_acc': train_accs,
		'val_acc': val_accs
	}

def plot_lr_scheduler_effect(scheduler, epochs, output_path):
	"""
	–†–∏—Å—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è LR –ø–æ —ç–ø–æ—Ö–∞–º –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞.

	Args:
		scheduler (torch.optim.lr_scheduler._LRScheduler): –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
		epochs (int): —á–∏—Å–ª–æ —ç–ø–æ—Ö
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
	"""
	lrs = []
	for _ in range(epochs):
		lrs.append(scheduler.get_last_lr()[0])
		scheduler.step()

	plt.figure(figsize=(10, 6))
	plt.plot(range(epochs), lrs, marker='o')
	plt.xlabel('Epoch')
	plt.ylabel('Learning Rate')
	plt.title('Learning Rate Schedule')
	plt.grid(True)
	plt.savefig(output_path)
	plt.close()
	print(f"–ì—Ä–∞—Ñ–∏–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ LR —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")

def freeze_layers(model, layer_names):
	"""
	–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–µ —Å–ª–æ–∏ –º–æ–¥–µ–ª–∏ (–æ—Ç–∫–ª—é—á–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã).

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		layer_names (list of str): –∏–º–µ–Ω–∞ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º—ã—Ö —Å–ª–æ—ë–≤
	"""
	for name, param in model.named_parameters():
		if any(layer in name for layer in layer_names):
			param.requires_grad = False
	print(f"–°–ª–æ–∏ –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã: {layer_names}")


def unfreeze_layers(model, layer_names=None):
	"""
	–†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç —Å–ª–æ–∏ –º–æ–¥–µ–ª–∏ (–≤–∫–ª—é—á–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã).


	–ï—Å–ª–∏ layer_names –Ω–µ —É–∫–∞–∑–∞–Ω, —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç –≤—Å–µ —Å–ª–æ–∏.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		layer_names (list of str, optional): –∏–º–µ–Ω–∞ —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º—ã—Ö —Å–ª–æ—ë–≤
	"""
	if layer_names is None:
		for param in model.parameters():
			param.requires_grad = True
		print("–í—Å–µ —Å–ª–æ–∏ —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω—ã")
	else:
		for name, param in model.named_parameters():
			if any(layer in name for layer in layer_names):
				param.requires_grad = True
		print(f"–°–ª–æ–∏ —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω—ã: {layer_names}")


def count_trainable_params_by_layer(model):
	"""
	–°—á–∏—Ç–∞–µ—Ç —á–∏—Å–ª–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Å–ª–æ—è–º.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å

	Returns:
		dict: {–∏–º—è_—Å–ª–æ—è: —á–∏—Å–ª–æ_–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤}
	"""
	counts = {}
	for name, param in model.named_parameters():
		if param.requires_grad:
			counts[name] = param.numel()
	return counts

def print_trainable_params_summary(model):
	"""
	–ü–µ—á–∞—Ç–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –æ–±—É—á–∞–µ–º—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –ø–æ —Å–ª–æ—è–º.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
	"""
	counts = count_trainable_params_by_layer(model)
	total = sum(counts.values())

	print("–°–≤–æ–¥–∫–∞ –ø–æ –æ–±—É—á–∞–µ–º—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º:")
	print("-" * 60)
	for name, num_params in counts.items():
		print(f"{name}: {num_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
	print("-" * 60)
	print(f"–í—Å–µ–≥–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total:,}")



def apply_gradient_clipping_by_norm(model, max_norm):
	"""
	–ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—Ä–µ–∑–∫—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ –Ω–æ—Ä–º–µ L2.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		max_norm (float): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
	"""
	torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
	print(f"Gradient clipping (norm): max_norm={max_norm}")

def apply_gradient_clipping_by_value(model, clip_value):
	"""
	–ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—Ä–µ–∑–∫—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		clip_value (float): –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
	"""
	torch.nn.utils.clip_grad_value_(model.parameters(), clip_value)
	print(f"Gradient clipping (value): clip_value={clip_value}")

def track_gradient_statistics(model):
	"""
	–°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º (—Å—Ä–µ–¥–Ω–µ–µ, std, min, max).

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å

	Returns:
		dict: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
	"""
	stats = {}
	for name, param in model.named_parameters():
		if param.grad is not None:
			grad = param.grad.data
			stats[name] = {
				'mean': grad.mean().item(),
				'std': grad.std().item(),
				'min': grad.min().item(),
				'max': grad.max().item(),
				'norm': grad.norm().item()
			}
	return stats

def print_gradient_stats(stats):
	"""
	–ü–µ—á–∞—Ç–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º.

	Args:
		stats (dict): —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑ track_gradient_statistics
	"""
	print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤:")
	print("-! * 80)
	for name, s in stats.items():
		print(f"{name}:")
		print(f"  Mean: {s['mean']:.6f}, Std: {s['std']:.6f}")
		print(f"  Min: {s['min']:.6f}, Max: {s['max']:.6f}, Norm: {s['norm']:.6f}")
	print("-! * 80)

def initialize_weights_advanced(model, init_method='kaiming', nonlinearity='relu'):
	"""
	–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		init_method (str): –º–µ—Ç–æ–¥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ('xavier', 'kaiming', 'orthogonal')
		nonlinearity (str): –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å –¥–ª—è —É—á—ë—Ç–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
	"""
	def init_fn(m):
		if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):
			if init_method == 'xavier':
				torch.nn.init.xavier_uniform_(m.weight, gain=torch.nn.init.calculate_gain(nonlinearity))
			elif init_method == 'kaiming':
				torch.nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity=nonlinearity)
			elif init_method == 'orthogonal':
				torch.nn.init.orthogonal_(m.weight)
			else:
				raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {init_method}")

			if m.bias is not None:
				torch.nn.init.constant_(m.bias, 0)

	model.apply(init_fn)
	print(f"–í–µ—Å–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã ({init_method}, {nonlinearity})")

def replace_activation_in_model(model, old_act_type, new_act_module):
	"""
	–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∑–∞–º–µ–Ω—è–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤ –º–æ–¥–µ–ª–∏.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		old_act_type (type): —Ç–∏–ø —Å—Ç–∞—Ä–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
		new_act_module (nn.Module): –Ω–æ–≤—ã–π –º–æ–¥—É–ª—å –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
	"""
	for child in model.children():
		if isinstance(child, old_act_type):
			# –ó–∞–º–µ–Ω–∞ –Ω–∞ –º–µ—Å—Ç–µ
			parent = model
			for name, module in parent.named_children():
				if module is child:
					parent._modules[name] = new_act_module
					break
		else:
			replace_activation_in_model(child, old_act_type, new_act_module)
	print(f"–ó–∞–º–µ–Ω–µ–Ω—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–∏: {old_act_type} ‚Üí {new_act_module}")

def get_model_flops_and_params(model, input_shape, device='cuda'):
	"""
	–û—Ü–µ–Ω–∏–≤–∞–µ—Ç FLOPs –∏ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		input_shape (tuple): —Ñ–æ—Ä–º–∞ –≤—Ö–æ–¥–∞ (C, H, W)
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ

	Returns:
		tuple: (—á–∏—Å–ª–æ_–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, FLOPs)
	"""
	try:
		from thop import profile
		model.to(device)
		input = torch.randn(1, *input_shape).to(device)
		flops, params = profile(model, inputs=(input,), verbose=False)
		return params, flops
	except ImportError:
		print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ thop: pip install thop")
		return None, None

def summarize_model_efficiency(model, input_shape, device='cuda'):
	"""
	–í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–∫—É –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		input_shape (tuple): —Ñ–æ—Ä–º–∞ –≤—Ö–æ–¥–∞
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
	"""
	params, flops = get_model_flops_and_params(model, input_shape, device)
	if params is not None and flops is not None:
		print("–°–≤–æ–¥–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏:")
		print(f"  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {params:,}")
		print(f"  FLOPs: {flops:,}")
		print(f"  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã (–º–ª–Ω): {params/1e6:.2f}")
		print(f"  FLOPs (–º–ª—Ä–¥): {flops/1e9:.2f}")

def export_model_for_inference(model, example_input, output_path, format='torchscript'):
	"""
	–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.

	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		example_input (torch.Tensor): –ø—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–∞
		output_path (str): –ø—É—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
		format (str): —Ñ–æ—Ä–º–∞—Ç ('torchscript', 'onnx')
	"""
	model.eval()
	if format == 'torchscript':
		scripted_model = torch.jit.trace(model, exampleinput)
		torch.jit.save(scripted_model, output_path)
		print(f"–ú–æ–¥–µ–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ TorchScript: {output_path}")
	elif format == 'onnx':
		torch.onnx.export(
			model, exampleinput, output_path,
			export_params=True, opset_version=11,
			do_constant_folding=True,
			input_names=['input'], output_names=['output']
		)
		print(f"–ú–æ–¥–µ–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ ONNX: {output_path}")
	else:
		raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: {format}")




def create_data_augmentation_pipeline(
	resize=224,
	horizontal_flip=True,
	vertical_flip=False,
	rotation_range=15,
	color_jitter_params=None,
	normalize_mean=None,
	normalize_std=None
):
	"""
	–°–æ–∑–¥–∞—ë—Ç –ø–∞–π–ø–ª–∞–π–Ω –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.

	Args:
		resize (int or tuple): —Ä–∞–∑–º–µ—Ä –¥–ª—è —Ä–µ—Å–∞–π–∑–∞ (H, W) –∏–ª–∏ –æ–¥–Ω–æ —á–∏—Å–ª–æ (–¥–ª—è –∫–≤–∞–¥—Ä–∞—Ç–∞)
		horizontal_flip (bool): —Å–ª—É—á–∞–π–Ω—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π —Ñ–ª–∏–ø
		vertical_flip (bool): —Å–ª—É—á–∞–π–Ω—ã–π –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π —Ñ–ª–∏–ø
		rotation_range (int): –º–∞–∫—Å. —É–≥–æ–ª –ø–æ–≤–æ—Ä–æ—Ç–∞ (–≥—Ä–∞–¥—É—Å—ã)
		color_jitter_params (dict): –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ColorJitter (brightness, contrast –∏ –¥—Ä.)
		normalize_mean (list): —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (–ø–æ –∫–∞–Ω–∞–ª–∞–º)
		normalize_std (list): std –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (–ø–æ –∫–∞–Ω–∞–ª–∞–º)

	Returns:
		torchvision.transforms.Compose: –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π
	"""
	from torchvision import transforms

	augmentations = []

	if resize:
		if isinstance(resize, int):
			augmentations.append(transforms.Resize((resize, resize)))
		else:
			augmentations.append(transforms.Resize(resize))


	augmentations.append(transforms.RandomApply([
		transforms.RandomRotation(rotation_range)
	], p=0.5))


	if horizontal_flip:
		augmentations.append(transforms.RandomHorizontalFlip(p=0.5))
	if vertical_flip:
		augmentations.append(transforms.RandomVerticalFlip(p=0.5))


	if color_jitter_params:
		augmentations.append(transforms.ColorJitter(**color_jitter_params))


	augmentations.append(transforms.ToTensor())

	if normalize_mean and normalize_std:
		augmentations.append(
			transforms.Normalize(mean=normalize_mean, std=normalize_std)
		)

	return transforms.Compose(augmentations)


def create_test_transform(resize=224, normalize_mean=None, normalize_std=None):
	"""
	–°–æ–∑–¥–∞—ë—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π).


	Args:
		resize (int or tuple): —Ä–∞–∑–º–µ—Ä —Ä–µ—Å–∞–π–∑–∞
		normalize_mean (list): —Å—Ä–µ–¥–Ω–∏–µ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
		normalize_std (list): std –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏

	Returns:
		torchvision.transforms.Compose
	"""
	from torchvision import transforms
	transform = [
		transforms.Resize(resize if isinstance(resize, tuple) else (resize, resize)),
		transforms.ToTensor()
	]
	if normalize_mean and normalize_std:
		transform.append(transforms.Normalize(mean=normalize_mean, std=normalize_std))
	return transforms.Compose(transform)


def compute_dataset_statistics(dataloader, device='cuda'):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–∏–µ –∏ std –ø–æ –∫–∞–Ω–∞–ª–∞–º –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞.


	–ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.

	Args:
		dataloader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ

	Returns:
		tuple: (mean, std) ‚Äî —Ç–µ–Ω–∑–æ—Ä—ã —Ä–∞–∑–º–µ—Ä–∞ (C,)
	"""
	n_samples = 0
	channel_sums = torch.zeros(3, device=device)
	channel_sq_sums = torch.zeros(3, device=device)


	with torch.no_grad():
		for data, _ in dataloader:
			data = data.to(device)
			n_batch = data.size(0)
			channel_sums += data.sum(dim=[0, 2, 3])
			channel_sq_sums += (data ** 2).sum(dim=[0, 2, 3])
			n_samples += n_batch

	mean = channel_sums / (n_samples * data.size(2) * data.size(3))
	std = torch.sqrt(
		(channel_sq_sums / (n_samples * data.size(2) * data.size(3))) - mean ** 2
	)
	return mean.cpu(), std.cpu()


def visualize_predictions(model, data_loader, class_names, output_path, n_images=8, device='cuda'):
	"""
	–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö.


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		data_loader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
		class_names (list): –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
		n_images (int): —á–∏—Å–ª–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
	"""
	import matplotlib.pyplot as plt

	model.to(device).eval()
	fig, axes = plt.subplots(2, 4, figsize=(12, 6))
	axes = axes.ravel()

	with torch.no_grad():
		data, targets = next(iter(data_loader))
		data, targets = data[:n_images].to(device), targets[:n_images]
		outputs = model(data)
		_, preds = torch.max(outputs, 1)


		for i in range(n_images):
			img = data[i].cpu().permute(1, 2, 0).numpy()
			img = (img - img.min()) / (img.max() - img.min())  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
			axes[i].imshow(img)
			axes[i].set_title(f"True: {class_names[targets[i]]}\nPred: {class_names[preds[i]]}")
			axes[i].axis('off')

	plt.tight_layout()
	plt.savefig(output_path)
	plt.close()
	print(f"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")


def plot_confusion_matrix(cm, class_names, output_path, title='Confusion Matrix'):
	"""
	–†–∏—Å—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫ (confusion matrix).

	Args:
		cm (np.ndarray): –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
		class_names (list): –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤
		output_path (str): –ø—É—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
		title (str): –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä–∞—Ñ–∏–∫–∞
	"""
	import seaborn as sns
	import pandas as pd

	df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)
	plt.figure(figsize=(10, 8))
	sns.heatmap(df_cm, annot=True, fmt='d', cmap='Blues')
	plt.title(title)
	plt.ylabel('True Label')
	plt.xlabel('Predicted Label')
	plt.tight_layout()
	plt.savefig(output_path)
	plt.close()
	print(f"–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")

def calculate_class_accuracy(predictions, targets, num_classes):
	"""
	–°—á–∏—Ç–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É.

	Args:
		predictions (np.ndarray): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		targets (np.ndarray): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		num_classes (int): —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤


	Returns:
		np.ndarray: —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É (—Ä–∞–∑–º–µ—Ä: num_classes)
	"""
	accuracy_per_class = np.zeros(num_classes)
	for cls in range(num_classes):
		cls_mask = targets == cls
		if cls_mask.sum() > 0:
			accuracy_per_class[cls] = (
				(predictions[cls_mask] == targets[cls_mask]).mean()
			)
	return accuracy_per_class

def log_training_progress(epoch, train_loss, val_loss, train_acc, val_acc, log_file):
	"""
	–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –≤ –ª–æ–≥‚Äë—Ñ–∞–π–ª.

	Args:
		epoch (int): –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏
		train_loss (float): —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è –ø–æ—Ç–µ—Ä—è
		val_loss (float): –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–æ—Ç–µ—Ä—è
		train_acc (float): —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
		val_acc (float): –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
		log_file (str): –ø—É—Ç—å –∫ –ª–æ–≥‚Äë—Ñ–∞–π–ª—É
	"""
	with open(log_file, 'a') as f:
		f.write(f"{epoch},{train_loss:.6f},{val_loss:.6f},"
				f"{train_acc:.6f},{val_acc:.6f}\n")
	print(f"–ü—Ä–æ–≥—Ä–µ—Å—Å —ç–ø–æ—Ö–∏ {epoch} –∑–∞–ø–∏—Å–∞–Ω –≤ {log_file}")


def setup_early_stopping(patience=5, min_delta=0.001):
	"""
	–°–æ–∑–¥–∞—ë—Ç –æ–±—ä–µ–∫—Ç –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–±—É—á–µ–Ω–∏—è (early stopping).


	Args:
		patience (int): —á–∏—Å–ª–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è, –ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä–æ–≥–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
		min_delta (float): –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (–∏–Ω–∞—á–µ —Å—á–∏—Ç–∞–µ—Ç—Å—è –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ–º —É–ª—É—á—à–µ–Ω–∏—è)


	Returns:
		dict: –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è early stopping
	"""
	return {
		'patience': patience,
		'min_delta': min_delta,
		'best_score': None,
		'counter': 0,
		'early_stop': False
	}

def check_early_stopping(early_stopper, val_loss, epoch):
	"""
	–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—é early stopping.


	Args:
		early_stopper (dict): –æ–±—ä–µ–∫—Ç early stopping –∏–∑ setup_early_stopping
		val_loss (float): —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–æ—Ç–µ—Ä–∏
		epoch (int): –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏


	Returns:
		bool: True, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ
	"""
	score = -val_loss  # –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º, —Ç.–∫. –∏—â–µ–º –º–∏–Ω–∏–º—É–º –ø–æ—Ç–µ—Ä–∏


	if early_stopper['best_score'] is None:
		early_stopper['best_score'] = score
	elif score < early_stopper['best_score'] + early_stopper['min_delta']:
		early_stopper['counter'] += 1
		print(f"EarlyStopping: {early_stopper['counter']}/{early_stopper['patience']} (epoch {epoch})")
		if early_stopper['counter'] >= early_stopper['patience']:
			early_stopper['early_stop'] = True
			print(f"Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch}!")
	else:
		early_stopper['best_score'] = score
		early_stopper['counter'] = 0


	return early_stopper['early_stop']


def save_best_model(model, optimizer, epoch, val_loss, best_loss, checkpoint_path):
	"""
	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ —Ç–µ–∫—É—â–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–æ—Ç–µ—Ä—è –ª—É—á—à–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –ª—É—á—à–µ–π.


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		optimizer (torch.optim.Optimizer): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
		epoch (int): –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏
		val_loss (float): —Ç–µ–∫—É—â–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–æ—Ç–µ—Ä—è
		best_loss (float): –ª—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–æ—Ç–µ—Ä—è –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç
		checkpoint_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–∞


	Returns:
		float: –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ best_loss
	"""
	if val_loss < best_loss:
		print(f"–ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –Ω–∞ —ç–ø–æ—Ö–µ {epoch}: val_loss={val_loss:.6f} (–±—ã–ª–æ {best_loss:.6f})")
		torch.save({
			'epoch': epoch,
			'model_state_dict': model.state_dict(),
			'optimizer_state_dict': optimizer.state_dict(),
			'val_loss': val_loss
		}, checkpoint_path)
		best_loss = val_loss
	return best_loss

def load_best_model(model, checkpoint_path, optimizer=None):
	"""
	–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞.


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å (–¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤)
		checkpoint_path (str): –ø—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É
		optimizer (torch.optim.Optimizer, optional): –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ)


	Returns:
		int: –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–π –±—ã–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å
	"""
	checkpoint = torch.load(checkpoint_path)
	model.load_state_dict(checkpoint['model_state_dict'])
	if optimizer:
		optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
	print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {checkpoint_path}, —ç–ø–æ—Ö–∞ {checkpoint['epoch']}")
	return checkpoint['epoch']


def calculate_precision_recall_f1(y_true, y_pred, num_classes):
	"""
	–°—á–∏—Ç–∞–µ—Ç precision, recall –∏ F1-score –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É.


	Args:
		y_true (np.ndarray): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		y_pred (np.ndarray): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
		num_classes (int): —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤


	Returns:
		dict: {'precision': ..., 'recall': ..., 'f1': ...} ‚Äî –º–∞—Å—Å–∏–≤—ã —Ä–∞–∑–º–µ—Ä–∞ num_classes
	"""
	precision = np.zeros(num_classes)
	recall = np.zeros(num_classes)
	f1 = np.zeros(num_classes)


	for cls in range(num_classes):
		true_positive = ((y_true == cls) & (y_pred == cls)).sum()
		false_positive = ((y_true != cls) & (y_pred == cls)).sum()
		false_negative = ((y_true == cls) & (y_pred != cls)).sum()


		prec = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0
		rec = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0
		f1_score = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0

		precision[cls] = prec
		recall[cls] = rec
		f1[cls] = f1_score

	return {'precision': precision, 'recall': recall, 'f1': f1}


def plot_training_curves(train_losses, val_losses, train_accs, val_accs, output_path):
	"""
	–†–∏—Å—É–µ—Ç –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è (–ø–æ—Ç–µ—Ä—è –∏ —Ç–æ—á–Ω–æ—Å—Ç—å).


	Args:
		train_losses (list): —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏ –ø–æ —ç–ø–æ—Ö–∞–º
		val_losses (list): –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏ –ø–æ —ç–ø–æ—Ö–∞–º
		train_accs (list): —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —ç–ø–æ—Ö–∞–º
		val_accs (list): –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —ç–ø–æ—Ö–∞–º
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
	"""
	epochs = len(train_losses)
	plt.figure(figsize=(14, 5))

	plt.subplot(1, 2, 1)
	plt.plot(range(epochs), train_losses, label='Train Loss')
	plt.plot(range(epochs), val_losses, label='Val Loss')
	plt.xlabel('Epoch')
	plt.ylabel('Loss')
	plt.title('Training and Validation Loss')
	plt.legend()
	plt.grid(True)

	plt.subplot(1, 2, 2)
	plt.plot(range(epochs), train_accs, label='Train Acc')
	plt.plot(range(epochs), val_accs, label='Val Acc')
	plt.xlabel('Epoch')
	plt.ylabel('Accuracy')
	plt.title('Training and Validation Accuracy')
	plt.legend()
	plt.grid(True)

	plt.tight_layout()
	plt.savefig(output_path)
	plt.close()
	print(f"–ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")

def compute_roc_auc(model, data_loader, num_classes, device='cuda'):
	"""
	–°—á–∏—Ç–∞–µ—Ç ROC AUC –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (one-vs-rest).


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		data_loader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
		num_classes (int): —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ('cuda' –∏–ª–∏ 'cpu')

	Returns:
		np.ndarray: –º–∞—Å—Å–∏–≤ AUC –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ (—Ä–∞–∑–º–µ—Ä: num_classes)
	"""
	from sklearn.metrics import roc_auc_score
	import numpy as np

	model.eval()
	y_true = []
	y_scores = []

	# –°–æ–±–∏—Ä–∞–µ–º –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
	with torch.no_grad():
		for data, target in data_loader:
			data, target = data.to(device), target.to(device)
			# –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ softmax
			output = torch.nn.functional.softmax(model(data), dim=1)
			y_true.extend(target.cpu().numpy())
			y_scores.extend(output.cpu().numpy())


	y_true = np.array(y_true)
	y_scores = np.array(y_scores)


	aucs = []
	for cls in range(num_classes):
		try:
			# –ë–∏–Ω–∞—Ä–∏–∑—É–µ–º –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∞ `cls`
			y_true_binary = (y_true == cls).astype(int)

			# –ë–µ—Ä—ë–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏–º–µ–Ω–Ω–æ –¥–ª—è –∫–ª–∞—Å—Å–∞ `cls`
			y_score_cls = y_scores[:, cls]


			# –í—ã—á–∏—Å–ª—è–µ–º AUC
			auc = roc_auc_score(y_true_binary, y_score_cls)
			aucs.append(auc)

		except ValueError as e:
			# –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫:
			# - –ù–µ—Ç –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Å–∞ `cls` –≤ –≤—ã–±–æ—Ä–∫–µ (–≤—Å–µ –º–µ—Ç–∫–∏ 0)
			# - –í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã (–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å ROC)
			print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ AUC –¥–ª—è –∫–ª–∞—Å—Å–∞ {cls}: {e}")
			aucs.append(0.0)  # –ò–ª–∏ np.nan –¥–ª—è —è–≤–Ω–æ–≥–æ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–∞

		except Exception as e:
			print(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è –∫–ª–∞—Å—Å–∞ {cls}: {e}")
			aucs.append(0.0)

	return np.array(aucs)



def plot_roc_curves(y_true, y_scores, num_classes, class_names=None, output_path=None):
	"""
	–†–∏—Å—É–µ—Ç ROC‚Äë–∫—Ä–∏–≤—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ (one‚Äëvs‚Äërest).


	Args:
		y_true (np.ndarray): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (N,)
		y_scores (np.ndarray): –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤ (N, num_classes)
		num_classes (int): —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤
		classnames (list of str, optional): –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤
		output_path (str, optional): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
	"""
	from sklearn.metrics import roc_curve, auc
	import matplotlib.pyplot as plt


	plt.figure(figsize=(10, 8))
	if classnames is None:
		classnames = [f"–ö–ª–∞—Å—Å {i}" for i in range(num_classes)]


	for cls in range(num_classes):
		fpr, tpr, _ = roc_curve(y_true == cls, y_scores[:, cls])
		roc_auc = auc(fpr, tpr)
		plt.plot(fpr, tpr, label=f'{classnames[cls]} (AUC = {roc_auc:.3f})')


	plt.plot([0, 1], [0, 1], 'k--', label='–°–ª—É—á–∞–π–Ω–∞—è –º–æ–¥–µ–ª—å')
	plt.xlim([0.0, 1.0])
	plt.ylim([0.0, 1.05])
	plt.xlabel('–õ–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ–ª—è (FPR)')
	plt.ylabel('–ò—Å—Ç–∏–Ω–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ–ª—è (TPR)')
	plt.title('ROC-–∫—Ä–∏–≤—ã–µ (–æ–¥–∏–Ω –ø—Ä–æ—Ç–∏–≤ –≤—Å–µ—Ö)')
	plt.legend(loc="lower right")
	plt.grid(True)


	if output_path:
		plt.savefig(output_path, bbox_inches='tight')
		print(f"ROC-–∫—Ä–∏–≤—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
	plt.close()



def evaluate_roc_auc_and_plot(model, data_loader, num_classes, classnames=None, device='cuda', output_dir=None):
	"""
	–í—ã—á–∏—Å–ª—è–µ—Ç AUC –∏ —Ä–∏—Å—É–µ—Ç ROC-–∫—Ä–∏–≤—ã–µ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∑–∞–¥–∞—á–∏.


	Args:
		model (nn.Module): –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
		data_loader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
		num_classes (int): —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤
		classnames (list): –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
		output_dir (str): –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)


	Returns:
		dict: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (AUC –ø–æ –∫–ª–∞—Å—Å–∞–º, —Å—Ä–µ–¥–Ω–∏–π AUC)
	"""
	# –í—ã—á–∏—Å–ª—è–µ–º AUC –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
	aucs = compute_roc_auc(model, data_loader, num_classes, device)

	mean_auc = np.mean(aucs)


	# –°–æ–±–∏—Ä–∞–µ–º y_true –∏ y_scores –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
	y_true, y_scores = [], []
	model.eval()
	with torch.no_grad():
		for data, target in data_loader:
			data, target = data.to(device), target.to(device)
			output = torch.nn.functional.softmax(model(data), dim=1)
			y_true.extend(target.cpu().numpy())
			y_scores.extend(output.cpu().numpy())
	y_true = np.array(y_true)
	y_scores = np.array(y_scores)

	# –†–∏—Å—É–µ–º ROC-–∫—Ä–∏–≤—ã–µ, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
	if output_dir:
		import os
		os.makedirs(output_dir, exist_ok=True)
		plot_roc_curves(
			y_true, y_scores, num_classes,
			classnames,
			output_path=os.path.join(output_dir, 'roc_curves.png')
		)

	return {
		'auc_per_class': aucs,
		'mean_auc': mean_auc,
		'y_true': y_true,
		'y_scores': y_scores
	}



# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
	import torch
	from torch.utils.data import DataLoader
	from torchvision import datasets, transforms

	# –ü—Ä–∏–º–µ—Ä: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
	transform = transforms.Compose([
		transforms.Resize((224, 224)),
		transforms.ToTensor(),
	])
	dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
	data_loader = DataLoader(dataset, batch_size=32, shuffle=False)

	# –ü—Ä–∏–º–µ—Ä –º–æ–¥–µ–ª–∏ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à—É)
	model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)
	model.fc = torch.nn.Linear(model.fc.in_features, 10)  # 10 –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è CIFAR-10

	device = 'cuda' if torch.cuda.is_available() else 'cpu'
	model.to(device)

	# –û—Ü–µ–Ω–∫–∞ AUC –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
	results = evaluate_roc_auc_and_plot(
		model, data_loader, num_classes=10,
		classnames=['airplane', 'automobile', 'bird', 'cat', 'deer',
				   'dog', 'frog', 'horse', 'ship', 'truck'],
		device=device,
		output_dir='./results'
	)

	print(f"AUC –ø–æ –∫–ª–∞—Å—Å–∞–º: {results['auc_per_class']}")
	print(f"–°—Ä–µ–¥–Ω–∏–π AUC: {results['mean_auc']:.4f}")

	return results




def calculate_precision_recall_f1(y_true, y_pred, num_classes):
	"""
	–°—á–∏—Ç–∞–µ—Ç precision, recall –∏ F1-score –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É.


	Args:
		y_true (np.ndarray): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (N,)
		y_pred (np.ndarray): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (N,)
		num_classes (int): —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤

	Returns:
		dict: {'precision': ..., 'recall': ..., 'f1': ...} ‚Äî –º–∞—Å—Å–∏–≤—ã —Ä–∞–∑–º–µ—Ä–∞ num_classes
	"""
	precision = np.zeros(num_classes)
	recall = np.zeros(num_classes)
	f1 = np.zeros(num_classes)

	for cls in range(num_classes):
		# True Positive: –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã –∫–ª–∞—Å—Å–∞ cls
		tp = ((y_true == cls) & (y_pred == cls)).sum()

		# False Positive: –æ–±—Ä–∞–∑—Ü—ã, –æ—à–∏–±–æ—á–Ω–æ –æ—Ç–Ω–µ—Å—ë–Ω–Ω—ã–µ –∫ –∫–ª–∞—Å—Å—É cls
		fp = ((y_true != cls) & (y_pred == cls)).sum()
		# False Negative: –æ–±—Ä–∞–∑—Ü—ã –∫–ª–∞—Å—Å–∞ cls, –æ—à–∏–±–æ—á–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–∞–∫ –¥—Ä—É–≥–∏–µ
		fn = ((y_true == cls) & (y_pred != cls)).sum()


		# Precision: –¥–æ–ª—è –≤–µ—Ä–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–∞–∫ cls
		prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
		# Recall: –¥–æ–ª—è –≤–µ—Ä–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –∏—Å—Ç–∏–Ω–Ω—ã—Ö cls
		rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0
		# F1: —Å—Ä–µ–¥–Ω–µ–µ –≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ precision –∏ recall
		f1_score = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0

		precision[cls] = prec
		recall[cls] = rec
		f1[cls] = f1_score

	return {'precision': precision, 'recall': recall, 'f1': f1}




def plot_confusion_matrix(cm, class_names, output_path, title='–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫'):
	"""
	–†–∏—Å—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫ (confusion matrix).


	Args:
		cm (np.ndarray): –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (num_classes, num_classes)
		class_names (list): –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
		title (str): –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä–∞—Ñ–∏–∫–∞
	"""
	import seaborn as sns
	import pandas as pd
	import matplotlib.pyplot as plt

	df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)
	plt.figure(figsize=(10, 8))
	sns.heatmap(df_cm, annot=True, fmt='d', cmap='Blues', cbar=True)
	plt.title(title)
	plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
	plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
	plt.tight_layout()
	plt.savefig(output_path, dpi=300)
	plt.close()
	print(f"–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")




def evaluate_model_full(model, data_loader, num_classes, class_names=None, device='cuda', output_dir=None):
	"""
	–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏: —Ç–æ—á–Ω–æ—Å—Ç—å, precision, recall, F1, AUC, –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫, ROC‚Äë–∫—Ä–∏–≤—ã–µ.


	Args:
		model (nn.Module): –º–æ–¥–µ–ª—å
		data_loader (DataLoader): –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
		num_classes (int): —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤
		class_names (list of str, optional): –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤
		device (str): —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
		output_dir (str, optional): –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤


	Returns:
		dict: –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
	"""
	model.eval()
	y_true = []
	y_pred = []
	y_scores = []

	with torch.no_grad():
		for data, target in data_loader:
			data, target = data.to(device), target.to(device)
			output = model(data)
			probs = torch.nn.functional.softmax(output, dim=1)

			_, pred = torch.max(output, 1)


			y_true.extend(target.cpu().numpy())
			y_pred.extend(pred.cpu().numpy())
			y_scores.extend(probs.cpu().numpy())


	y_true = np.array(y_true)
	y_pred = np.array(y_pred)
	y_scores = np.array(y_scores)


	# 1. –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
	accuracy = (y_pred == y_true).mean()


	# 2. Precision, Recall, F1 –ø–æ –∫–ª–∞—Å—Å–∞–º
	metrics = calculate_precision_recall_f1(y_true, y_pred, num_classes)


	# 3. AUC (one-vs-rest)
	aucs = compute_roc_auc(model, data_loader, num_classes, device)


	# 4. –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
	cm = calculate_confusion_matrix(y_true, y_pred, num_classes)


	results = {
		'accuracy': accuracy,
		'precision': metrics['precision'],
		'recall': metrics['recall'],
		'f1': metrics['f1'],
		'auc': aucs,
		'confusion_matrix': cm,
		'y_true': y_true,
		'y_pred': y_pred,
		'y_scores': y_scores
	}

	# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
	if output_dir:
		import os
		os.makedirs(output_dir, exist_ok=True)

		# ROC-–∫—Ä–∏–≤—ã–µ
		plot_roc_curves(
			y_true, y_scores, num_classes,
			classnames=classnames,
			output_path=os.path.join(output_dir, 'roc_curves.png')
		)

		# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
		plot_confusion_matrix(
			cm, classnames or [f"–ö–ª–∞—Å—Å {i}" for i in range(num_classes)],
			output_path=os.path.join(output_dir, 'confusion_matrix.png'),
			title='–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫'
		)

	return results



def print_evaluation_summary(results, class_names=None):
	"""
	–ü–µ—á–∞—Ç–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏.

	Args:
		results (dict): —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ evaluate_model_full
		class_names (list of str, optional): –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤
	"""
	num_classes = len(results['precision'])
	classnames = class_names or [f"–ö–ª–∞—Å—Å {i}" for i in range(num_classes)]

	print("=" * 60)
	print("–°–í–û–î–ö–ê –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ò")
	print("=" * 60)
	print(f"–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy): {results['accuracy']:.4f}")
	print("\n–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
	print("-" * 60)
	for i in range(num_classes):
		print(f"{classnames[i]:<12} "
			  f"Precision={results['precision'][i]:.4f} "
			  f"Recall={results['recall'][i]:.4f} "
			  f"F1={results['f1'][i]:.4f} "
			  f"AUC={results['auc'][i]:.4f}")
	print("-" * 60)
	print(f"–°—Ä–µ–¥–Ω–∏–π AUC: {results['auc'].mean():.4f}")
	print("=" * 60)



# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
	import torch
	from torch.utils.data import DataLoader
	from torchvision import datasets, transforms

	# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
	num_classes = 10
	device = 'cuda' if torch.cuda.is_available() else 'cpu'
	output_dir = './evaluation_results'


	# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
	transform = transforms.Compose([
		transforms.Resize((224, 224)),
		transforms.ToTensor(),
		transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
	])

	# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–∏–º–µ—Ä: CIFAR-10)
	dataset = datasets.CIFAR10(
		root='./data', train=False, download=True, transform=transform
	)
	data_loader = DataLoader(dataset, batch_size=32, shuffle=False)

	# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–ø—Ä–∏–º–µ—Ä —Å ResNet18)
	model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)
	model.fc = torch.nn.Linear(model.fc.in_features, num_classes)  # –∑–∞–º–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
	model.to(device)

	# –ò–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è CIFAR-10
	class_names = [
		'airplane', 'automobile', 'bird', 'cat', 'deer',
		'dog', 'frog', 'horse', 'ship', 'truck'
	]

	# –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
	results = evaluate_model_full(
		model=model,
		data_loader=data_loader,
		num_classes=num_classes,
		class_names=class_names,
		device=device,
		output_dir=output_dir
	)

	# –í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏
	print_evaluation_summary(results, class_names)


	# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –≤—ã–≤–æ–¥ —Å—Ä–µ–¥–Ω–µ–≥–æ F1
	mean_f1 = results['f1'].mean()
	print(f"\n–°—Ä–µ–¥–Ω–∏–π F1-score: {mean_f1:.4f}")


	# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª
	import json
	with open(os.path.join(output_dir, 'evaluation_results.json'), 'w') as f:
		# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy-–º–∞—Å—Å–∏–≤—ã –≤ —Å–ø–∏—Å–∫–∏ –¥–ª—è JSON
		results_for_json = {
			k: (v.tolist() if isinstance(v, np.ndarray) else v)
			for k, v in results.items()
		}
		json.dump(results_for_json, f, indent=2)
	print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_dir}/evaluation_results.json")



def calculate_confusion_matrix(y_true, y_pred, num_classes):
	"""
	–°—á–∏—Ç–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫ (confusion matrix).

	Args:
		y_true (np.ndarray): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (N,)
		y_pred (np.ndarray): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (N,)
		num_classes (int): —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤


	Returns:
		np.ndarray: –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Ä–∞–∑–º–µ—Ä–∞ (num_classes, num_classes)
	"""
	cm = np.zeros((num_classes, num_classes), dtype=int)
	for i in range(len(y_true)):
		cm[y_true[i], y_pred[i]] += 1
	return cm



def plot_training_curves(train_losses, val_losses, train_accs, val_accs, output_path):
	"""
	–†–∏—Å—É–µ—Ç –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è (–ø–æ—Ç–µ—Ä—è –∏ —Ç–æ—á–Ω–æ—Å—Ç—å).


	Args:
		train_losses (list): —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏ –ø–æ —ç–ø–æ—Ö–∞–º
		val_losses (list): –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏ –ø–æ —ç–ø–æ—Ö–∞–º
		train_accs (list): —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —ç–ø–æ—Ö–∞–º
		val_accs (list): –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —ç–ø–æ—Ö–∞–º
		output_path (str): –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
	"""
	epochs = len(train_losses)
	plt.figure(figsize=(14, 5))

	plt.subplot(1, 2, 1)
	plt.plot(range(epochs), train_losses, label='Train Loss')
	plt.plot(range(epochs), val_losses, label='Val Loss')
	plt.xlabel('–≠–ø–æ—Ö–∞')
	plt.ylabel('–ü–æ—Ç–µ—Ä—è')
	plt.title('–ü–æ—Ç–µ—Ä—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
	plt.legend()
	plt.grid(True)


	plt.subplot(1, 2, 2)
	plt.plot(range(epochs), train_accs, label='Train Acc')
	plt.plot(range(epochs), val_accs, label='Val Acc')
	plt.xlabel('–≠–ø–æ—Ö–∞')
	plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
	plt.title('–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏')
	plt.legend()
	plt.grid(True)

	plt.tight_layout()
	plt.savefig(output_path, dpi=300)
	plt.close()
	print(f"–ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")


def save_training_log(epoch, train_loss, val_loss, train_acc, val_acc, log_path):
	"""
	–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–≥ –æ–±—É—á–µ–Ω–∏—è –≤ CSV-—Ñ–∞–π–ª.


	Args:
		epoch (int): –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏
		train_loss (float): –ø–æ—Ç–µ—Ä—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏
		val_loss (float): –ø–æ—Ç–µ—Ä—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
		train_acc (float): —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏
		val_acc (float): —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
		log_path (str): –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –ª–æ–≥–∞
	"""
	import csv
	with open(log_path, 'a', newline='') as f:
		writer = csv.writer(f)
		if f.tell() == 0:  # –µ—Å–ª–∏ —Ñ–∞–π–ª –ø—É—Å—Ç–æ–π ‚Äî –ø–∏—à–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
			writer.writerow(['epoch', 'train_loss', 'val_loss', 'train_acc', 'val_acc'])
		writer.writerow([epoch, train_loss, val_loss, train_acc, val_acc])


def load_training_log(log_path):
	"""
	–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–æ–≥ –æ–±—É—á–µ–Ω–∏—è –∏–∑ CSV-—Ñ–∞–π–ª–∞.

	Args:
		log_path (str): –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –ª–æ–≥–∞

	Returns:
		dict: {'epochs': [...], 'train_losses': [...], ...}
	"""
	import pandas as pd
	df = pd.read_csv(log_path)
	return {
		'epochs': df['epoch'].tolist(),
		'train_losses': df['train_loss'].tolist(),
		'val_losses': df['val_loss'].tolist(),
		'train_accs': df['train_acc'].tolist(),
		'val_accs': df['val_acc'].tolist()
	}




