print("=" * 80)
print("üß† –û–ë–£–ß–ï–ù–ò–ï –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–û–ì–û –ë–û–¢–ê - –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –í–ï–†–°–ò–Ø")
print("=" * 80)

# –î–û–ë–ê–í–¨–¢–ï –í–´–ë–û–† –†–ï–ñ–ò–ú–ê
print("\nüîß –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º –≤—ã–≤–æ–¥–∞:")
print("1. üöÄ –ë—ã—Å—Ç—Ä—ã–π (—Ç–æ–ª—å–∫–æ LR, Loss, –ø—Ä–æ–≥—Ä–µ—Å—Å)")
print("2. üêõ –û—Ç–ª–∞–¥–æ—á–Ω—ã–π (–≤—Å–µ –¥–µ—Ç–∞–ª–∏)")
print("3. üìä –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π (–º–µ—Ç—Ä–∏–∫–∏ + –≥—Ä–∞—Ñ–∏–∫–∏)")
debug_mode = input("–í—ã–±–µ—Ä–∏—Ç–µ (1-3, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1): ").strip() or "1"
DEBUG_MODE = int(debug_mode)


import time
import torch
from pathlib import Path
from transformers import GPTNeoForCausalLM, GPT2Tokenizer, BitsAndBytesConfig
import bitsandbytes as bnb
from datetime import datetime
import os
import sys
import time
import math
import json
import numpy as np

print("=" * 80)
print("üß† –û–ë–£–ß–ï–ù–ò–ï –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–û–ì–û –ë–û–¢–ê - –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –í–ï–†–°–ò–Ø")
print("   –° –ê–î–ê–ü–¢–ò–í–ù–´–ú–ò –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò –ò –ú–ï–¢–†–ò–ö–ê–ú–ò")
print("=" * 80)

# ================= –ü–†–ê–í–ò–õ–¨–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ =================
BATCH_SIZE = 3 
MAX_LENGTH = 729
GRADIENT_ACCUMULATION = 9
LEARNING_RATE = 2e-4
print("–í–≤–µ–¥–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö...")
EPOCHS = int(input())
WARMUP_RATIO = 0.9

print(f"\nüéØ –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø:")
print(f"   ‚Ä¢ Batch size: {BATCH_SIZE}")
print(f"   ‚Ä¢ Max length: {MAX_LENGTH}")
print(f"   ‚Ä¢ Gradient accumulation: {GRADIENT_ACCUMULATION}")
print(f"   ‚Ä¢ Learning rate: {LEARNING_RATE:.1e}")
print(f"   ‚Ä¢ Epochs: {EPOCHS}")
print(f"   ‚Ä¢ Warmup: {WARMUP_RATIO*100}%")

# ================= –ö–û–ù–¢–ï–ö–°–¢–ù–´–ï –ú–ï–ù–ï–î–ñ–ï–†–´ –î–õ–Ø –†–ï–ñ–ò–ú–û–í =================

class TrainingMode:
	"""–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è"""
	def __init__(self, model):
		self.model = model
		self.original_cache = None
		self.original_training = None
	
	def __enter__(self):
		self.original_cache = self.model.config.use_cache
		self.original_training = self.model.training
		self.model.config.use_cache = False  # ‚õî –í—ã–∫–ª—é—á–∞–µ–º –∫—ç—à
		self.model.gradient_checkpointing_enable()  # ‚úÖ –í–∫–ª—é—á–∞–µ–º checkpointing
		self.model.train()  # ‚úÖ –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
		return self
	
	def __exit__(self, exc_type, exc_val, exc_tb):
		self.model.config.use_cache = self.original_cache
		if not self.original_training:
			self.model.eval()

class ValidationMode:
	"""–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–µ–∂–∏–º–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ loss)"""
	def __init__(self, model):
		self.model = model
		self.original_cache = None
		self.original_training = None
		self.original_gradient_checkpointing = None
	
	def __enter__(self):
		self.original_cache = self.model.config.use_cache
		self.original_training = self.model.training
		self.original_gradient_checkpointing = self.model.is_gradient_checkpointing
		
		self.model.config.use_cache = False  # ‚õî –ö—ç—à –Ω–µ –Ω—É–∂–µ–Ω
		if self.model.is_gradient_checkpointing:  # ‚õî –í—ã–∫–ª—é—á–∞–µ–º checkpointing –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
			try:
				self.model.gradient_checkpointing_disable()
			except:
				self.model.gradient_checkpointing = False
		self.model.eval()  # ‚úÖ –†–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
		return self
	
	def __exit__(self, exc_type, exc_val, exc_tb):
		self.model.config.use_cache = self.original_cache
		if self.original_gradient_checkpointing and not self.model.is_gradient_checkpointing:
			self.model.gradient_checkpointing_enable()
		if self.original_training:
			self.model.train()

class GenerationMode:
	"""–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–µ–∂–∏–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Ç–µ—Å—Ç—ã/–∏–Ω—Ñ–µ—Ä–µ–Ω—Å)"""
	def __init__(self, model):
		self.model = model
		self.original_cache = None
		self.original_training = None
		self.original_gradient_checkpointing = None
	
	def __enter__(self):
		self.original_cache = self.model.config.use_cache
		self.original_training = self.model.training
		self.original_gradient_checkpointing = self.model.is_gradient_checkpointing
		
		self.model.config.use_cache = True  # ‚úÖ –í–∫–ª—é—á–∞–µ–º –∫—ç—à –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
		if self.model.is_gradient_checkpointing:  # ‚õî –í—ã–∫–ª—é—á–∞–µ–º checkpointing –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
			try:
				self.model.gradient_checkpointing_disable()
			except:
				self.model.gradient_checkpointing = False
		self.model.eval()  # ‚úÖ –†–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
		return self
	
	def __exit__(self, exc_type, exc_val, exc_tb):
		self.model.config.use_cache = self.original_cache
		if self.original_gradient_checkpointing and not self.model.is_gradient_checkpointing:
			self.model.gradient_checkpointing_enable()
		if self.original_training:
			self.model.train()

# ================= –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ô –®–ï–î–£–õ–ï–† =================

class OptimalScheduler:
	"""
	–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —à–µ–¥—É–ª–µ—Ä –¥–ª—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏
	Warmup ‚Üí Cosine Decay ‚Üí Linear Final
	"""
	
	def __init__(self, optimizer, total_steps, initial_lr, warmup_ratio=0.10):
		self.optimizer = optimizer
		self.total_steps = total_steps
		self.initial_lr = initial_lr
		self.warmup_steps = int(total_steps * warmup_ratio)
		self.cosine_steps = int(total_steps * 0.6)
		self.linear_steps = total_steps - self.warmup_steps - self.cosine_steps
		self.current_step = 0
		
		print(f"\nüéØ –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ô –®–ï–î–£–õ–ï–† (3 —Ñ–∞–∑—ã):")
		print(f"   ‚Ä¢ –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_steps}")
		print(f"   ‚Ä¢ Warmup: {self.warmup_steps} —à–∞–≥–æ–≤ ({warmup_ratio*100}%)")
		print(f"   ‚Ä¢ Cosine decay: {self.cosine_steps} —à–∞–≥–æ–≤ (60%)")
		print(f"   ‚Ä¢ Linear final: {self.linear_steps} —à–∞–≥–æ–≤ (–æ—Å—Ç–∞–ª—å–Ω–æ–µ)")
	
	def step(self):
		"""–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω —à–∞–≥ —à–µ–¥—É–ª–µ—Ä–∞"""
		self.current_step += 1
		
		if self.current_step <= self.warmup_steps:
			# 1. Warmup: –ª–∏–Ω–µ–π–Ω—ã–π —Ä–æ—Å—Ç
			lr = self.initial_lr * (self.current_step / self.warmup_steps)
			phase = "WARMUP"
			
		elif self.current_step <= self.warmup_steps + self.cosine_steps:
			# 2. Cosine decay
			progress = (self.current_step - self.warmup_steps) / self.cosine_steps
			lr = self.initial_lr * 0.5 * (1 + math.cos(math.pi * progress))
			phase = "COSINE"
			
		else:
			# 3. –õ–∏–Ω–µ–π–Ω–æ–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ
			progress = (self.current_step - self.warmup_steps - self.cosine_steps) / self.linear_steps
			lr = self.initial_lr * 0.1 * (1 - progress * 0.5)
			phase = "FINAL"
		
		# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º LR –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
		for param_group in self.optimizer.param_groups:
			param_group['lr'] = lr
		
		return lr, phase
# ================= –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò =================

def get_gpu_power():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –º–æ—â–Ω–æ—Å—Ç–∏ GPU –≤ –≤–∞—Ç—Ç–∞—Ö"""
    try:
        import pynvml
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # –º–í—Ç ‚Üí –í—Ç
        pynvml.nvmlShutdown()
        return f"{power:.0f}"
    except:
        return "N/A"


# ================= –ü–†–û–î–í–ò–ù–£–¢–´–ô –ú–û–ù–ò–¢–û–†–ò–ù–ì =================
class AdvancedTrainingMonitor:
	def __init__(self, log_dir, debug_mode=1):
		self.debug_mode = debug_mode
		
		if self.debug_mode >= 2:  # –¢–æ–ª—å–∫–æ –≤ –æ—Ç–ª–∞–¥–æ—á–Ω–æ–º —Ä–µ–∂–∏–º–µ
			print(f"\nüîß [DEBUG] –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞...")
			
		self.log_dir = Path(log_dir)
		self.log_dir.mkdir(parents=True, exist_ok=True)
		
		self.metrics = {
			'loss': [], 'lr': [], 'grad_norm': [],
			'step_time': [], 'memory_usage': [],
			'perplexity': [], 'empathy_score': []
		}
		self.quality_scores = []
		self.error_log = self.log_dir / "generation_errors.log"
		
		# –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
		self.empathy_words = [
			"–ø–æ–Ω–∏–º–∞—é", "—á—É–≤—Å—Ç–≤—É—é", "–≤–∞–∂–Ω–æ", "—Ü–µ–Ω—é", "–ø—Ä–∏–Ω–∏–º–∞—é", 
			"—Å–ø–∞—Å–∏–±–æ", "—Å–ª—ã—à—É", "–≤–∏–∂—É", "–∑–∞–º–µ—á–∞—é", "—É–≤–∞–∂–∞—é",
			"—Å–æ–ø–µ—Ä–µ–∂–∏–≤–∞—é", "—Ä–∞–∑–¥–µ–ª—è—é", "–æ—Å–æ–∑–Ω–∞—é", "–ø—Ä–∏–∑–Ω–∞—é"
		]
		
		self.advice_words = [
			"–¥–æ–ª–∂–µ–Ω", "–Ω–∞–¥–æ", "–æ–±—è–∑–∞–Ω", "—Ä–µ–∫–æ–º–µ–Ω–¥—É—é", 
			"—Å–æ–≤–µ—Ç—É—é", "—Å–ª–µ–¥—É–µ—Ç", "–Ω—É–∂–Ω–æ", "—Å—Ç–æ–∏—Ç"
		]

	

	def log_batch(self, step, loss, lr, grad_norm=None, memory_gb=None, 
				  step_time=None, phase="TRAIN", perplexity=None, empathy_score=None):
		"""–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –±–∞—Ç—á–∞"""
		self.metrics['loss'].append(loss)
		self.metrics['lr'].append(lr)
		
		if grad_norm is not None:
			self.metrics['grad_norm'].append(grad_norm)
		if memory_gb is not None:
			self.metrics['memory_usage'].append(memory_gb)
		if step_time is not None:
			self.metrics['step_time'].append(step_time)
		if perplexity is not None:
			self.metrics['perplexity'].append(perplexity)
		if empathy_score is not None:
			self.metrics['empathy_score'].append(empathy_score)
		
		# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV (–≤—Å–µ–≥–¥–∞, –Ω–æ –≤—ã–≤–æ–¥ —Ç–æ–ª—å–∫–æ –≤ –æ—Ç–ª–∞–¥–æ—á–Ω–æ–º —Ä–µ–∂–∏–º–µ)
		self.save_to_csv(step, loss, lr, memory_gb, phase, perplexity, empathy_score)
	
	def save_to_csv(self, step, loss, lr, memory_gb, phase, perplexity=None, empathy_score=None):
		"""–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–∞ –≤ CSV - –¢–ò–•–ê–Ø –í–ï–†–°–ò–Ø"""
		csv_file = self.log_dir / "advanced_training_log.csv"
		
		if self.debug_mode >= 2:
			print(f"\nüíæ [DEBUG] save_to_csv —à–∞–≥ {step}...")
		
		try:
			write_header = not csv_file.exists()
			
			with open(csv_file, 'a', encoding='utf-8', newline='') as f:
				if write_header:
					f.write("timestamp,step,loss,lr,memory_gb,phase,perplexity,empathy_score\n")
				
				perp_str = f"{perplexity:.2f}" if perplexity is not None else ""
				empathy_str = f"{empathy_score:.3f}" if empathy_score is not None else ""
				line = f"{datetime.now().isoformat()},{step},{loss:.6f},{lr:.6f},{memory_gb:.1f},{phase},{perp_str},{empathy_str}\n"
				f.write(line)
			
			if self.debug_mode >= 2:
				print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")
			
		except Exception as e:
			if self.debug_mode >= 2:
				print(f"   ‚ùå –û–®–ò–ë–ö–ê save_to_csv: {e}")
			
			# –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–ø–∏—Å–∞—Ç—å –≤ –¥—Ä—É–≥–æ–π —Ñ–∞–π–ª
			backup = Path.cwd() / f"backup_log_{datetime.now().strftime('%H%M%S')}.csv"
			try:
				with open(backup, 'w') as f:
					f.write(f"–û—à–∏–±–∫–∞: {e}\n")
				print(f"   üíæ –°–æ–∑–¥–∞–Ω backup: {backup}")
			except:
				pass
	
	# –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
	
	def log_problematic_response(self, prompt, response, issue):
		"""–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤"""
		with open(self.error_log, 'a', encoding='utf-8') as f:
			f.write(f"\n[{datetime.now()}] {issue}\n")
			f.write(f"Prompt: {prompt}\n")
			f.write(f"Response: {response}\n")
			f.write("-"*80 + "\n")
	
	def calculate_perplexity(self, model, val_data, batch_size=2):
		"""–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç perplexity (–±–µ–∑ –æ—à–∏–±–∫–∏ pad_token_id)"""
		with ValidationMode(model):
			total_loss = 0.0
			num_batches = 0
			
			with torch.no_grad():
				for i in range(0, len(val_data), batch_size):
					if i + batch_size > len(val_data):
						continue
					
					batch = val_data[i:i+batch_size].cuda()
					outputs = model(batch, labels=batch)
					
					if outputs.loss is not None and not torch.isnan(outputs.loss):
						total_loss += outputs.loss.item()
						num_batches += 1
			
			if num_batches == 0:
				return float('inf')
			
			avg_loss = total_loss / num_batches
			# –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
			avg_loss = min(avg_loss, 50)
			return math.exp(avg_loss)
	
	def calculate_empathy_score(self, text):
		"""–†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ —ç–º–ø–∞—Ç–∏–∏ –ø–æ —Å–ª–æ–≤–∞—Ä—é"""
		if not text:
			return 0.0
		
		text_lower = text.lower()
		empathy_count = sum(1 for word in self.empathy_words if word in text_lower)
		
		# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 0-1, –Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º —Å—Ç—Ä–æ–≥–æ
		max_empathy = min(len(self.empathy_words), 5)  # –ú–∞–∫—Å–∏–º—É–º 5 —Å–ª–æ–≤ —ç–º–ø–∞—Ç–∏–∏ –≤ –æ—Ç–≤–µ—Ç–µ
		return min(empathy_count / max_empathy, 1.0)
	
	def advanced_quality_check(self, model, tokenizer, step, adaptive_temp=True):
		"""–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
		test_prompts = [
			"–ü–∞—Ü–∏–µ–Ω—Ç: –ù–µ –º–æ–≥—É –ø–µ—Ä–µ—Å—Ç–∞—Ç—å –≤–æ–ª–Ω–æ–≤–∞—Ç—å—Å—è.",
			"–ü–∞—Ü–∏–µ–Ω—Ç: –ß—É–≤—Å—Ç–≤—É—é —Å–µ–±—è –æ—á–µ–Ω—å –æ–¥–∏–Ω–æ–∫–æ.",
			"–ü–∞—Ü–∏–µ–Ω—Ç: –ö–∞–∫ –Ω–∞–π—Ç–∏ —Å–º—ã—Å–ª –≤ –∂–∏–∑–Ω–∏?"
		]
		
		scores = []
		empathy_scores = []
		responses = []
		
		with GenerationMode(model):  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∂–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
			# –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
			if adaptive_temp and self.quality_scores:
				last_avg_score = self.quality_scores[-1][1] if self.quality_scores else 0.5
				# –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
				temperature = max(0.6, 0.9 - (last_avg_score * 0.3))
			else:
				temperature = 0.729  # –ë–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
			
			for prompt in test_prompts:
				response = self.generate_adaptive_response(model, tokenizer, prompt, temperature)
				score = self.evaluate_response_comprehensive(prompt, response)
				empathy_score = self.calculate_empathy_score(response)
				
				scores.append(score)
				empathy_scores.append(empathy_score)
				responses.append(response)
		
		avg_score = sum(scores) / len(scores) if scores else 0
		avg_empathy = sum(empathy_scores) / len(empathy_scores) if empathy_scores else 0
		
		self.quality_scores.append((step, avg_score, avg_empathy, temperature))
		
		# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏
		quality_file = self.log_dir / "advanced_quality_checks.json"
		quality_data = {
			'step': step,
			'timestamp': datetime.now().isoformat(),
			'avg_score': avg_score,
			'avg_empathy': avg_empathy,
			'temperature': temperature,
			'tests': []
		}
		
		for prompt, response, score, empathy in zip(test_prompts, responses, scores, empathy_scores):
			quality_data['tests'].append({
				'prompt': prompt,
				'response': response,
				'score': score,
				'empathy_score': empathy
			})
		
		if quality_file.exists():
			with open(quality_file, 'r', encoding='utf-8') as f:
				existing = json.load(f)
		else:
			existing = []
		
		existing.append(quality_data)
		
		with open(quality_file, 'w', encoding='utf-8') as f:
			json.dump(existing, f, ensure_ascii=False, indent=2)
		
		return avg_score, avg_empathy, temperature
	
	def generate_adaptive_response(self, model, tokenizer, prompt, temperature=0.729):
		"""–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
		try:
			full_prompt = f"{prompt}\n\n–ü—Å–∏—Ö–æ–ª–æ–≥:"
			inputs = tokenizer(full_prompt, return_tensors="pt", max_length=512, truncation=True).to(model.device)
			
			# –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π top_p –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
			top_p = 0.95 if temperature > 0.8 else 0.9
			
			with torch.no_grad():
				outputs = model.generate(
					**inputs,
					max_new_tokens=256,
					min_new_tokens=16,
					temperature=temperature,
					do_sample=True,
					top_p=top_p,
					top_k=50,
					pad_token_id=tokenizer.eos_token_id,
					num_return_sequences=1,
					repetition_penalty=1.1,
					length_penalty=0.8
				)
			
			response = tokenizer.decode(outputs[0], skip_special_tokens=True)
			response = response[len(full_prompt):].strip()
			
			# –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
			response = self.clean_response(response)
			
			return response
		except Exception as e:
			self.log_problematic_response(prompt, str(e), "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
			return ""
	
	def clean_response(self, text):
		"""–û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞"""
		if not text:
			return ""
		
		# –£–¥–∞–ª—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
		text = text.replace('ÔøΩ', '').replace('\x00', '')
		
		# –û–±—Ä–µ–∑–∞–µ–º –ø–æ —Å—Ç–æ–ø-—Ñ—Ä–∞–∑–∞–º
		stops = ['\n–ü–∞—Ü–∏–µ–Ω—Ç:', '\n–ü—Å–∏—Ö–æ–ª–æ–≥:', '\n---', '\n===']
		for stop in stops:
			if stop in text:
				text = text.split(stop)[0].strip()
		
		# –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
		text = ' '.join(text.split())
		
		return text
	
	def evaluate_response_comprehensive(self, prompt, response):
		"""–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""
		if not response:
			return 0.0
		
		score = 0.0
		words = response.split()
		
		# 1. –ë–∞–∑–æ–≤–∞—è –¥–ª–∏–Ω–∞ (5-80 —Å–ª–æ–≤)
		if 5 <= len(words) <= 80:
			score += 1.0
		
		# 2. –≠–º–ø–∞—Ç–∏—è (—É–∂–µ —Å—á–∏—Ç–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ, –Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –±–æ–Ω—É—Å)
		empathy_score = self.calculate_empathy_score(response)
		score += empathy_score  # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä—è–º–æ –∫–∞–∫ —á–∞—Å—Ç—å –æ—Ü–µ–Ω–∫–∏
		
		# 3. –í–æ–ø—Ä–æ—Å—ã (–≤–∞–∂–Ω–æ –¥–ª—è –ø—Å–∏—Ö–æ–ª–æ–≥–∞)
		if '?' in response:
			score += 1.0
		
		# 4. –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å–æ–≤–µ—Ç–æ–≤
		if not any(word in response.lower() for word in self.advice_words):
			score += 1.0
		
		# 5. –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤ (–º–µ–Ω—å—à–µ –ø–æ–≤—Ç–æ—Ä–æ–≤)
		if len(words) > 5:
			unique_words = len(set(words))
			if unique_words / len(words) > 0.6:
				score += 1.0
		
		# 6. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å—É
		prompt_words = set(prompt.lower().split()[:10])
		response_words = set(response.lower().split())
		if len(prompt_words.intersection(response_words)) >= 1:
			score += 1.0
		
		# 7. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–Ω–∞–ª–∏—á–∏–µ —Ç–æ—á–µ–∫)
		if '.' in response:
			score += 0.5
		
		# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 0-1 (–º–∞–∫—Å–∏–º—É–º 7.5 –±–∞–ª–ª–æ–≤)
		return min(score / 7.5, 1.0)

# ================= –£–õ–£–ß–®–ï–ù–ù–û–ï –°–û–•–†–ê–ù–ï–ù–ò–ï =================

def save_checkpoint(model, tokenizer, optimizer, step, loss, epoch, checkpoint_dir, 
					is_best=False, scheduler=None, monitor=None):
	"""
	–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
	"""
	try:
		checkpoint_dir = Path(checkpoint_dir)
		checkpoint_dir.mkdir(parents=True, exist_ok=True)
		
		print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ —à–∞–≥ {step}...")
		
		# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
		model.save_pretrained(str(checkpoint_dir))
		tokenizer.save_pretrained(str(checkpoint_dir))
		
		# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
		checkpoint_state = {
			'step': step,
			'epoch': epoch,
			'model_state_dict': model.state_dict(),
			'optimizer_state_dict': optimizer.state_dict(),
			'loss': float(loss),
			'batch_size': BATCH_SIZE,
			'learning_rate': LEARNING_RATE,
			'timestamp': datetime.now().isoformat(),
		}
		
		if scheduler:
			checkpoint_state['scheduler_step'] = scheduler.current_step
		
		if monitor and monitor.quality_scores:
			checkpoint_state['last_quality'] = monitor.quality_scores[-1] if monitor.quality_scores else None
		
		torch.save(checkpoint_state, checkpoint_dir / "checkpoint.pt")
		
		# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–µ–∫–ø–æ–∏–Ω—Ç–µ
		info_file = checkpoint_dir / "checkpoint_info.txt"
		with open(info_file, 'w', encoding='utf-8') as f:
			f.write(f"–ß–ï–ö–ü–û–ò–ù–¢ {step}\n")
			f.write(f"–≠–ø–æ—Ö–∞: {epoch}\n")
			f.write(f"Loss: {loss:.6f}\n")
			f.write(f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
			if is_best:
				f.write(f"\nüèÜ –°–¢–ê–¢–£–°: –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨\n")
		
		print(f"   ‚úÖ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω")
		return True
		
	except Exception as e:
		print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
		return False

def load_last_checkpoint(checkpoint_dir, model, optimizer=None):
	"""–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
	try:
		checkpoint_dir = Path(checkpoint_dir)
		checkpoints = sorted(checkpoint_dir.glob("step_*"), 
						   key=lambda x: int(x.name.split('_')[1]) if x.name.split('_')[1].isdigit() else 0,
						   reverse=True)
		
		if checkpoints:
			last_checkpoint = checkpoints[0]
			checkpoint = torch.load(last_checkpoint / "checkpoint.pt", map_location='cpu')
			
			model.load_state_dict(checkpoint['model_state_dict'])
			if optimizer and 'optimizer_state_dict' in checkpoint:
				optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
			
			print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {last_checkpoint.name}")
			return checkpoint['step'], checkpoint['loss'], checkpoint['epoch']
	
	except Exception as e:
		print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")
	
	return 0, float('inf'), 0

# ================= –ü–£–¢–ò =================
BASE_DIR = Path("D:/Ruzanna")
CHECKPOINTS_DIR = BASE_DIR / "checkpoints_advanced"
FINAL_MODEL_DIR = BASE_DIR / "final_model_advanced"
LOGS_DIR = BASE_DIR / "logs_advanced"
DATA_DIR = Path("C:/Files/processed_epitome")

CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)
FINAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)
LOGS_DIR.mkdir(parents=True, exist_ok=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ä–µ–∂–∏–º–æ–º
monitor = AdvancedTrainingMonitor(LOGS_DIR, debug_mode=DEBUG_MODE)

# –¢–æ–ª—å–∫–æ –≤ –æ—Ç–ª–∞–¥–æ—á–Ω–æ–º —Ä–µ–∂–∏–º–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
if DEBUG_MODE >= 2:
	print(f"\nüîç –ü–†–û–í–ï–†–ö–ê –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê:")
	print(f"   ‚Ä¢ log_dir: {monitor.log_dir}")
	
	# –¢–µ—Å—Ç–æ–≤–∞—è –∑–∞–ø–∏—Å—å
	monitor.save_to_csv(0, 1.0, 1e-4, 5.0, "TEST", 10.0, 0.5)

# –ü–†–û–í–ï–†–ö–ê –°–†–ê–ó–£ –ü–û–°–õ–ï –°–û–ó–î–ê–ù–ò–Ø
print(f"\nüîç –ü–†–û–í–ï–†–ö–ê –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê:")
print(f"   ‚Ä¢ log_dir: {monitor.log_dir}")
print(f"   ‚Ä¢ –°—É—â–µ—Å—Ç–≤—É–µ—Ç: {monitor.log_dir.exists()}")

# –¢–µ—Å—Ç–æ–≤–∞—è –∑–∞–ø–∏—Å—å —á–µ—Ä–µ–∑ monitor
monitor.save_to_csv(0, 1.0, 1e-4, 5.0, "TEST", 10.0, 0.5)

# –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–∞–π–ª
csv_file = monitor.log_dir / "advanced_training_log.csv"
print(f"   ‚Ä¢ CSV —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {csv_file.exists()}")
if csv_file.exists():
	print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {csv_file.stat().st_size} –±–∞–π—Ç")
	with open(csv_file, 'r') as f:
		print(f"   ‚Ä¢ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ:\n{f.read()}")

# ================= –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• =================
print(f"\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

data_path = DATA_DIR / "quality_psych_dialogues_enhanced.json"
if not data_path.exists():
	data_path = DATA_DIR / "quality_psych_dialogues.json"

if data_path.exists():
	with open(data_path, 'r', encoding='utf-8') as f:
		dialogues = json.load(f)
	
	print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dialogues)} –¥–∏–∞–ª–æ–≥–æ–≤")
	
	texts = [dialogue['text'] for dialogue in dialogues]
	
else:
	print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
	sys.exit(1)

# ================= –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø =================
print(f"\nüî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")

tokenizer = GPT2Tokenizer.from_pretrained("C:/Files/datasets/neo")
tokenizer.pad_token = tokenizer.eos_token

all_tokens = []
for text in texts:
	tokens = tokenizer.encode(
		text,
		max_length=MAX_LENGTH,
		truncation=True,
		padding='max_length',
		return_tensors='pt'
	)
	all_tokens.append(tokens)

all_tokens = torch.cat(all_tokens, dim=0)

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
indices = torch.randperm(len(all_tokens))
all_tokens = all_tokens[indices]

split_idx = int(0.85 * len(all_tokens))
train_data = all_tokens[:split_idx]
val_data = all_tokens[split_idx:]

print(f"   Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
print(f"   Validation: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")

# ================= –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò =================
print(f"\nüß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ GPT-Neo 2.7B...")

quant_config = BitsAndBytesConfig(
	load_in_8bit=True,
	llm_int8_threshold=6.0,
)

model = GPTNeoForCausalLM.from_pretrained(
	"C:/Files/datasets/neo",
	quantization_config=quant_config,
	device_map="auto",
	torch_dtype=torch.float16,
)

print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

# ================= –û–ü–¢–ò–ú–ò–ó–ê–¢–û–† =================
print(f"\n‚ö° –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞...")

optimizer = bnb.optim.AdamW8bit(
	model.parameters(),
	lr=LEARNING_RATE,
	betas=(0.9, 0.95),
	weight_decay=0.01,
)

# ================= –†–ê–°–ß–ï–¢ –®–ê–ì–û–í –ò –®–ï–î–£–õ–ï–† =================

# –ù–ê–ô–î–ò–¢–ï –≠–¢–£ –°–¢–†–û–ö–£ (~740) –ò –ò–°–ü–†–ê–í–¨–¢–ï:
total_batches = len(train_data) // BATCH_SIZE
# total_steps = (total_batches // GRADIENT_ACCUMULATION) * EPOCHS  # ‚ùå –°–¢–ê–†–û–ï

# ‚¨áÔ∏è –ù–û–í–û–ï:
if GRADIENT_ACCUMULATION > 0:
	total_steps = max(1, (total_batches + GRADIENT_ACCUMULATION - 1) // GRADIENT_ACCUMULATION * EPOCHS)
else:
	total_steps = max(1, total_batches * EPOCHS)

print(f"\nüìà –ü–õ–ê–ù –û–ë–£–ß–ï–ù–ò–Ø:")
print(f"   ‚Ä¢ –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_steps}")

scheduler = OptimalScheduler(optimizer, total_steps, LEARNING_RATE, WARMUP_RATIO)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–∞–Ω–Ω–µ–≥–æ —Å—Ç–æ–ø–ø–∏–Ω–≥–∞
checkpoint_steps = [25, 50, 100, 200, 400, 600, 800]
best_loss = float('inf')
best_model_step = 0
patience = 3
patience_counter = 0
previous_val_loss = float('inf')
min_delta = 0.001  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–∏–º–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ

# –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
nan_loss_count = 0
max_nan_losses = 3

# ================= –û–ë–£–ß–ï–ù–ò–ï –° –ê–î–ê–ü–¢–ò–í–ù–´–ú–ò –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò =================
print(f"\nüéØ –ù–ê–ß–ò–ù–ê–Æ –û–ë–£–ß–ï–ù–ò–ï –° –ê–î–ê–ü–¢–ò–í–ù–´–ú–ò –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò...")

with TrainingMode(model):  # ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç use_cache –∏ gradient_checkpointing
	print(f"   ‚Ä¢ –†–µ–∂–∏–º: –û–ë–£–ß–ï–ù–ò–ï")
	print(f"   ‚Ä¢ use_cache: {model.config.use_cache}")
	print(f"   ‚Ä¢ gradient_checkpointing: {model.is_gradient_checkpointing}")

global_step = 0
start_time = datetime.now()

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç
initial_checkpoint_dir = CHECKPOINTS_DIR / "initial_model"
save_checkpoint(model, tokenizer, optimizer, 0, float('inf'), 0, initial_checkpoint_dir)

for epoch in range(EPOCHS):
	print(f"\n{'='*60}")
	print(f"üìö –≠–ü–û–•–ê {epoch+1}/{EPOCHS}")
	print(f"{'='*60}")
	
	epoch_loss = 0.0
	batch_count = 0
	accumulation_count = 0
	epoch_start_time = time.time()      # ‚¨ÖÔ∏è –î–û–ë–ê–í–¨–¢–ï
	last_print_time = time.time()       # ‚¨ÖÔ∏è –î–û–ë–ê–í–¨–¢–ï
	
	# –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
	train_indices = torch.randperm(len(train_data))
	train_data_shuffled = train_data[train_indices]
	
	with TrainingMode(model):  # ‚¨ÖÔ∏è –£–ë–ï–î–ò–¢–ï–°–¨ –ß–¢–û –ï–°–¢–¨ –û–¢–°–¢–£–ü (4 –ø—Ä–æ–±–µ–ª–∞)
		for batch_idx in range(0, len(train_data_shuffled), BATCH_SIZE):
			if batch_idx + BATCH_SIZE > len(train_data_shuffled):
				continue
				
			batch_start_time = time.time()
			batch = train_data_shuffled[batch_idx:batch_idx+BATCH_SIZE].cuda()
			
			try:
				# Forward pass
				outputs = model(batch, labels=batch)
				loss = outputs.loss
				
				# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
				if math.isnan(loss.item()):
					nan_loss_count += 1
					print(f"   ‚ö†Ô∏è  NaN loss detected ({nan_loss_count}/{max_nan_losses})")
					
					if nan_loss_count >= max_nan_losses:
						print(f"   üîÑ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞...")
						global_step, _, _ = load_last_checkpoint(CHECKPOINTS_DIR, model, optimizer)
						nan_loss_count = 0
						continue
					
					# –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –±–∞—Ç—á
					optimizer.zero_grad()
					continue
				
				loss_value = loss.item()
				epoch_loss += loss_value
				batch_count += 1
				# ================= –í–´–í–û–î –ü–†–û–ì–†–ï–°–°–ê =================
				current_lr = LEARNING_RATE  # –Ω–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

				current_time = time.time()
				if current_time - last_print_time > 10:  # –ö–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥
					progress = (batch_idx / len(train_data_shuffled)) * 100
					avg_loss_so_far = epoch_loss / (batch_count + 1e-8)
	
					# –†–ê–°–ß–ï–¢ –°–ö–û–†–û–°–¢–ò
					elapsed_since_last_print = current_time - last_print_time
					batches_since_last_print = (batch_idx // BATCH_SIZE) - last_batch_count if 'last_batch_count' in locals() else 1
					last_batch_count = batch_idx // BATCH_SIZE
	
					dialogs_per_second = batches_since_last_print * BATCH_SIZE / elapsed_since_last_print if elapsed_since_last_print > 0 else 0
					tokens_per_second = dialogs_per_second * MAX_LENGTH  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –≤ —Ç–æ–∫–µ–Ω–∞—Ö
	
					if DEBUG_MODE == 1:
					# –¶–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥ (–µ—Å–ª–∏ —Ç–µ—Ä–º–∏–Ω–∞–ª –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç)
						try:
						# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
							if dialogs_per_second > 0.5:
								speed_color = "\033[92m"  # –∑–µ–ª–µ–Ω—ã–π
								speed_icon = "üöÄ"
							elif dialogs_per_second > 0.2:
								speed_color = "\033[93m"  # –∂–µ–ª—Ç—ã–π
								speed_icon = "‚ö°"
							else:
								speed_color = "\033[91m"  # –∫—Ä–∞—Å–Ω—ã–π
								speed_icon = "üêå"
			
							reset_color = "\033[0m"
			
							print(f"\r   üîÑ {progress:5.1f}% | üìâ {loss_value:7.4f} | üéõÔ∏è {current_lr:.1e} | üß∫ {batch_idx//BATCH_SIZE:4d} | {speed_icon} {speed_color}{dialogs_per_second:5.2f} –¥–∏–∞–ª/—Å{reset_color}", end='', flush=True)
						except:
							# –ë–µ–∑ —Ü–≤–µ—Ç–æ–≤ –µ—Å–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
							print(f"\r   üîÑ {progress:5.1f}% | Loss: {loss_value:7.4f} | LR: {current_lr:.2e} | –ë–∞—Ç—á: {batch_idx//BATCH_SIZE:4d} | üöÄ {dialogs_per_second:5.2f} –¥/—Å", end='', flush=True)
	
					elif DEBUG_MODE >= 2:
						# –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥
						print(f"\n   ‚è∞ {datetime.now().strftime('%H:%M:%S')}")
						print(f"   üìç –ë–∞—Ç—á {batch_idx//BATCH_SIZE} ({progress:.1f}%)")
						print(f"   üìâ Loss: {loss_value:.4f} (—Å—Ä–µ–¥–Ω: {avg_loss_so_far:.4f})")
						print(f"   üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {dialogs_per_second:.2f} –¥/—Å (~{tokens_per_second/1000:.1f}K —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫)")
						print(f"   üíæ GPU –ø–∞–º—è—Ç—å: {torch.cuda.memory_allocated()/1024**3:.1f} GB")
						print(f"   ‚ö° GPU –º–æ—â–Ω–æ—Å—Ç—å: {get_gpu_power()}W")  # –µ—Å–ª–∏ –µ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ—â–Ω–æ—Å—Ç–∏
	
					last_print_time = current_time

				elif DEBUG_MODE == 1:
					# –ë—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (–±–µ–∑ —Ä–∞—Å—á–µ—Ç–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏)
					progress = (batch_idx / len(train_data_shuffled)) * 100
					print(f"\r   üîÑ {progress:5.1f}% | Loss: {loss_value:7.4f} | LR: {current_lr:.2e} | –ë–∞—Ç—á: {batch_idx//BATCH_SIZE:4d} | ‚è≥...", end='', flush=True)
				# ================= –ö–û–ù–ï–¶ –í–´–í–û–î–ê –ü–†–û–ì–†–ï–°–°–ê =================
				
				# Gradient accumulation
				loss = loss / GRADIENT_ACCUMULATION
				loss.backward()
				
				accumulation_count += 1
				
				# Step —Å gradient accumulation
				if accumulation_count % GRADIENT_ACCUMULATION == 0:
					# Gradient clipping
					grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
					
					# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
					optimizer.step()
					current_lr, phase = scheduler.step()  # ‚¨ÖÔ∏è –¢–ï–ü–ï–†–¨ current_lr –æ–±–Ω–æ–≤–ª–µ–Ω
					optimizer.zero_grad()
					
					global_step += 1
					step_time = time.time() - batch_start_time
					
					# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
					memory_gb = torch.cuda.memory_allocated() / 1024**3
					monitor.log_batch(global_step, loss_value, current_lr, grad_norm, memory_gb, step_time, phase)
					
					# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 10 —à–∞–≥–æ–≤
					if global_step % 10 == 0:
						avg_loss = epoch_loss / batch_count
						elapsed = (datetime.now() - start_time).seconds / 60
						
						print(f"\n   –®–∞–≥ {global_step} [{phase}]:")
						print(f"   ‚Ä¢ Loss: {loss_value:.4f} | Avg: {avg_loss:.4f}")
						print(f"   ‚Ä¢ LR: {current_lr:.2e}")
						print(f"   ‚Ä¢ –í—Ä–µ–º—è: {elapsed:.1f} –º–∏–Ω")
					
					# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
					if global_step % 50 == 0:
						quality_score, empathy_score, current_temp = monitor.advanced_quality_check(
							model, tokenizer, global_step, adaptive_temp=True
						)
						print(f"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ: {quality_score:.2f} | –≠–º–ø–∞—Ç–∏—è: {empathy_score:.2f} | Temp: {current_temp:.3f}")
					
					# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
					if global_step in checkpoint_steps:
						checkpoint_dir = CHECKPOINTS_DIR / f"step_{global_step}_epoch_{epoch+1}"
						save_checkpoint(model, tokenizer, optimizer, global_step, 
									  epoch_loss/batch_count, epoch+1, checkpoint_dir, 
									  scheduler=scheduler, monitor=monitor)
				
			except Exception as e:
				print(f"\n   ‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ: {e}")
				optimizer.zero_grad()
				continue
	
	# ================= –ö–û–ù–ï–¶ –≠–ü–û–•–ò =================
	if DEBUG_MODE == 1:
		print()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º —Å—Ç—Ä–æ–∫—É –ø–æ—Å–ª–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
	
	# –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏
	avg_epoch_loss = epoch_loss / batch_count if batch_count > 0 else float('inf')
	print(f"\n‚úÖ –≠–ü–û–•–ê {epoch+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
	print(f"   ‚Ä¢ Train Loss: {avg_epoch_loss:.4f}")
	print(f"   ‚Ä¢ –®–∞–≥–æ–≤: {global_step}")
	
	# –†–∞—Å—á–µ—Ç perplexity –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
	perplexity = monitor.calculate_perplexity(model, val_data, BATCH_SIZE)
	print(f"   ‚Ä¢ Perplexity: {perplexity:.2f}")
	
	# –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞–Ω–Ω–∏–π —Å—Ç–æ–ø–ø–∏–Ω–≥
	if previous_val_loss != float('inf'):
		improvement = previous_val_loss - perplexity
		
		if improvement < min_delta:
			patience_counter += 1
			print(f"   ‚ö†Ô∏è  –ú–∞–ª–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ perplexity ({improvement:.4f} < {min_delta}). Patience: {patience_counter}/{patience}")
		else:
			patience_counter = 0
			print(f"   ‚úÖ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ perplexity: {improvement:.4f}")
		
		if patience_counter >= patience:
			print(f"\nüö´ –†–ê–ù–ù–Ø–Ø –û–°–¢–ê–ù–û–í–ö–ê: –Ω–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π {patience} —ç–ø–æ—Ö–∏ –ø–æ–¥—Ä—è–¥")
			break
	
	# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
	if perplexity < best_loss:
		best_loss = perplexity
		best_model_step = global_step
		
		best_dir = CHECKPOINTS_DIR / f"BEST_epoch_{epoch+1}_perplexity_{best_loss:.2f}"
		save_checkpoint(model, tokenizer, optimizer, global_step, 
					  best_loss, epoch+1, best_dir, is_best=True, 
					  scheduler=scheduler, monitor=monitor)
		print(f"   üèÜ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: perplexity={best_loss:.2f}")
	
	previous_val_loss = perplexity
	
	# –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç —ç–ø–æ—Ö–∏
	epoch_checkpoint_dir = CHECKPOINTS_DIR / f"epoch_{epoch+1}_final"
	save_checkpoint(model, tokenizer, optimizer, global_step, avg_epoch_loss, 
				   epoch+1, epoch_checkpoint_dir, scheduler=scheduler, monitor=monitor)

# ================= –°–û–•–†–ê–ù–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò =================
print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏...")

try:
	model.save_pretrained(str(FINAL_MODEL_DIR))
	tokenizer.save_pretrained(str(FINAL_MODEL_DIR))
	
	training_info = {
		'total_steps': global_step,
		'final_train_loss': avg_epoch_loss,
		'best_perplexity': best_loss,
		'best_step': best_model_step,
		'epochs_completed': epoch + 1,
		'early_stopped': patience_counter >= patience,
		'final_perplexity': perplexity,
		'batch_size': BATCH_SIZE,
		'learning_rate': LEARNING_RATE,
		'training_time_minutes': (datetime.now() - start_time).seconds / 60,
		'completion_time': datetime.now().isoformat(),
		'adaptive_training': True,
		'advanced_metrics': True,
		'gradient_checkpointing': True,
		'use_cache_strategy': 'adaptive'
	}
	
	with open(FINAL_MODEL_DIR / "training_info.json", 'w', encoding='utf-8') as f:
		json.dump(training_info, f, ensure_ascii=False, indent=2)
	
	print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
	
except Exception as e:
	print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

# ================= –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢ =================
print(f"\nüß™ –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢ –° –ê–î–ê–ü–¢–ò–í–ù–´–ú–ò –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò...")
with GenerationMode(model):  # ‚úÖ –ì–ï–ù–ï–†–ê–¶–ò–Ø: cache=ON, gc=OFF
	print(f"   ‚Ä¢ –†–µ–∂–∏–º: –ì–ï–ù–ï–†–ê–¶–ò–Ø")
	print(f"   ‚Ä¢ use_cache: {model.config.use_cache}")
	print(f"   ‚Ä¢ gradient_checkpointing: {model.is_gradient_checkpointing}")

test_prompts = [
	"–ü–∞—Ü–∏–µ–Ω—Ç: –ù–µ –º–æ–≥—É –ø–µ—Ä–µ—Å—Ç–∞—Ç—å –≤–æ–ª–Ω–æ–≤–∞—Ç—å—Å—è.",
	"–ü–∞—Ü–∏–µ–Ω—Ç: –ß—É–≤—Å—Ç–≤—É—é —Å–µ–±—è –æ—á–µ–Ω—å –æ–¥–∏–Ω–æ–∫–æ.",
	"–ü–∞—Ü–∏–µ–Ω—Ç: –ö–∞–∫ –Ω–∞–π—Ç–∏ —Å–º—ã—Å–ª –≤ –∂–∏–∑–Ω–∏?",
	"–ü–∞—Ü–∏–µ–Ω—Ç: –í—Å—ë –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ, –Ω–µ –≤–∏–∂—É –ø—Ä–∏—á–∏–Ω –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å.",
	"–ü–∞—Ü–∏–µ–Ω—Ç: –ë–æ—é—Å—å, —á—Ç–æ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –∏–∑–º–µ–Ω—é—Å—å."
]

for i, prompt in enumerate(test_prompts):
	try:
		# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
		last_quality = monitor.quality_scores[-1][1] if monitor.quality_scores else 0.5
		adaptive_temp = max(0.6, 0.9 - (last_quality * 0.3))
		
		with GenerationMode(model):  # –ö–∞–∂–¥—ã–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ
			response = monitor.generate_adaptive_response(model, tokenizer, prompt, adaptive_temp)
			score = monitor.evaluate_response_comprehensive(prompt, response)
			empathy_score = monitor.calculate_empathy_score(response)
		
		print(f"\n{i+1}. üí≠ {prompt}")
		print(f"   üå°Ô∏è  Temp: {adaptive_temp:.3f}")
		print(f"   üí¨ {response[:120]}{'...' if len(response) > 120 else ''}")
		print(f"   üìä –û—Ü–µ–Ω–∫–∞: {score:.2f} | –≠–º–ø–∞—Ç–∏—è: {empathy_score:.2f}")
		
	except Exception as e:
		print(f"\n{i+1}. ‚ùå –û—à–∏–±–∫–∞: {e}")

print(f"\n{'='*80}")
print("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
print(f"{'='*80}")
print(f"üìä –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò:")
print(f"   ‚Ä¢ –®–∞–≥–æ–≤: {global_step}")
print(f"   ‚Ä¢ –õ—É—á—à–∏–π perplexity: {best_loss:.2f}")
print(f"   ‚Ä¢ –§–∏–Ω–∞–ª—å–Ω—ã–π perplexity: {perplexity:.2f}")
print(f"   ‚Ä¢ –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞: {'–î–∞' if patience_counter >= patience else '–ù–µ—Ç'}")
print(f"   ‚Ä¢ NaN –æ–±—Ä–∞–±–æ—Ç–æ–∫: {nan_loss_count}")
print(f"   ‚Ä¢ –í—Ä–µ–º—è: {(datetime.now() - start_time).seconds/60:.1f} –º–∏–Ω")
print(f"   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∂–∏–º—ã:")
print(f"      - –û–±—É—á–µ–Ω–∏–µ: cache=OFF, gradient_checkpointing={model.is_gradient_checkpointing}")
print(f"      - –í–∞–ª–∏–¥–∞—Ü–∏—è: cache=OFF, gradient_checkpointing=OFF")
print(f"      - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: cache=ON, gradient_checkpointing=OFF")
print(f"{'='*80}")
